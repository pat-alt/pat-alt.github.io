<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>bayes | Patrick Altmeyer</title>
    <link>https://www.paltmeyer.com/tag/bayes/</link>
      <atom:link href="https://www.paltmeyer.com/tag/bayes/index.xml" rel="self" type="application/rss+xml" />
    <description>bayes</description>
    <generator>Wowchemy (https://wowchemy.com)</generator><language>en-us</language><copyright>© 2022 Patrick Altmeyer</copyright><lastBuildDate>Fri, 18 Feb 2022 00:00:00 +0000</lastBuildDate>
    <image>
      <url>https://www.paltmeyer.com/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_512x512_fill_lanczos_center_3.png</url>
      <title>bayes</title>
      <link>https://www.paltmeyer.com/tag/bayes/</link>
    </image>
    
    <item>
      <title>Go deep, but also ... go Bayesian!</title>
      <link>https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/</link>
      <pubDate>Fri, 18 Feb 2022 00:00:00 +0000</pubDate>
      <guid>https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/</guid>
      <description>&lt;div class=&#34;intro-gif&#34;&gt;
&lt;figure&gt;
&lt;img src=&#34;www/anim.gif&#34;&gt;
&lt;figcaption&gt;
A Bayesian Neural Network gradually learns.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;Deep learning has dominated AI research in recent years&lt;sup id=&#34;fnref:1&#34;&gt;&lt;a href=&#34;#fn:1&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;1&lt;/a&gt;&lt;/sup&gt; - but how
much promise does it really hold? That is very much an ongoing and
increasingly polarising debate that you can follow live on
&lt;a href=&#34;https://twitter.com/ilyasut/status/1491554478243258368&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Twitter&lt;/a&gt;. On
one side you have optimists like Ilya Sutskever, chief scientist of
OpenAI, who believes that large deep neural networks may already be
slightly conscious - that&amp;rsquo;s &amp;ldquo;may&amp;rdquo; and &amp;ldquo;slightly&amp;rdquo; and only if you just go
deep enough? On the other side you have prominent skeptics like Judea
Pearl who has long since argued that deep learning still boils down to
curve fitting - purely associations and not even remotely intelligent
(Pearl and Mackenzie 2018).&lt;/p&gt;
&lt;h2 id=&#34;the-case-for-bayesian-deep-learning&#34;&gt;The case for Bayesian Deep Learning&lt;/h2&gt;
&lt;p&gt;Whatever side of this entertaining debate you find yourself on, the
reality is that deep-learning systems have already been deployed at
large scale both in academia and industry. More pressing debates
therefore revolve around the trustworthiness of these existing systems.
How robust are they and in what way exactly do they arrive at decisions
that affect each and every one of us? Robustifying deep neural networks
generally involves some form of adversarial training, which is costly,
can hurt generalization (Raghunathan et al. 2019) and does ultimately
not guarantee stability (Bastounis, Hansen, and Vlačić 2021). With
respect to interpretability, surrogate explainers like LIME and SHAP are
among the most popular tools, but they too have been shown to lack
robustness (Slack et al. 2020).&lt;/p&gt;
&lt;p&gt;Exactly why are deep neural networks unstable and in-transparent? Let
$\mathcal{D}={x,y}_{n=1}^N$ denote our feature-label pairs and let
$f(x;\theta)=y$ denote some deep neural network specified by its
parameters $\theta$. Then the first thing to note is that the number of
free parameters $\theta$ is typically huge (if you ask Mr Sutskever it
really probably cannot be huge enough!). That alone makes it very hard
to monitor and interpret the inner workings of deep-learning algorithms.
Perhaps more importantly though, the number of parameters &lt;em&gt;relative&lt;/em&gt; to
the size of $\mathcal{D}$ is generally huge:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;[&amp;hellip;] deep neural networks are typically very underspecified by the
available data, and [&amp;hellip;] parameters [therefore] correspond to a
diverse variety of compelling explanations for the data. (Wilson 2020)&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;In other words, training a single deep neural network may (and usually
does) lead to one random parameter specification that fits the
underlying data very well. But in all likelihood there are many other
specifications that also fit the data very well. This is both a strength
and vulnerability of deep learning: it is a strength because it
typically allows us to find one such &amp;ldquo;compelling explanation&amp;rdquo; for the
data with ease through stochastic optimization; it is a vulnerability
because one has to wonder:&lt;/p&gt;
&lt;blockquote&gt;
&lt;p&gt;How compelling is an explanation really if it competes with many other
equally compelling, but potentially very different explanations?&lt;/p&gt;
&lt;/blockquote&gt;
&lt;p&gt;A scenario like this very much calls for treating predictions from deep
learning models probabilistically [Wilson (2020)]&lt;sup id=&#34;fnref:2&#34;&gt;&lt;a href=&#34;#fn:2&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;2&lt;/a&gt;&lt;/sup&gt;&lt;sup id=&#34;fnref:3&#34;&gt;&lt;a href=&#34;#fn:3&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;3&lt;/a&gt;&lt;/sup&gt;.&lt;/p&gt;
&lt;p&gt;Formally, we are interested in estimating the posterior predictive
distribution as the following Bayesian model average (BMA):&lt;/p&gt;
&lt;p&gt;$$
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
$$&lt;/p&gt;
&lt;p&gt;The integral implies that we essentially need many predictions from many
different specifications of $\theta$. Unfortunately, this means more
work for us or rather our computers. Fortunately though, researchers
have proposed many ingenious ways to approximate the equation above: Gal
and Ghahramani (2016) propose using dropout at test time while
Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over
an ensemble of just five models seems to do the trick. Still, despite
their simplicity and usefulness these approaches involve additional
computational costs compared to training just a single network. As we
shall see now though, another promising approach has recently entered
the limelight: &lt;strong&gt;Laplace approximation&lt;/strong&gt; (LA).&lt;/p&gt;
&lt;p&gt;If you have read my &lt;a href=&#34;https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;previous
post&lt;/a&gt;
on Bayesian Logistic Regression, then the term Laplace should already
sound familiar to you. As a matter of fact, we will see that all
concepts covered in that previous post can be naturally extended to deep
learning. While some of these concepts will be revisited below, I
strongly recommend you check out the previous post before reading on
here. Without further ado let us now see how LA can be used for truly
effortless deep learning.&lt;/p&gt;
&lt;h2 id=&#34;laplace-approximation&#34;&gt;Laplace Approximation&lt;/h2&gt;
&lt;p&gt;While LA was first proposed in the 18th century, it has so far not
attracted serious attention from the deep learning community largely
because it involves a possibly large Hessian computation. Daxberger et
al. (2021) are on a mission to change the perception that LA has no use
in DL: in their &lt;a href=&#34;https://arxiv.org/pdf/2106.14806.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS 2021
paper&lt;/a&gt; they demonstrate
empirically that LA can be used to produce Bayesian model averages that
are at least at par with existing approaches in terms of uncertainty
quantification and out-of-distribution detection. They show that recent
advancements in autodifferentation can be leveraged to produce fast and
accurate approximations of the Hessian and even provide a fully-fledged
&lt;a href=&#34;https://aleximmer.github.io/Laplace/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Python library&lt;/a&gt; that can be used
with any pretrained Torch model. For this post, I have built a much less
comprehensive, pure-play equivalent of their package in Julia -
&lt;a href=&#34;https://www.paltmeyer.com/BayesLaplace.jl/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BayesLaplace.jl&lt;/a&gt; can be
used with deep learning models built in &lt;a href=&#34;https://fluxml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flux.jl&lt;/a&gt;,
which is Julia&amp;rsquo;s main DL library. As in the previous post on Bayesian
logistic regression I will rely on Julia code snippits instead of
equations to convey the underlying maths. If you&amp;rsquo;re curious about the
maths, the &lt;a href=&#34;https://arxiv.org/pdf/2106.14806.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS 2021 paper&lt;/a&gt;
provides all the detail you need.&lt;/p&gt;
&lt;h3 id=&#34;from-bayesian-logistic-regression-&#34;&gt;From Bayesian Logistic Regression &amp;hellip;&lt;/h3&gt;
&lt;p&gt;Let&amp;rsquo;s recap: in the case of logistic regression we had a assumed a
zero-mean Gaussian prior
$p(\mathbf{w}) \sim \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)=\mathcal{N} \left( \mathbf{w} | \mathbf{0}, \mathbf{H}_0^{-1} \right)$
for the weights that are used to compute logits
$\mu_n=\mathbf{w}^T\mathbf{x}_n$, which in turn are fed to a sigmoid
function to produce probabilities $p(y_n=1)=\sigma(\mu_n)$. We saw that
under this assumption solving the logistic regression problem
corresponds to minimizing the following differentiable loss function:&lt;/p&gt;
&lt;p&gt;$$
\ell(\mathbf{w})= - \sum_{n}^N [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0)
$$&lt;/p&gt;
&lt;p&gt;As our first step towards Bayesian deep learning, we observe the
following: the loss function above corresponds to the objective faced by
a single-layer artificial neural network with sigmoid activation and
weight decay&lt;sup id=&#34;fnref:4&#34;&gt;&lt;a href=&#34;#fn:4&#34; class=&#34;footnote-ref&#34; role=&#34;doc-noteref&#34;&gt;4&lt;/a&gt;&lt;/sup&gt;. In other words, regularized logistic regression is
equivalent to a very simple neural network architecture and hence it is
not surprising that underlying concepts can in theory be applied in much
the same way.&lt;/p&gt;
&lt;p&gt;So let&amp;rsquo;s quickly recap the next core concept: LA relies on the fact that
the second-order Taylor expansion of our loss function $\ell$ evaluated
at the &lt;strong&gt;maximum a posteriori&lt;/strong&gt; (MAP) estimate
$\mathbf{\hat{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})$
amounts to a multi-variate Gaussian distribution. In particular, that
Gaussian is centered around the MAP estimate with covariance equal to
the inverse Hessian evaluated at the mode
$\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{w}}))^{-1}$ (Murphy 2022).&lt;/p&gt;
&lt;p&gt;That is basically all there is to the story: if we have a good estimate
of $\mathbf{H}(\mathbf{\hat{w}})$ we have an analytical expression for
an (approximate) posterior over parameters. So let&amp;rsquo;s go ahead and start
by run Bayesian Logistic regression using &lt;a href=&#34;https://fluxml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flux.jl&lt;/a&gt;.
We begin by loading some required packages including
&lt;a href=&#34;https://www.paltmeyer.com/BayesLaplace.jl/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BayesLaplace.jl&lt;/a&gt;. It
ships with a helper function &lt;code&gt;toy_data_linear&lt;/code&gt; that creates a toy data
set composed of linearly separable samples evenly balanced across the
two classes.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Import libraries.&lt;/span&gt;
&lt;span style=&#34;color:#ff79c6&#34;&gt;using&lt;/span&gt; Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace
theme(&lt;span style=&#34;color:#f1fa8c&#34;&gt;:juno&lt;/span&gt;)
&lt;span style=&#34;color:#6272a4&#34;&gt;# Number of points to generate.&lt;/span&gt;
xs, y &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; toy_data_linear(&lt;span style=&#34;color:#bd93f9&#34;&gt;100&lt;/span&gt;)
X &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; hcat(xs&lt;span style=&#34;color:#ff79c6&#34;&gt;...&lt;/span&gt;); &lt;span style=&#34;color:#6272a4&#34;&gt;# bring into tabular format&lt;/span&gt;
data &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; zip(xs,y);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Then we proceed to prepare the single-layer neural network with weight
decay. The term $\lambda$ determines the strength of the $\ell2$
penalty: we regularize parameters $\theta$ more heavily for higher
values. Equivalently, we can say that from the Bayesian perspective it
governs the strength of the prior
$p(\theta) \sim \mathcal{N} \left( \theta | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)= \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \lambda_0^{-2} \mathbf{I} \right)$:
a higher value of $\lambda$ indicates a higher conviction about our
prior belief that $\theta=\mathbf{0}$, which is of course equivalent to
regularizing more heavily. The exact choice of $\lambda=0.5$ for this
toy example is somewhat arbitrary (it made for good visualizations
below). Note that I have used $\theta$ to denote our neural parameters
to distinguish the case from Bayesian logistic regression, but we are in
fact still solving the same problem.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;nn &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Chain(Dense(&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;))
λ &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;0.5&lt;/span&gt;
sqnorm(x) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; sum(abs2, x)
weight_regularization(λ&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;λ) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; λ&lt;span style=&#34;color:#ff79c6&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; sum(sqnorm, Flux&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;params(nn))
loss(x, y) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Flux&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Losses&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;logitbinarycrossentropy(nn(x), y) &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; weight_regularization();
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Before we apply Laplace approximation we train our model:&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;using&lt;/span&gt; Flux&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Optimise&lt;span style=&#34;color:#ff79c6&#34;&gt;:&lt;/span&gt; update!, ADAM
opt &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; ADAM()
epochs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;50&lt;/span&gt;

&lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; epoch &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;:&lt;/span&gt;epochs
  &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; data
    gs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; gradient(params(nn)) &lt;span style=&#34;color:#ff79c6&#34;&gt;do&lt;/span&gt;
      l &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; loss(d&lt;span style=&#34;color:#ff79c6&#34;&gt;...&lt;/span&gt;)
    &lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
    update!(opt, params(nn), gs)
  &lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Up until this point we have just followed the standard recipe for
training a regularized artificial neural network in
&lt;a href=&#34;https://fluxml.ai/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;Flux.jl&lt;/a&gt; for a simple binary classification task.
To compute the Laplace approximation using
&lt;a href=&#34;https://www.paltmeyer.com/BayesLaplace.jl/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BayesLaplace.jl&lt;/a&gt; we
need just two more lines of code:&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;la &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; laplace(nn, λ&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;λ)
fit!(la, data);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Under the hood the Hessian is approximated through the &lt;strong&gt;empirical
Fisher&lt;/strong&gt;, which can be computed using only the gradients of our loss
function $\nabla_{\theta}\ell(f(\mathbf{x}_n;\theta,y_n))$ where
${\mathbf{x}_n,y_n}$ are training data (see &lt;a href=&#34;https://arxiv.org/pdf/2106.14806.pdf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;NeurIPS 2021
paper&lt;/a&gt; for details). Finally,
&lt;a href=&#34;https://www.paltmeyer.com/BayesLaplace.jl/dev/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;BayesLaplace.jl&lt;/a&gt; ships
with a function
&lt;code&gt;predict(𝑳::LaplaceRedux, X::AbstractArray; link_approx=:probit)&lt;/code&gt; that
computes the posterior predictive using a probit approximation, much
like we saw in the previous post. That function is used under the hood
of the &lt;code&gt;plot_contour&lt;/code&gt; function below to create the right panel of
&lt;a href=&#34;#fig-logit&#34;&gt;Figure 1&lt;/a&gt;. It visualizes the posterior predictive
distribution in the 2D feature space. For comparison I have added the
corresponding plugin estimate as well. Note how for the Laplace
approximation the predicted probabilities fan out indicating that
confidence decrease in regions scarce of data.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;p_plugin &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Plugin&amp;#34;&lt;/span&gt;,type&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;:plugin&lt;/span&gt;);
p_laplace &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Laplace&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#6272a4&#34;&gt;# Plot the posterior distribution with a contour plot.&lt;/span&gt;
plt &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot(p_plugin, p_laplace, layout&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;), size&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1000&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;400&lt;/span&gt;))
savefig(plt, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;www/posterior_predictive_logit.png&amp;#34;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&#34;www/posterior_predictive_logit.png&#34; id=&#34;fig-logit&#34;
alt=&#34;Figure 1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 1: Posterior predictive
distribution of Logistic regression in the 2D feature space using plugin
estimator (left) and Laplace approximation (right).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h3 id=&#34;-to-bayesian-neural-networks&#34;&gt;&amp;hellip; to Bayesian Neural Networks&lt;/h3&gt;
&lt;p&gt;Now let&amp;rsquo;s step it up a notch: we will repeat the exercise from above,
but this time for data that is not linearly separable using a simple MLP
instead of the single-layer neural network we used above. The code below
is almost the same as above, so I will not go through the various steps
again.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;&lt;span style=&#34;color:#6272a4&#34;&gt;# Number of points to generate:&lt;/span&gt;
xs, y &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; toy_data_non_linear(&lt;span style=&#34;color:#bd93f9&#34;&gt;200&lt;/span&gt;)
X &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; hcat(xs&lt;span style=&#34;color:#ff79c6&#34;&gt;...&lt;/span&gt;); &lt;span style=&#34;color:#6272a4&#34;&gt;# bring into tabular format&lt;/span&gt;
data &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; zip(xs,y)

&lt;span style=&#34;color:#6272a4&#34;&gt;# Build MLP:&lt;/span&gt;
n_hidden &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;32&lt;/span&gt;
D &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; size(X)[&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;]
nn &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Chain(
    Dense(D, n_hidden, σ),
    Dense(n_hidden, &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;)
)  
λ &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;0.01&lt;/span&gt;
sqnorm(x) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; sum(abs2, x)
weight_regularization(λ&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;λ) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;/&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; λ&lt;span style=&#34;color:#ff79c6&#34;&gt;^&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt; &lt;span style=&#34;color:#ff79c6&#34;&gt;*&lt;/span&gt; sum(sqnorm, Flux&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;params(nn))
loss(x, y) &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; Flux&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;Losses&lt;span style=&#34;color:#ff79c6&#34;&gt;.&lt;/span&gt;logitbinarycrossentropy(nn(x), y) &lt;span style=&#34;color:#ff79c6&#34;&gt;+&lt;/span&gt; weight_regularization()

&lt;span style=&#34;color:#6272a4&#34;&gt;# Training:&lt;/span&gt;
epochs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;200&lt;/span&gt;
&lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; epoch &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; &lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;&lt;span style=&#34;color:#ff79c6&#34;&gt;:&lt;/span&gt;epochs
  &lt;span style=&#34;color:#ff79c6&#34;&gt;for&lt;/span&gt; d &lt;span style=&#34;color:#ff79c6&#34;&gt;in&lt;/span&gt; data
    gs &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; gradient(params(nn)) &lt;span style=&#34;color:#ff79c6&#34;&gt;do&lt;/span&gt;
      l &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; loss(d&lt;span style=&#34;color:#ff79c6&#34;&gt;...&lt;/span&gt;)
    &lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
    update!(opt, params(nn), gs)
  &lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
&lt;span style=&#34;color:#ff79c6&#34;&gt;end&lt;/span&gt;
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;p&gt;Fitting the Laplace approximation is also analogous, but note that this
we have added an argument: &lt;code&gt;subset_of_weights=:last_layer&lt;/code&gt;. This
specifies that we only want to use the parameters of the last layer of
our MLP. While we could have used all of them
(&lt;code&gt;subset_of_weights=:all&lt;/code&gt;), Daxberger et al. (2021) find that the
last-layer Laplace approximation produces satisfying results, while be
computationally cheaper. &lt;a href=&#34;#fig-mlp&#34;&gt;Figure 2&lt;/a&gt; demonstrates that once
again the Laplace approximation yields a posterior predictive
distribution that is more conservative than the over-confident plugin
estimate.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;la &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; laplace(nn, λ&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;λ, subset_of_weights&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;:last_layer&lt;/span&gt;)
fit!(la, data);
p_plugin &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Plugin&amp;#34;&lt;/span&gt;,type&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;:plugin&lt;/span&gt;)
p_laplace &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Laplace&amp;#34;&lt;/span&gt;)
&lt;span style=&#34;color:#6272a4&#34;&gt;# Plot the posterior distribution with a contour plot.&lt;/span&gt;
plt &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot(p_plugin, p_laplace, layout&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;), size&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1000&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;400&lt;/span&gt;))
savefig(plt, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;www/posterior_predictive_mlp.png&amp;#34;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&#34;www/posterior_predictive_mlp.png&#34; id=&#34;fig-mlp&#34;
alt=&#34;Figure 2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 2: Posterior predictive
distribution of MLP in the 2D feature space using plugin estimator
(left) and Laplace approximation (right).&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;p&gt;To see why this is a desirable outcome consider the zoomed out version
of &lt;a href=&#34;#fig-mlp&#34;&gt;Figure 2&lt;/a&gt; below: the plugin estimator classifies with full
confidence in regions completely scarce of any data. Arguably Laplace
approximation produces a much more reasonable picture, even though it
too could likely be improved by fine-tuning our choice of $\lambda$ and
the neural network architecture.&lt;/p&gt;
&lt;div class=&#34;cell&#34;&gt;
&lt;div class=&#34;highlight&#34;&gt;&lt;pre tabindex=&#34;0&#34; style=&#34;color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4&#34;&gt;&lt;code class=&#34;language-julia&#34; data-lang=&#34;julia&#34;&gt;zoom&lt;span style=&#34;color:#ff79c6&#34;&gt;=-&lt;/span&gt;&lt;span style=&#34;color:#bd93f9&#34;&gt;50&lt;/span&gt;
p_plugin &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Plugin&amp;#34;&lt;/span&gt;,type&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;:plugin&lt;/span&gt;,zoom&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;zoom);
p_laplace &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot_contour(X&lt;span style=&#34;color:#ff79c6&#34;&gt;&amp;#39;&lt;/span&gt;,y,la;title&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;&lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;Laplace&amp;#34;&lt;/span&gt;,zoom&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;zoom);
&lt;span style=&#34;color:#6272a4&#34;&gt;# Plot the posterior distribution with a contour plot.&lt;/span&gt;
plt &lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt; plot(p_plugin, p_laplace, layout&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;2&lt;/span&gt;), size&lt;span style=&#34;color:#ff79c6&#34;&gt;=&lt;/span&gt;(&lt;span style=&#34;color:#bd93f9&#34;&gt;1000&lt;/span&gt;,&lt;span style=&#34;color:#bd93f9&#34;&gt;400&lt;/span&gt;));
savefig(plt, &lt;span style=&#34;color:#f1fa8c&#34;&gt;&amp;#34;www/posterior_predictive_mlp_zoom.png&amp;#34;&lt;/span&gt;);
&lt;/code&gt;&lt;/pre&gt;&lt;/div&gt;&lt;/div&gt;
&lt;figure&gt;
&lt;img src=&#34;www/posterior_predictive_mlp_zoom.png&#34; id=&#34;fig-mlp-zoom&#34;
alt=&#34;Figure 3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out.&#34; /&gt;
&lt;figcaption aria-hidden=&#34;true&#34;&gt;Figure 3: Posterior predictive
distribution of MLP in the 2D feature space using plugin estimator
(left) and Laplace approximation (right). Zoomed out.&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;h2 id=&#34;wrapping-up&#34;&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;Recent state-of-the-art research on neural information processing
suggests that Bayesian deep learning can be effortless: Laplace
approximation for deep neural networks appears to work very well and it
does so at minimal computational cost (Daxberger et al. 2021). This is
great news, because the case for turning Bayesian is strong: society
increasingly relies on complex automated decision-making systems that
need to be trustworthy. More and more of these systems involve deep
learning which in and of itself is not trustworthy. We have seen that
typically there exist various viable parameterizations of deep neural
networks each with their own distinct and compelling explanation for the
data at hand. When faced with many viable options, don&amp;rsquo;t put all of your
eggs in one basket. In other words, go Bayesian!&lt;/p&gt;
&lt;h2 id=&#34;resources&#34;&gt;Resources&lt;/h2&gt;
&lt;p&gt;To get started with Bayesian deep learning I have found many useful and
free resources online, some of which are listed below:&lt;/p&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;https://turing.ml/dev/tutorials/03-bayesian-neural-network/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;&lt;code&gt;Turing.jl&lt;/code&gt;
tutorial&lt;/a&gt;
on Bayesian deep learning in Julia&lt;/li&gt;
&lt;li&gt;Various RStudio AI blog posts including &lt;a href=&#34;https://blogs.rstudio.com/ai/posts/2018-11-12-uncertainty_estimates_dropout/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
one&lt;/a&gt;
and &lt;a href=&#34;https://blogs.rstudio.com/ai/posts/2019-06-05-uncertainty-estimates-tfprobability/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
one&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;TensorFlow blog
post&lt;/a&gt;
on regression with probabilistic layers&lt;/li&gt;
&lt;li&gt;Kevin Murphy&amp;rsquo;s &lt;a href=&#34;https://probml.github.io/pml-book/book1.html&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;draft text
book&lt;/a&gt;, now also
available as print&lt;/li&gt;
&lt;/ul&gt;
&lt;h2 id=&#34;references&#34;&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bastounis2021mathematics&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Bastounis, Alexander, Anders C Hansen, and Verner Vlačić. 2021. &amp;ldquo;The
Mathematics of Adversarial Attacks in AI&amp;ndash;Why Deep Learning Is Unstable
Despite the Existence of Stable Neural Networks.&amp;rdquo; &lt;em&gt;arXiv Preprint
arXiv:2109.06098&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-daxberger2021laplace&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Daxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
Matthias Bauer, and Philipp Hennig. 2021. &amp;ldquo;Laplace Redux-Effortless
Bayesian Deep Learning.&amp;rdquo; &lt;em&gt;Advances in Neural Information Processing
Systems&lt;/em&gt; 34.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-gal2016dropout&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Gal, Yarin, and Zoubin Ghahramani. 2016. &amp;ldquo;Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep Learning.&amp;rdquo; In
&lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 1050&amp;ndash;59. PMLR.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-lakshminarayanan2016simple&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016.
&amp;ldquo;Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.&amp;rdquo; &lt;em&gt;arXiv Preprint arXiv:1612.01474&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-murphy2022probabilistic&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Murphy, Kevin P. 2022. &lt;em&gt;Probabilistic Machine Learning: An
Introduction&lt;/em&gt;. MIT Press.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-pearl2018book&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Pearl, Judea, and Dana Mackenzie. 2018. &lt;em&gt;The Book of Why: The New
Science of Cause and Effect&lt;/em&gt;. Basic books.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-raghunathan2019adversarial&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Raghunathan, Aditi, Sang Michael Xie, Fanny Yang, John C Duchi, and
Percy Liang. 2019. &amp;ldquo;Adversarial Training Can Hurt Generalization.&amp;rdquo;
&lt;em&gt;arXiv Preprint arXiv:1906.06032&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-slack2020fooling&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu
Lakkaraju. 2020. &amp;ldquo;Fooling Lime and Shap: Adversarial Attacks on Post Hoc
Explanation Methods.&amp;rdquo; In &lt;em&gt;Proceedings of the AAAI/ACM Conference on AI,
Ethics, and Society&lt;/em&gt;, 180&amp;ndash;86.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;ref-wilson2020case&#34; class=&#34;csl-entry&#34;&gt;
&lt;p&gt;Wilson, Andrew Gordon. 2020. &amp;ldquo;The Case for Bayesian Deep Learning.&amp;rdquo;
&lt;em&gt;arXiv Preprint arXiv:2001.10995&lt;/em&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;section class=&#34;footnotes&#34; role=&#34;doc-endnotes&#34;&gt;
&lt;hr&gt;
&lt;ol&gt;
&lt;li id=&#34;fn:1&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See for example &lt;a href=&#34;https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;this
article&lt;/a&gt;
in the MIT Technology Review&amp;#160;&lt;a href=&#34;#fnref:1&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:2&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;In fact, not treating probabilistic deep learning models as such
is sheer madness because remember that the underlying parameters
$\theta$ are random variables. Frequentists and Bayesians alike will
tell you that relying on a single point estimate of random variables
is just nuts!&amp;#160;&lt;a href=&#34;#fnref:2&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:3&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;Proponents of Causal AI like Judea Pearl would argue that the
Bayesian treatment still does not go far enough: in their view model
explanations can only be truly compelling if they are causally
found.&amp;#160;&lt;a href=&#34;#fnref:3&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;li id=&#34;fn:4&#34; role=&#34;doc-endnote&#34;&gt;
&lt;p&gt;See this &lt;a href=&#34;https://stats.stackexchange.com/a/500973/288736&#34; target=&#34;_blank&#34; rel=&#34;noopener&#34;&gt;answer&lt;/a&gt;
on Stack Exchange for a detailed discussion.&amp;#160;&lt;a href=&#34;#fnref:4&#34; class=&#34;footnote-backref&#34; role=&#34;doc-backlink&#34;&gt;&amp;#x21a9;&amp;#xfe0e;&lt;/a&gt;&lt;/p&gt;
&lt;/li&gt;
&lt;/ol&gt;
&lt;/section&gt;
</description>
    </item>
    
    <item>
      <title>Bayesian Logistic Regression</title>
      <link>https://www.paltmeyer.com/post/bayesian-logistic-regression/</link>
      <pubDate>Mon, 15 Nov 2021 00:00:00 +0000</pubDate>
      <guid>https://www.paltmeyer.com/post/bayesian-logistic-regression/</guid>
      <description>
&lt;script src=&#34;https://www.paltmeyer.com/post/bayesian-logistic-regression/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;


&lt;div id=&#34;uncertainty&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Uncertainty&lt;/h2&gt;
&lt;div class=&#34;intro-gif&#34;&gt;
&lt;figure&gt;
&lt;img src=&#34;www/toy.gif&#34;&gt;
&lt;figcaption&gt;
Simulation of changing parameter distribution.
&lt;/figcaption&gt;
&lt;/figure&gt;
&lt;/div&gt;
&lt;p&gt;If you’ve ever searched for evaluation metrics to assess model accuracy, chances are that you found many different options to choose from (too many?). Accuracy is in some sense the holy grail of prediction so it’s not at all surprising that the machine learning community spends a lot time thinking about it. In a world where more and more high-stake decisions are being automated, model accuracy is in fact a very valid concern.&lt;/p&gt;
&lt;p&gt;But does this recipe for model evaluation seem like a sound and complete approach to automated decision-making? Haven’t we forgot anything? Some would argue that we need to pay more attention to &lt;strong&gt;model uncertainty&lt;/strong&gt;. No matter how many times you have cross-validated your model, the loss metric that it is being optimized against as well as its parameters and predictions remain inherently random variables. Focusing merely on prediction accuracy and ignoring uncertainty altogether can install a false level of confidence in automated decision-making systems. Any &lt;strong&gt;trustworthy&lt;/strong&gt; approach to learning from data should therefore at the very least be transparent about its own uncertainty.&lt;/p&gt;
&lt;p&gt;How can we estimate uncertainty around model parameters and predictions? &lt;strong&gt;Frequentist&lt;/strong&gt; methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example &lt;a href=&#34;https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture26.pdf&#34;&gt;here&lt;/a&gt; for the case of logistic regression). In Bayesian statistics and machine learning we are instead concerned with modelling the &lt;strong&gt;posterior distribution&lt;/strong&gt; over model parameters. This approach to uncertainty quantification is known as &lt;strong&gt;Bayesian Inference&lt;/strong&gt; because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on &lt;strong&gt;prior&lt;/strong&gt; knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as &lt;em&gt;un&lt;/em&gt;scientific. However, frequentist methods come with their own assumptions and pitfalls (see for example &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-murphy2012machine&#34; role=&#34;doc-biblioref&#34;&gt;Murphy&lt;/a&gt; (&lt;a href=&#34;#ref-murphy2012machine&#34; role=&#34;doc-biblioref&#34;&gt;2012&lt;/a&gt;)&lt;/span&gt;) for a discussion). Without diving further into this argument, let us now see how &lt;strong&gt;Bayesian Logistic Regression&lt;/strong&gt; can be implemented from the bottom up.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-ground-truth&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The ground truth&lt;/h2&gt;
&lt;p&gt;In this post we will work with a synthetic toy data set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; composed of &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; binary labels &lt;span class=&#34;math inline&#34;&gt;\(y_n\in\{0,1\}\)&lt;/span&gt; and corresponding feature vectors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{x}_n\in \mathbb{R}^D\)&lt;/span&gt;. Working with synthetic data has the benefit that we have control over the &lt;strong&gt;ground truth&lt;/strong&gt; that generates our data. In particular, we will assume that the binary labels &lt;span class=&#34;math inline&#34;&gt;\(y_n\)&lt;/span&gt; are generated by a logistic regression model&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:logreg&#34;&gt;\[
\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; p(y_n|\mathbf{x}_n;\mathbf{w})&amp;amp;\sim\text{Ber}(y_n|\sigma(\mathbf{w}^T\mathbf{x}_n)) \\
\end{aligned}
\tag{1}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma(a)=1/(1+e^{-a})\)&lt;/span&gt; is the &lt;strong&gt;sigmoid&lt;/strong&gt; or &lt;strong&gt;logit&lt;/strong&gt; function &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;Murphy 2022&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt; Features are generated from a mixed Gaussian model.&lt;/p&gt;
&lt;p&gt;To add a little bit of life to our example we will assume that the binary labels classify samples into cats and dogs, based on their height and tail length. Figure &lt;a href=&#34;#fig:ground&#34;&gt;1&lt;/a&gt; shows the synthetic data in the two-dimensional feature domain. Following an introduction to Bayesian Logistic Regression in the next section we will use the synthetic data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; to estimate our model.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:ground&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.paltmeyer.com/post/bayesian-logistic-regression/index_files/figure-html/ground-1.png&#34; alt=&#34;Ground truth labels.&#34; width=&#34;480&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 1: Ground truth labels.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-maths&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The maths&lt;/h2&gt;
&lt;p&gt;Estimation usually boils down to finding the vector of parameters &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{w}}\)&lt;/span&gt; that maximizes the likelihood of observing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}=\{x_m:m=1,...,M\}\)&lt;/span&gt;.&lt;/p&gt;
&lt;div id=&#34;problem-setup&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Problem setup&lt;/h3&gt;
&lt;p&gt;The starting point for Bayesian Logistic Regression is &lt;strong&gt;Bayes’ Theorem&lt;/strong&gt;:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:posterior&#34;&gt;\[
\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; p(\mathbf{w}|\mathcal{D})&amp;amp;\propto p(\mathcal{D}|\mathbf{w})p(\mathbf{w}) \\
\end{aligned}
\tag{2}
\end{equation}
\]&lt;/span&gt;
Formally, this says that the posterior distribution of parameters &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; is proportional to the product of the likelihood of observing &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt; given &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; and the prior density of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt;. Applied to our context this can intuitively be understood as follows: our posterior beliefs around &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; are formed by both our prior beliefs and the evidence we observe. Yet another way to look at this is that maximising &lt;a href=&#34;#eq:posterior&#34;&gt;(2)&lt;/a&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; corresponds to maximum likelihood estimation regularized by prior beliefs (we will come back to this).&lt;/p&gt;
&lt;p&gt;Under the assumption that individual label-feature pairs are &lt;strong&gt;independently&lt;/strong&gt; and &lt;strong&gt;identically&lt;/strong&gt; distributed, their joint likelihood is simply the product over their individual densities. The prior beliefs around &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; are at our discretion. In practice they may be derived from previous experiments. Here we will use a zero-mean spherical Gaussian prior for reasons explained further below. To sum this up we have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:prior&#34;&gt;\[
\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; p(\mathcal{D}|\mathbf{w})&amp;amp; \sim \prod_{n=1}^N p(y_n|\mathbf{x}_n;\mathbf{w})\\
&amp;amp;&amp;amp; p(\mathbf{w})&amp;amp; \sim \mathcal{N} \left( \mathbf{w} | \mathbf{w}_0, \Sigma_0 \right) \\
\end{aligned}
\tag{3}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;with &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}_0=\mathbf{0}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\Sigma_0=\sigma^2\mathbf{I}\)&lt;/span&gt;. Plugging this into Bayes’ rule we finally have&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34;&gt;\[
\begin{aligned}
&amp;amp;&amp;amp; p(\mathbf{w}|\mathcal{D})&amp;amp;\propto\prod_{n=1}^N \text{Ber}(y_n|\sigma(\mathbf{w}^T\mathbf{x}_n))\mathcal{N} \left( \mathbf{w} | \mathbf{w}_0, \Sigma_0 \right) \\
\end{aligned}
\]&lt;/span&gt;&lt;/p&gt;
&lt;p&gt;Unlike with linear regression there are no closed-form analytical solutions to estimating or maximising this posterior, but fortunately accurate approximations do exist &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;Murphy 2022&lt;/a&gt;)&lt;/span&gt;. One of the simplest approaches called &lt;strong&gt;Laplace Approximation&lt;/strong&gt; is straight-forward to implement and computationally very efficient. It relies on the observation that under the assumption of a Gaussian prior, the posterior of logistic regression is also approximately Gaussian: in particular, this Gaussian distribution is centered around the &lt;strong&gt;maximum a posteriori&lt;/strong&gt; (MAP) estimate &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})\)&lt;/span&gt; with a covariance matrix equal to the inverse Hessian evaluated at the mode &lt;span class=&#34;math inline&#34;&gt;\(\hat{\Sigma}=(\mathbf{H}(\hat{\mathbf{w}}))^{-1}\)&lt;/span&gt;. With that in mind, finding &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{w}}\)&lt;/span&gt; seems like a natural next step.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;solving-the-problem&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Solving the problem&lt;/h3&gt;
&lt;p&gt;In practice we do not maximize the posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\mathbf{w}|\mathcal{D})\)&lt;/span&gt; directly. Instead we minimize the negative log likelihood, which is an equivalent optimization problem and easier to implement. In &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt; below I have denoted the negative log likelihood as &lt;span class=&#34;math inline&#34;&gt;\(\ell(\mathbf{w})\)&lt;/span&gt; indicating that this is the &lt;strong&gt;loss function&lt;/strong&gt; we aim to minimize. The following two lines in &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt; show the gradient and Hessian - so the first- and second-order derivatives of &lt;span class=&#34;math inline&#34;&gt;\(\ell\)&lt;/span&gt; with respect to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; - where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}_0=\Sigma_0^{-1}\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(\mu_n=\sigma(\mathbf{w}^T\mathbf{x}_n)\)&lt;/span&gt;. To understand how exactly the gradient and Hessian are derived see for example chapter 10 in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;Murphy&lt;/a&gt; (&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:likeli&#34;&gt;\[
\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; \ell(\mathbf{w})&amp;amp;=- \sum_{n=1}^{N} [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0) \\
&amp;amp;&amp;amp; \nabla_{\mathbf{w}}\ell(\mathbf{w})&amp;amp;= \sum_{n=1}^{N} (\mu_n-y_n) \mathbf{x}_n + \mathbf{H}_0(\mathbf{w}-\mathbf{w}_0) \\
&amp;amp;&amp;amp; \nabla^2_{\mathbf{w}}\ell(\mathbf{w})&amp;amp;= \sum_{n=1}^{N} (\mu_n-y_n) \left( \mu_n(1-\mu_n) \mathbf{x}_n \mathbf{x}_n^T \right) + \mathbf{H}_0\\
\end{aligned}
\tag{4}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;sidenote&#34;&gt;
&lt;p&gt;&lt;strong&gt;SIDENOTE&lt;/strong&gt; 💡&lt;/p&gt;
&lt;p&gt;Note how earlier I mentioned that maximising the posterior likelihood can be seen as regularized maximum likelihood estimation. We can now make that connection explicit: in &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt; let us assume that &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}_0=\mathbf{0}\)&lt;/span&gt;. Then since &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{H}_0=\lambda\mathbf{I}\)&lt;/span&gt; with &lt;span class=&#34;math inline&#34;&gt;\(1/\sigma^2\)&lt;/span&gt; the second term in the first line is simply &lt;span class=&#34;math inline&#34;&gt;\(\lambda \frac{1}{2} \mathbf{w}^T\mathbf{w}=\lambda \frac{1}{2} ||\mathbf{w}||_2^2\)&lt;/span&gt;. This is equivalent to running logistic regression with an &lt;span class=&#34;math inline&#34;&gt;\(\ell_2\)&lt;/span&gt;-penalty &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;Since minimizing the loss function in &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt; is a convex optimization problem we have many efficient algorithms to choose from in order to solve this problem. With the Hessian at hand it seems natural to use a second-order method, because incorporating information about the curvature of the loss function generally leads to faster convergence. Here we will implement &lt;strong&gt;Newton’s method&lt;/strong&gt; in line with the presentation in chapter 8 of &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;Murphy&lt;/a&gt; (&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;posterior-predictive&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Posterior predictive&lt;/h3&gt;
&lt;p&gt;Suppose now that we have trained the Bayesian Logistic Regression model as our binary classifier &lt;span class=&#34;math inline&#34;&gt;\(g_N(\mathbf{x})\)&lt;/span&gt; using our training data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;. A new unlabelled sample &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{x}_{N+1},?)\)&lt;/span&gt; arrives. As with any binary classifier we can predict the missing label by simply plugging the new sample into our classifier &lt;span class=&#34;math inline&#34;&gt;\(\hat{y}_{N+1}=g_N(\mathbf{x}_{N+1})=\sigma(\hat{\mathbf{w}}^T\mathbf{x}_{N+1})\)&lt;/span&gt;, where &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{w}}\)&lt;/span&gt; is the MAP estimate as before. If at training phase we have found &lt;span class=&#34;math inline&#34;&gt;\(g_N(\mathbf{x})\)&lt;/span&gt; to achieve good accuracy, we may expect &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{x}_{N+1},\hat{y}_{N+1})\)&lt;/span&gt; to be a reasonably good approximation of the true and unobserved pair &lt;span class=&#34;math inline&#34;&gt;\((\mathbf{x}_{N+1},y_{N+1})\)&lt;/span&gt;. But since we are still dealing with an expected value of a random variable, we would generally like to have an idea of how noisy this prediction is.&lt;/p&gt;
&lt;p&gt;Formally, we are interested in the &lt;strong&gt;posterior predictive&lt;/strong&gt; distribution:&lt;/p&gt;
&lt;p&gt;&lt;span class=&#34;math display&#34; id=&#34;eq:posterior-pred&#34;&gt;\[
\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; p(y=1|\mathbf{x}, \mathcal{D})&amp;amp;= \int \sigma(\mathbf{w}^T \mathbf{x})p(\mathbf{w}|\mathcal{D})d\mathbf{w} \\
\end{aligned}
\tag{5}
\end{equation}
\]&lt;/span&gt;&lt;/p&gt;
&lt;div class=&#34;sidenote&#34;&gt;
&lt;p&gt;&lt;strong&gt;SIDENOTE&lt;/strong&gt; 💡&lt;/p&gt;
&lt;p&gt;The approach that ignores uncertainty altogether corresponds to what is referred to as &lt;strong&gt;plugin&lt;/strong&gt; approximation of the posterior predictive. Formally, it imposes &lt;span class=&#34;math inline&#34;&gt;\(p(y=1|\mathbf{x}, \mathcal{D})\approx p(y=1|\mathbf{x}, \hat{\mathbf{w}})\)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;p&gt;With the posterior distribution over model parameters &lt;span class=&#34;math inline&#34;&gt;\(p(\mathbf{w}|\mathcal{D})\)&lt;/span&gt; at hand we have the necessary ingredients to estimate the posterior predictive distribution &lt;span class=&#34;math inline&#34;&gt;\(p(y=1|\mathbf{x}, \mathcal{D})\)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;An obvious, but computationally expensive way to estimate it is through Monte Carlo: draw &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}_s\)&lt;/span&gt; from &lt;span class=&#34;math inline&#34;&gt;\(p(\mathbf{w}|\mathcal{D})\)&lt;/span&gt; for &lt;span class=&#34;math inline&#34;&gt;\(s=1:S\)&lt;/span&gt; and compute fitted values &lt;span class=&#34;math inline&#34;&gt;\(\sigma(\mathbf{w_s}^T\mathbf{x})\)&lt;/span&gt; each. Then the posterior predictive distribution corresponds to the average over all fitted values, &lt;span class=&#34;math inline&#34;&gt;\(p(y=1|\mathbf{x}, \mathcal{D})=1/S \sum_{s=1}^{S}\sigma(\mathbf{w_s}^T\mathbf{x})\)&lt;/span&gt;. By the law of large numbers the Monte Carlo estimate is an accurate estimate of the true posterior predictive for large enough &lt;span class=&#34;math inline&#34;&gt;\(S\)&lt;/span&gt;. Of course, “large enough” is somewhat loosely defined here and depending on the problem can mean “very large.” Consequently, the computational costs involved essentially know no upper bound.&lt;/p&gt;
&lt;p&gt;Fortunately, it turns out that we can trade off a little bit of accuracy in return for a convenient analytical solution. In particular, we have that &lt;span class=&#34;math inline&#34;&gt;\(\sigma(a) \approx \Phi(\lambda a)\)&lt;/span&gt; where &lt;span class=&#34;math inline&#34;&gt;\(\Phi(.)\)&lt;/span&gt; is the standard Gaussian cdf and &lt;span class=&#34;math inline&#34;&gt;\(\lambda=\pi/8\)&lt;/span&gt; ensures that the two functions have the same slope at the origin (Figure &lt;a href=&#34;#fig:probit&#34;&gt;2&lt;/a&gt;). Without dwelling further on the details we can use this finding to approximate the integral in &lt;a href=&#34;#eq:posterior-pred&#34;&gt;(5)&lt;/a&gt; as a sigmoid function. This is called &lt;strong&gt;probit approximation&lt;/strong&gt; and implemented below.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:probit&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;https://www.paltmeyer.com/post/bayesian-logistic-regression/index_files/figure-html/probit-1.png&#34; alt=&#34;Demonstration of the probit approximation.&#34; width=&#34;672&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 2: Demonstration of the probit approximation.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;the-code&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The code&lt;/h2&gt;
&lt;p&gt;We now have all the necessary ingredients to code Bayesian Logistic Regression up from scratch. While in practice we would usually want to rely on existing packages that have been properly tested, I often find it very educative and rewarding to program algorithms from the bottom up. You will see that Julia’s syntax so closely resembles the mathematical formulas we have seen above, that going from maths to code is incredibly easy. Seeing those formulas and algorithms then actually doing their magic is quite fun! The code chunk below, for example, shows the implementation of the loss function and its derivatives from &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt; above. Take a moment to go through the code line-by-line and try to understand how it relates back to the equations in &lt;a href=&#34;#eq:likeli&#34;&gt;(4)&lt;/a&gt;. Isn’t it amazing how closely the code resembles the actual equations?&lt;/p&gt;
&lt;script src=&#34;https://gist.github.com/pat-alt/cc53a11470e4fb736f24bb6de2393f54.js&#34;&gt;&lt;/script&gt;
&lt;p&gt;Aside from the optimization routine this is essentially all there is to coding up Bayesian Logistic Regression from scratch in Julia Language. If you are curious to see the full source code in detail you can check out this &lt;a href=&#34;https://colab.research.google.com/github/pat-alt/pat-alt.github.io/blob/main/content/post/2021-11-15-bayesian-logistic-regression/julia_implementation.ipynb&#34;&gt;interactive notebook&lt;/a&gt;. Now let us finally turn back to our synthetic data and see how Bayesian Logistic Regression can help us understand the uncertainty around our model predictions.&lt;/p&gt;
&lt;div class=&#34;disclaimer&#34;&gt;
&lt;p&gt;&lt;strong&gt;DISCLAIMER&lt;/strong&gt; ❗️&lt;/p&gt;
&lt;p&gt;I should mention that this is the first time I program in Julia, so for any Julia pros out there: please bear with me! Happy to hear your suggestions/comments.&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;&lt;br&gt;&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;the-estimates&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;The estimates&lt;/h2&gt;
&lt;p&gt;Figure &lt;a href=&#34;#fig:posterior&#34;&gt;3&lt;/a&gt; below shows the resulting posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(w_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_3\)&lt;/span&gt; at varying degrees of prior uncertainty &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;. The constant &lt;span class=&#34;math inline&#34;&gt;\(w_1\)&lt;/span&gt; is held constant at the mode (&lt;span class=&#34;math inline&#34;&gt;\(\hat{w}_1\)&lt;/span&gt;). The red dot indicates the MLE. Note how for the choice of &lt;span class=&#34;math inline&#34;&gt;\(\sigma\rightarrow 0\)&lt;/span&gt; the posterior is equal to the prior. This is intuitive since we have imposed that we have no uncertainty around our prior beliefs and hence no amount of new evidence can move us in any direction. Conversely, for &lt;span class=&#34;math inline&#34;&gt;\(\sigma \rightarrow \infty\)&lt;/span&gt; the posterior distribution is centered around the unconstrained MLE: prior knowledge is very uncertain and hence the posterior is dominated by the likelihood of the data.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:posterior&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;www/posterior.png&#34; alt=&#34;Posterior distribution for $w_2$ and $w_3$ at varying degrees of prior uncertainty $\sigma$.&#34; width=&#34;750&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 3: Posterior distribution for &lt;span class=&#34;math inline&#34;&gt;\(w_2\)&lt;/span&gt; and &lt;span class=&#34;math inline&#34;&gt;\(w_3\)&lt;/span&gt; at varying degrees of prior uncertainty &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;p&gt;What about the posterior predictive? The story is similar: since for &lt;span class=&#34;math inline&#34;&gt;\(\sigma\rightarrow 0\)&lt;/span&gt; the posterior is completely dominated by the zero-mean prior we have &lt;span class=&#34;math inline&#34;&gt;\(p(y=1|\mathbf{x},\hat{\mathbf{w}})=0.5\)&lt;/span&gt; everywhere (top left panel in Figure &lt;a href=&#34;#fig:predictive&#34;&gt;4&lt;/a&gt;). As we gradually increase uncertainty around our prior the predictive posterior depends more and more on the data &lt;span class=&#34;math inline&#34;&gt;\(\mathcal{D}\)&lt;/span&gt;: uncertainty around predicted labels is high only in regions that are not populated by samples &lt;span class=&#34;math inline&#34;&gt;\((y_n, \mathbf{x}_n)\)&lt;/span&gt;. Not surprisingly, this effect is strongest for the MLE (&lt;span class=&#34;math inline&#34;&gt;\(\sigma\rightarrow \infty\)&lt;/span&gt;) where we see some evidence of overfitting.&lt;/p&gt;
&lt;div class=&#34;figure&#34; style=&#34;text-align: center&#34;&gt;&lt;span style=&#34;display:block;&#34; id=&#34;fig:predictive&#34;&gt;&lt;/span&gt;
&lt;img src=&#34;www/predictive.png&#34; alt=&#34;Predictive posterior distribution at varying degrees of prior uncertainty $\sigma$.&#34; width=&#34;750&#34; /&gt;
&lt;p class=&#34;caption&#34;&gt;
Figure 4: Predictive posterior distribution at varying degrees of prior uncertainty &lt;span class=&#34;math inline&#34;&gt;\(\sigma\)&lt;/span&gt;.
&lt;/p&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;wrapping-up&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;Wrapping up&lt;/h2&gt;
&lt;p&gt;In this post we have seen how Bayesian Logistic Regression can be implemented from scratch in Julia language. The estimated posterior distribution over model parameters can be used to quantify uncertainty around coefficients and model predictions. I have argued that it is important to be transparent about model uncertainty to avoid being overly confident in estimates.&lt;/p&gt;
&lt;p&gt;There are many more benefits associated with Bayesian (probabilistic) machine learning. Understanding where in the input domain our model exerts high uncertainty can for example be instrumental in labelling data: see for example &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-gal2017deep&#34; role=&#34;doc-biblioref&#34;&gt;Gal, Islam, and Ghahramani&lt;/a&gt; (&lt;a href=&#34;#ref-gal2017deep&#34; role=&#34;doc-biblioref&#34;&gt;2017&lt;/a&gt;)&lt;/span&gt; and follow-up works for an interesting application to &lt;strong&gt;active learning&lt;/strong&gt; for image data. Similarly, there is a recent work that uses estimates of the posterior predictive in the context of &lt;strong&gt;algorithmic recourse&lt;/strong&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-schut2021generating&#34; role=&#34;doc-biblioref&#34;&gt;Schut et al. 2021&lt;/a&gt;)&lt;/span&gt;. For a brief introduction to algorithmic recourse see one of my &lt;a href=&#34;../2021-04-26-individual-recourse-for-black-box-models/index.html&#34;&gt;previous posts&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;As a great reference for further reading about probabilistic machine learning I can highly recommend &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;Murphy&lt;/a&gt; (&lt;a href=&#34;#ref-murphy2022probabilistic&#34; role=&#34;doc-biblioref&#34;&gt;2022&lt;/a&gt;)&lt;/span&gt;. An electronic version of the book is currently freely available as a draft. Finally, remember that if you want to try yourself at the code, you can check out this &lt;a href=&#34;https://colab.research.google.com/github/pat-alt/pat-alt.github.io/blob/main/content/post/2021-11-15-bayesian-logistic-regression/julia_implementation.ipynb&#34;&gt;interactive notebook&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level2&#34;&gt;
&lt;h2&gt;References&lt;/h2&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. springer.
&lt;/div&gt;
&lt;div id=&#34;ref-gal2017deep&#34; class=&#34;csl-entry&#34;&gt;
Gal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. &lt;span&gt;“Deep Bayesian Active Learning with Image Data.”&lt;/span&gt; In &lt;em&gt;International Conference on Machine Learning&lt;/em&gt;, 1183–92. PMLR.
&lt;/div&gt;
&lt;div id=&#34;ref-murphy2012machine&#34; class=&#34;csl-entry&#34;&gt;
Murphy, Kevin P. 2012. &lt;em&gt;Machine Learning: A Probabilistic Perspective&lt;/em&gt;. MIT press.
&lt;/div&gt;
&lt;div id=&#34;ref-murphy2022probabilistic&#34; class=&#34;csl-entry&#34;&gt;
———. 2022. &lt;em&gt;Probabilistic Machine Learning: An Introduction&lt;/em&gt;. MIT Press.
&lt;/div&gt;
&lt;div id=&#34;ref-schut2021generating&#34; class=&#34;csl-entry&#34;&gt;
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. &lt;span&gt;“Generating Interpretable Counterfactual Explanations by Implicit Minimisation of Epistemic and Aleatoric Uncertainties.”&lt;/span&gt; In &lt;em&gt;International Conference on Artificial Intelligence and Statistics&lt;/em&gt;, 1756–64. PMLR.
&lt;/div&gt;
&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;We let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}=(10, 0.75, -2.5)^T\)&lt;/span&gt; define the true coefficients.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;Note that the author works with the negative log likelihood scaled by the sample size&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>A peek inside the &#39;Black Box&#39; - interpreting neural networks</title>
      <link>https://www.paltmeyer.com/post/2021-02-01-a-peek-inside-the-black-box-interpreting-neural-networks/</link>
      <pubDate>Sun, 07 Feb 2021 00:00:00 +0000</pubDate>
      <guid>https://www.paltmeyer.com/post/2021-02-01-a-peek-inside-the-black-box-interpreting-neural-networks/</guid>
      <description>
&lt;script src=&#34;https://www.paltmeyer.com/post/2021-02-01-a-peek-inside-the-black-box-interpreting-neural-networks/index_files/header-attrs/header-attrs.js&#34;&gt;&lt;/script&gt;

&lt;div id=&#34;TOC&#34;&gt;
&lt;ul&gt;
&lt;li&gt;&lt;a href=&#34;#motivation&#34;&gt;Motivation&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpretable-dl&#34;&gt;Interpretable DL - a whistle-stop tour&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#rate&#34;&gt;An entropy-based approach to variable importance&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#interpreting-bnns&#34;&gt;Application to Bayesian neural networks&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#conclusion&#34;&gt;Conclusion&lt;/a&gt;&lt;/li&gt;
&lt;li&gt;&lt;a href=&#34;#references&#34;&gt;References&lt;/a&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;/div&gt;

&lt;div id=&#34;motivation&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Motivation&lt;/h1&gt;
&lt;p&gt;Propelled by advancements in modern computer technology, deep learning has re-emerged as perhaps the most promising artificial intelligence (AI) technology of the last two decades. By treating problems as a nested, hierarchy of hidden layers deep artificial neural networks achieve the power and flexibility necessary for AI systems to navigate complex real-world environments. Unfortunately, their very nature has earned them a reputation as &lt;em&gt;Black Box&lt;/em&gt; algorithms and their lack of interpretability remains a major impediment to their more wide-spread application.&lt;/p&gt;
&lt;p&gt;In science, research questions usually demand not just answers but also explanations and variable selection is often as important as prediction &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al. 2019&lt;/a&gt;)&lt;/span&gt;. Economists, for example, recognise the undeniable potential of deep learning, but are rightly hesitant to employ novel tools that are not fully transparent and ultimately cannot be trusted. Similarly, real-world applications of AI have come under increasing scrutiny with regulators imposing that individuals influenced by algorithms should have the right to obtain explanations &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang 2020&lt;/a&gt;)&lt;/span&gt;. In high-risk decision-making fields such as AI systems that drive autonomous vehicles the need for explanations is self-evident &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al. 2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;In light of these challenges it is not surprising that research on explainable AI has recently gained considerable momentum &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-arrieta2020explainable&#34; role=&#34;doc-biblioref&#34;&gt;Arrieta et al. 2020&lt;/a&gt;)&lt;/span&gt;. While in this short essay we will focus on deep learning in particular, it should be noted that this growing body of literature is concerned with a broader realm of machine learning models. The rest of this note is structured as follows: the first section provides a brief overview of recent advancements towards interpreting deep neural networks largely drawing on &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang&lt;/a&gt; (&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt;; the second section considers a novel entropy-based approach towards interpretability proposed by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;; finally, in the last section we will see how this approach can be applied to deep neural networks as proposed in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpretable-dl&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Interpretable DL - a whistle-stop tour&lt;/h1&gt;
&lt;p&gt;Before delving further into &lt;em&gt;how&lt;/em&gt; the intrinsics of deep neural networks can be disentangled we should first clarify &lt;em&gt;what&lt;/em&gt; interpretability in the context of algorithms actually means. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang&lt;/a&gt; (&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; describes model interpretability simply as the extent to which humans can “understand and reason” the model. This may concern an understanding of both the &lt;em&gt;ad-hoc&lt;/em&gt; workings of the algorithm as well as the &lt;em&gt;post-hoc&lt;/em&gt; interpretability of its output. In the context of linear regression, for example, &lt;em&gt;ad-hoc&lt;/em&gt; workings of the model are often described through the intuitive idea of linearly projecting the outcome variable &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; onto the column space of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;. &lt;em&gt;Post-hoc&lt;/em&gt; interpretations usually center around variable importance – the main focus of the following sections. Various recent advancements tackle interpretability of DNNs from different angles depending on whether the focus is on &lt;em&gt;ad-hoc&lt;/em&gt; or &lt;em&gt;post-hoc&lt;/em&gt; interpretability. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang&lt;/a&gt; (&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;2020&lt;/a&gt;)&lt;/span&gt; further asses that model interpretability hinges on three main aspects of &lt;em&gt;simulatability&lt;/em&gt;, &lt;em&gt;decomposability&lt;/em&gt; and &lt;em&gt;algorithmic transparency&lt;/em&gt;, but for the purpose of this short note the &lt;em&gt;ad-hoc&lt;/em&gt; vs. &lt;em&gt;post-hoc&lt;/em&gt; taxonomy provides a simpler more natural framework.&lt;a href=&#34;#fn1&#34; class=&#34;footnote-ref&#34; id=&#34;fnref1&#34;&gt;&lt;sup&gt;1&lt;/sup&gt;&lt;/a&gt;&lt;/p&gt;
&lt;p&gt;Understanding the &lt;em&gt;ad-hoc&lt;/em&gt; intrinsic mechanisms of a DNN is inherently difficult. While generally transparency may be preserved in the presence of nonlinearity (e.g. decision trees), multiple hidden layers of networks (each of them) involving nonlinear operations are usually out of the realm of human comprehension &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang 2020&lt;/a&gt;)&lt;/span&gt;. Training also generally involves optimization of non-convex functions that involve an increasing number of saddle points as the dimensionality increases &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang 2020&lt;/a&gt;)&lt;/span&gt;. Methods to circumvent this problematic usually boil down to decreasing the overall complexity, either by regularizing the model or through proxy methods. Regularization – while traditionally done to avoid overfitting – has been found to be useful to create more interpretable representations. Monotonicity constraints, for example, impose that as the value of a specified covariate increases model predictions either monotonically decrease or increase. Proxy methods construct simpler representations of a learned DNN, such as a rule-based decision tree. This essentially involves repeatedly querying the trained network while varying the inputs and then deriving decision rules based on the model output.&lt;/p&gt;
&lt;p&gt;Post-hoc interpretability usually revolves around the understanding of feature importance. A greedy approach to this issue involves simply removing features one by one and checking how model predictions change. A more sophisticated approach along these lines is &lt;em&gt;Shapley&lt;/em&gt; value, which draws on cooperative game theory. The Shapley value assigns varying payouts to players depending on their contribution to overall payout. In the context of neural networks input covariate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_p\)&lt;/span&gt; represents a player while overall payout is represented by the difference between average and individual outcome predictions.&lt;a href=&#34;#fn2&#34; class=&#34;footnote-ref&#34; id=&#34;fnref2&#34;&gt;&lt;sup&gt;2&lt;/sup&gt;&lt;/a&gt; Exact computations of Shapley values are prohibitive as the dimensionality increases, though approximate methods have recently been developed &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-fan2020interpretability&#34; role=&#34;doc-biblioref&#34;&gt;Fan, Xiong, and Wang 2020&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The remainder of this note focuses on a novel approach to feature extraction that measures entropy shifts in a learned probabilistic neural network in response to model inputs &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X_1},...,\mathbf{X}_P\)&lt;/span&gt;. We will first introduce this methodology in the context of Gaussian Process regression in the following section before finally turning to its application to Bayesian neural networks.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;rate&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;An entropy-based approach to variable importance&lt;/h1&gt;
&lt;p&gt;&lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; motivate their methodology for interpreting neural networks through Gaussian Process regression. Consider the following Bayesian regression model with Gaussian priors:&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:bayes&#34;&gt;\[\begin{equation}
\begin{aligned}
&amp;amp;&amp;amp; f(\mathbf{X}|\mathbf{w})&amp;amp;=\phi(\mathbf{X})^T\mathbf{w} + \varepsilon, &amp;amp;&amp;amp;\varepsilon \sim \mathcal{N}(0,\mathbf{I}) \\
&amp;amp;&amp;amp; \mathbf{w}&amp;amp; \sim \mathcal{N}(0,{1\over{\lambda}} \mathbf{I})\\
\end{aligned}
\tag{1}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;This naturally gives rise to a particular example of a Gaussian Process (GP). In particular, since &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}(\mathbf{X})=\Phi(\mathbf{X})^T\mathbf{w}\)&lt;/span&gt; is just a linear combination fo Gaussian random variables it follows a Gaussian Process itself&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:khbs&#34;&gt;\[\begin{equation}
\begin{aligned}
&amp;amp;&amp;amp; \mathbf{u}(\mathbf{X})=\Phi(\mathbf{X})^T\mathbf{w}&amp;amp; \sim \mathcal{N}(\mathbf{0}, \mathbf{K}) \\
\end{aligned}
\tag{2}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{K}\)&lt;/span&gt; is the Kernel (or Gram) matrix and &lt;span class=&#34;math inline&#34;&gt;\(K_{i,j}=k(\mathbf{X_i,\mathbf{X}_j})={1\over{\lambda}}\phi(\mathbf{X_i})^T\phi(\mathbf{X_m})\)&lt;/span&gt; is the kernel function &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-bishop2006pattern&#34; role=&#34;doc-biblioref&#34;&gt;Bishop 2006&lt;/a&gt;)&lt;/span&gt;. In other words, the prior distribution over &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}\)&lt;/span&gt; induces a probability distribution over random functions &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}(\mathbf{X})\)&lt;/span&gt;. Similarly, the GP can be understood as a prior distribution over a an infinite-dimensional reproducible kernel Hilbert space (RKHS) &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al. 2019&lt;/a&gt;)&lt;/span&gt;, which in a finite-dimensional setting becomes multivariate Gaussian.&lt;/p&gt;
&lt;p&gt;In a standard linear regression model coefficients characterize the projection of the outcome variable &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; onto the column space of the regressors &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;. In particular, with ordinary least square we define:&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:ols&#34;&gt;\[\begin{equation}
\begin{aligned}
&amp;amp;&amp;amp; \beta&amp;amp;=(\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{y} \\
\end{aligned}
\tag{3}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;The primary focus here is to learn the mapping from input to output. The key differentiating feature between this approach and the non-parametric model in &lt;a href=&#34;#eq:bayes&#34;&gt;(1)&lt;/a&gt; is the fact that in case of the latter we are interested in learning not only the mapping from inputs to outputs, but also the representation (&lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}(\mathbf{X})\)&lt;/span&gt;) of the inputs (see for example &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goodfellow2016deep&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow, Bengio, and Courville 2016&lt;/a&gt;)&lt;/span&gt;). To be even more specific, treating the feature representation itself as random as in &lt;a href=&#34;#eq:bayes&#34;&gt;(1)&lt;/a&gt; allows us to learn non-linear relationships between the covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;, since they are implicitly captured by the RKHS &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al. 2019&lt;/a&gt;)&lt;/span&gt;. Neural networks share this architecture and hence it is worth dwelling on it a bit further: the fact that the learned model inherently incorporates variable interactions leads to the observation that an individual feature is rarely important on its own with respect to the mapping from &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{y}\)&lt;/span&gt; &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al. 2019&lt;/a&gt;)&lt;/span&gt;. Hence, in order to gain an understanding of individual variable importance, one should aim to understand what role feature &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt; plays &lt;em&gt;within&lt;/em&gt; the learned model, thereby taking into account its interactions with other covariates. Formally, &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and define the &lt;em&gt;effect size analogue&lt;/em&gt; as the equivalent of the familiar regression coefficient in the non-parametric setting&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:effect-size&#34;&gt;\[\begin{equation}
\begin{aligned}
&amp;amp;&amp;amp; \tilde\beta&amp;amp;=\mathbf{X}^+\Phi^T\mathbf{w}=\mathbf{X}^+\mathbf{u} \\
\end{aligned}
\tag{4}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}^+=\lim_{\alpha} (\mathbf{X}^T\mathbf{X}+\alpha \mathbf{I})^{-1}\mathbf{X}^T\)&lt;/span&gt; denotes the Moore-Penrose pseudo-inverse (see for example &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-goodfellow2016deep&#34; role=&#34;doc-biblioref&#34;&gt;Goodfellow, Bengio, and Courville&lt;/a&gt; (&lt;a href=&#34;#ref-goodfellow2016deep&#34; role=&#34;doc-biblioref&#34;&gt;2016&lt;/a&gt;)&lt;/span&gt;). Intuitively the effect size analogue can be thought of as the resulting coefficients from regressing the fitted values &lt;span class=&#34;math inline&#34;&gt;\(\hat{\mathbf{u}}\)&lt;/span&gt; from the learned probabilistic model on the covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;. It can be interpreted in the same way as linear regression coefficients, in the sense that &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta_j\)&lt;/span&gt; describes the marginal change in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}\)&lt;/span&gt; given a unit increase in &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt; holding all else constant. Note here the subtle, but crucial difference between &lt;a href=&#34;#eq:ols&#34;&gt;(3)&lt;/a&gt; – a projection from the outcome variable onto the column space of &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; – and &lt;a href=&#34;#eq:effect-size&#34;&gt;(4)&lt;/a&gt; – a projection from the learned model to &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt;. In other words, looking at &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta\)&lt;/span&gt; can be thought of peeking directly into the &lt;em&gt;Block Box&lt;/em&gt;. Unfortunately, as &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; point out, working with &lt;a href=&#34;#eq:effect-size&#34;&gt;(4)&lt;/a&gt; is usually not straight-forward. From a practitioner’s point of view, it may also not be obvious how to interpret a coefficient that describes marginal effects of input variables on a learned model. A more useful indicator in this context would provide a measure of how much individual variables contribute to the overall variation in the learned model. For this purpose &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; propose to work with a distributional centrality measure based on &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta\)&lt;/span&gt;, which we shall turn to next.&lt;/p&gt;
&lt;p&gt;The proposed methodology in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; depends on the availability of a posterior distribution over &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta\)&lt;/span&gt; in that it measures its entropic shifts in response to the introduction of covariates. The intuition is straight-forward: within the context of the learned probabilistic model is covariate &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt; informative or not? More formally this boils down to determining if the posterior distribution of &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde\beta_{-j})\)&lt;/span&gt; is dependent on the effect of &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta_j\)&lt;/span&gt;. This can be quantified through the Kullback-Leibler divergence (KLD) between &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde\beta_{-j})\)&lt;/span&gt; and the conditional posterior &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde\beta_{-j}|\tilde\beta_j)\)&lt;/span&gt;:&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:kld&#34;&gt;\[\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; \text{KLD}_j&amp;amp;=\text{KL}\left(p(\tilde\beta_{-j}) || p(\tilde\beta_{-j}|\tilde\beta_j)\right) \\
\end{aligned}
\tag{5}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;Covariates that contribute significant information to the model will have &lt;span class=&#34;math inline&#34;&gt;\(\text{KLD}&amp;gt;0\)&lt;/span&gt;, while for insignificant covariates &lt;span class=&#34;math inline&#34;&gt;\(\text{KLD}\approx0\)&lt;/span&gt;. The measure of induced entropy change gives rise to a ranking of the covariates in terms of their relative importance in the model. The RATE criterion of variable &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt; is then simply defined as&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:rate&#34;&gt;\[\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; \gamma_j&amp;amp;=\frac{\text{KLD}_j}{\sum_{p=1}^{P}\text{KLD}_p}\in[0,1] \\
\end{aligned}
\tag{6}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;which in light of its bounds can naturally be interpreted as &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_j\)&lt;/span&gt;`s percentage contribution to the learned model. It is worth noting that &lt;span class=&#34;math inline&#34;&gt;\(p(\tilde\beta_{-j}|\tilde\beta_j)\)&lt;/span&gt; of course depends on the value of the conditioning variable. A natural choice is &lt;span class=&#34;math inline&#34;&gt;\(\tilde\beta_j=0\)&lt;/span&gt; which usually corresponds to the null hypothesis.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;interpreting-bnns&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Application to Bayesian neural networks&lt;/h1&gt;
&lt;p&gt;In order to use the RATE criterion in the context of deep learning we need to work in the Bayesian setting. Contrary to standard artificial neural networks which work under the assumption that weights have some true latent value, Bayesian neural networks place a prior distribution over network parameters and hence treat weights as random variables &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goan2020bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Goan and Fookes 2020&lt;/a&gt;)&lt;/span&gt;. Not only does it perhaps seem more natural to treat unobserved weights as random, but the Bayesian setting also naturally gives rise to reason about uncertainty in predictions, which can ultimately help us develop more trustworthy models &lt;span class=&#34;citation&#34;&gt;(&lt;a href=&#34;#ref-goan2020bayesian&#34; role=&#34;doc-biblioref&#34;&gt;Goan and Fookes 2020&lt;/a&gt;)&lt;/span&gt;. A drawback of BNNs is that exact computation of posteriors is computationally challenging and often intractable (a non-trivial issue that we will turn back to in a moment).&lt;/p&gt;
&lt;p&gt;When the prior placed over parameters is Gaussian, the output of the BNN approaches a Gaussian Process as the width of the network grows, in line with the discussion in the previous section. This is exactly the assumption that &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; work with. They propose an architecture for a multi-layer perceptron (MLP) composed of (1) an input layer collecting covariates &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}_1,...,\mathbf{X}_p\)&lt;/span&gt;, (2) a single deterministic, hidden layer and (3) an outer layer producing predictions from a probabilistic model &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}(\mathbf{X})\)&lt;/span&gt;. Let &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{X}\)&lt;/span&gt; be a &lt;span class=&#34;math inline&#34;&gt;\((N \times P)\)&lt;/span&gt; matrix of covariates. Then formally, we have&lt;/p&gt;
&lt;span class=&#34;math display&#34; id=&#34;eq:bnn&#34;&gt;\[\begin{equation} 
\begin{aligned}
&amp;amp;&amp;amp; \hat{\mathbf{y}}&amp;amp;=\sigma(\mathbf{u}) \\
&amp;amp;&amp;amp; \mathbf{u}(\mathbf{Z})&amp;amp;=\mathbf{Z}(\mathbf{X})\mathbf{w}^{(L+1)}, &amp;amp;&amp;amp; \mathbf{w}^{(L+1)} \sim \mathcal{N}(\mathbf{m}, \mathbf{V}) \\
&amp;amp;&amp;amp; \mathbf{Z}(\mathbf{X})&amp;amp;=f(\mathbf{X}\mathbf{w}^{(L)}) \\
\end{aligned}
\tag{7}
\end{equation}\]&lt;/span&gt;
&lt;p&gt;where &lt;span class=&#34;math inline&#34;&gt;\(\sigma(.)\)&lt;/span&gt; is a link function and &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{u}(\mathbf{X})\)&lt;/span&gt; represents the probabilistic model learned in the outer layer with weights &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}^{(L+1)}\)&lt;/span&gt; assumed to be Gaussian random variables.&lt;a href=&#34;#fn3&#34; class=&#34;footnote-ref&#34; id=&#34;fnref3&#34;&gt;&lt;sup&gt;3&lt;/sup&gt;&lt;/a&gt; Finally, &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{Z}(\mathbf{X})\)&lt;/span&gt; denotes the inner (or more generally penultimate) layer, an &lt;span class=&#34;math inline&#34;&gt;\((N \times P)\)&lt;/span&gt; matrix of neural activations through &lt;span class=&#34;math inline&#34;&gt;\(f:(\mathbf{X}\mathbf{w}^{(L)})\mapsto \mathbf{Z}\)&lt;/span&gt;. &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; work with a simple single-layer MLP, but it should be evident that this be extended to arbitrary depth and complexity, while still maintaining the high-level structure imposed by &lt;a href=&#34;#eq:bnn&#34;&gt;(7)&lt;/a&gt;. This flexibility allows RATE to be applied to a wide range of Bayesian network architectures, since all that is really required is the posterior distribution over weights &lt;span class=&#34;math inline&#34;&gt;\(\mathbf{w}^{(L+1)}\)&lt;/span&gt;, which arises from the probabilistic outer layer. The fact that only the outer layer needs to be probabilistic has the additional benefit of mitigating the computational burden that comes with Bayesian inference, which was mentioned earlier.&lt;/p&gt;
&lt;!-- ![An MLP with a single, deterministic hidden layer and probabilistic outer layer.](www/bnn.png){#fig:bnn width=&#34;500&#34;} --&gt;
&lt;p&gt;Having established this basic, flexible set-up the &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; go on to derive closed-form expressions for RATE in this setting. The details are omitted here since the logic is largely analogous to what we learned above, but can be found in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt;.&lt;/p&gt;
&lt;/div&gt;
&lt;div id=&#34;conclusion&#34; class=&#34;section level1&#34;&gt;
&lt;h1&gt;Conclusion&lt;/h1&gt;
&lt;p&gt;The RATE criterion originally proposed by &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;Crawford et al.&lt;/a&gt; (&lt;a href=&#34;#ref-crawford2019variable&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; and shown to be applicable to Bayesian neural networks in &lt;span class=&#34;citation&#34;&gt;&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;Ish-Horowicz et al.&lt;/a&gt; (&lt;a href=&#34;#ref-ish2019interpreting&#34; role=&#34;doc-biblioref&#34;&gt;2019&lt;/a&gt;)&lt;/span&gt; offers an intuitive way to measure variable importance in the context of deep learning. By defining variable importance as the contribution inputs make to a probabilistic model, it implicitly incorporates the interactions between covariates and nonlinearities that the model has learned. In other words, it allows researchers to peek directly into the &lt;em&gt;Black Box&lt;/em&gt;. This opens up interesting avenues for future research, as the approach can be readily applied in academic disciplines and real-world applications that rely heavily on explainability of outcomes.&lt;/p&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div id=&#34;references&#34; class=&#34;section level1 unnumbered&#34;&gt;
&lt;h1&gt;References&lt;/h1&gt;
&lt;div id=&#34;refs&#34; class=&#34;references csl-bib-body hanging-indent&#34;&gt;
&lt;div id=&#34;ref-arrieta2020explainable&#34; class=&#34;csl-entry&#34;&gt;
Arrieta, Alejandro Barredo, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. 2020. &lt;span&gt;“Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI.”&lt;/span&gt; &lt;em&gt;Information Fusion&lt;/em&gt; 58: 82–115.
&lt;/div&gt;
&lt;div id=&#34;ref-bishop2006pattern&#34; class=&#34;csl-entry&#34;&gt;
Bishop, Christopher M. 2006. &lt;em&gt;Pattern Recognition and Machine Learning&lt;/em&gt;. springer.
&lt;/div&gt;
&lt;div id=&#34;ref-crawford2019variable&#34; class=&#34;csl-entry&#34;&gt;
Crawford, Lorin, Seth R Flaxman, Daniel E Runcie, and Mike West. 2019. &lt;span&gt;“Variable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study.”&lt;/span&gt; &lt;em&gt;The Annals of Applied Statistics&lt;/em&gt; 13 (2): 958.
&lt;/div&gt;
&lt;div id=&#34;ref-fan2020interpretability&#34; class=&#34;csl-entry&#34;&gt;
Fan, Fenglei, Jinjun Xiong, and Ge Wang. 2020. &lt;span&gt;“On Interpretability of Artificial Neural Networks.”&lt;/span&gt; &lt;em&gt;Preprint at Https://Arxiv. Org/Abs/2001.02522&lt;/em&gt;.
&lt;/div&gt;
&lt;div id=&#34;ref-goan2020bayesian&#34; class=&#34;csl-entry&#34;&gt;
Goan, Ethan, and Clinton Fookes. 2020. &lt;span&gt;“Bayesian Neural Networks: An Introduction and Survey.”&lt;/span&gt; In &lt;em&gt;Case Studies in Applied Bayesian Data Science&lt;/em&gt;, 45–87. Springer.
&lt;/div&gt;
&lt;div id=&#34;ref-goodfellow2016deep&#34; class=&#34;csl-entry&#34;&gt;
Goodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. &lt;em&gt;Deep Learning&lt;/em&gt;. MIT Press.
&lt;/div&gt;
&lt;div id=&#34;ref-ish2019interpreting&#34; class=&#34;csl-entry&#34;&gt;
Ish-Horowicz, Jonathan, Dana Udwin, Seth Flaxman, Sarah Filippi, and Lorin Crawford. 2019. &lt;span&gt;“Interpreting Deep Neural Networks Through Variable Importance.”&lt;/span&gt; &lt;em&gt;arXiv Preprint arXiv:1901.09839&lt;/em&gt;.
&lt;/div&gt;
&lt;/div&gt;
&lt;div style=&#34;page-break-after: always;&#34;&gt;&lt;/div&gt;
&lt;/div&gt;
&lt;div class=&#34;footnotes&#34;&gt;
&lt;hr /&gt;
&lt;ol&gt;
&lt;li id=&#34;fn1&#34;&gt;&lt;p&gt;Simulatability describes the overall, high-level understandability of the mechanisms underlying the model – put simply, the less complex the model, the higher its simulatability. Decomposability concerns the extent to which the model can be taken apart into smaller pieces – neural networks by there very nature are compositions of multiple layers. Finally, algorithmic transparency refers to the extent to which the training of the algorithm is well-understood and to some extent observable – since DNNs generally deal with optimization of non-convex functions and often lack unique solution they are inherently intransparent.&lt;a href=&#34;#fnref1&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn2&#34;&gt;&lt;p&gt;For more detail see for example &lt;a href=&#34;https://christophm.github.io/interpretable-ml-book/shapley.html&#34;&gt;here&lt;/a&gt;.&lt;a href=&#34;#fnref2&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;li id=&#34;fn3&#34;&gt;&lt;p&gt;For simplicity I have omitted the deterministic bias term.&lt;a href=&#34;#fnref3&#34; class=&#34;footnote-back&#34;&gt;↩︎&lt;/a&gt;&lt;/p&gt;&lt;/li&gt;
&lt;/ol&gt;
&lt;/div&gt;
</description>
    </item>
    
  </channel>
</rss>
