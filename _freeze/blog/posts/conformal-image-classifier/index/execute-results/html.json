{
  "hash": "7cab5d4944db785ea82da80ad2a413f3",
  "result": {
    "markdown": "---\ntitle: How to Conformalize a Deep Image Classifier\nsubtitle: Conformal Prediction in Julia --- Part 2\ndate: '2022-12-05'\ncategories:\n  - conformal prediction\n  - uncertainty\n  - Julia\ndescription: 'A guide demonstrating how to use [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl) to conformalize a deep image classifier in a few lines of code.'\nimage: www/intro.gif\ndraft: false\n---\n\n\n\n<div class=\"intro-gif\">\n  <figure>\n    <img src=\"www/intro.gif\">\n    <figcaption>Conformalized prediction sets for a<br>simple Deep Image Classifier.</figcaption>\n  </figure>\n</div>\n\nDeep Learning is popular and --- for some tasks like image classification --- remarkably powerful. But it is also well-known that Deep Neural Networks (DNN) can be unstable [@goodfellow2014explaining] and poorly calibrated. Conformal Prediction can be used to mitigate these pitfalls. \n\nIn the [first part](../conformal-prediction/index.qmd) of this series of posts on Conformal Prediction, we looked at the basic underlying methodology and how CP can be implemented in Julia using [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl). This second part of the series is a more goal-oriented how-to guide: it demonstrates how you can conformalize a deep learning image classifier built in `Flux.jl` in just a few lines of code. \n\nSince this is meant to be more of a hands-on article, we will avoid diving too deeply into methodological concepts. If you need more colour on this, be sure to check out the [first article](../conformal-prediction/index.qmd) on this topic and also @angelopoulos2021gentle. For a more formal treatment of Conformal Prediction see also @angelopoulos2022uncertainty.\n\n## 🎯 The Task at Hand \n\nThe task at hand is to predict the labels of handwritten images of digits using the famous MNIST dataset [@lecun1998mnist]. Importing this popular machine learning dataset in Julia is made remarkably easy through `MLDatasets.jl`:\n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing MLDatasets\nN = 1000\nXraw, yraw = MNIST(split=:train)[:]\nXraw = Xraw[:,:,1:N]\nyraw = yraw[1:N]\n```\n:::\n\n\n@fig-samples below shows a few random samples from the training data:\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nusing MLJ\nusing Images\nX = map(x -> convert2image(MNIST, x), eachslice(Xraw, dims=3))\ny = coerce(yraw, Multiclass)\n\nn_samples = 10\nmosaic(rand(X, n_samples)..., ncol=n_samples)\n```\n\n::: {#fig-samples .cell-output .cell-output-display execution_count=4}\n```{=html}\n<img src=\"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAAjAAAAA4CAAAAADGVp33AAAABGdBTUEAALGPC/xhBQAAAAFzUkdCAK7OHOkAAAAgY0hSTQAAeiYAAICEAAD6AAAAgOgAAHUwAADqYAAAOpgAABdwnLpRPAAACa5JREFUeAHtwQl0VYWBAND78QUcwAWEY4MLuFBB7dQqjB2BmKhVBOsSx2JHURFxmbrgPhUkAXQUC+IgjOi4o0hbQRygolQTEkCLCi4YHRkcqAsFRBAaBIT8Oe/8EwPJ3x5ij2fOuzcQi0UQiMUiCMRiEQRisQgCsVgGFeYqt7NALBZBYLc4zmJ1Yv+fJKUTiMUiCOwGRW52pt3lCAHek5/pxpormr2tN98M/M5ysXQqhEo0FojFIgjkoaMZjjJZf+kN9rak3eFYNzhXAapd7y25lDrZFNHs7WlJJzgBv/a151whik5m6qqZJX7jSVEF2muwSp3dr58btVejjwcUGCS6CsUYrqlALBZBIA/POFJSUnqBQ4307TV3pbu0kFLkZZebaYvM9nOn500RTZE+6u2Ngd4zTj66a4duukiq09WDhjhHjXwdZoXblGnwlIvtft0ch4PVKVLoXTzhS/mrUIxK5ZoKxGIRBHJo6QxHCz0uvfMc6jPfVlcTFElIWmC5lg7UzW896nKZ7OU5nY20o1+aaaNsWrteaJ4ROhnocPu5XgeP+lAu5U6TkFSvwGFmOts78vFjr5utr3o19nGhi+V2sl7+yZF43SXel8sKH/kXoSp3uV1br1koX8WKhUqkE4jFIgjk8JDzhSotkF5Py6ySWYGzvW+J7EYowjrnq7QNLZzqGecbZZn09tXDVyrUCwx1k842yqyVf1cs9CNLvOwR4/zKwW52gZMslY8a03Qxz1Dt0NGdfi63q91rmzqbzfSYUJX2usqlhf4maG6tL7T1D9rK7UEvWiplpFO0da6F8lOsQqhEeoFYLIJAVt2dKfSpUpuld6D/lUmB61zgEOuMN0YmhabrhmV+qN4WM7yg1GDXSK+vpKusVO8yQw2wUmbNTdBf6L/8xiqh2xzqJC10cLnbfC23FcqFxuvsffm51j328J6pRnpTvRVWyK6VKfr6wh2medRJRlgkt68tVe9cR4miTKhEpfQCsVgEgawKtRRaY4PMpkqv0P0OMc5LtnvXGJksVChpqkvtbIlznOUa6fXBUg1ut9ZrsrlAfylvWCDlr87Qy0uau0GlWbKpcRraa2+NKE41SgF6WyWKQksUuMVDtvi9kzzmTltFsZfTJFQbIT8VilGiUiaBWCyCQBY/9aDQB/rJ7MfulN5s2/W2Wmib9Jq7XwdJz7nIFk11kN5x+nrTIvUG+YHxlsqmh5QX3G1H1V7RG/9mlmwmGKCNbk7zFFrpKB8HGqU5Jlolmie1MdxotHOGFz3rOvf5Wv5e08W7zlYrH8WKUalSZoFYLIJAFoO0FzrHMpl1lN4vbNXbWqHzbJZOcxMMwFQX2SKdl6R3oma+sFW9Qp+6TjZHOk9ohX6229m9emqtjUN9JLPlSlXgPn+WMNjPUeUh2f3I3wv1t8QDojjGWsOFWuMI0zXzskXy08HRukpaaZ38lAnNVa5BpUo7CsRiEQQy2NNQl0hitY2ySUpnfxOVWCulpwnSudIArDPUFk0djeell1TnYfX2c5WtsjtDa6H31WrsZQuc6gCXGiqbKtV6aaNCM3XWm2SeGbKb43jD9NXKBKOsM8Ij8hXogUJD0AnDLZKvkz2O6W6Sn3LFQmV2VKZEpQaBWCyCQAad/avQox7xF9EN9rJ3pPR2hRM1dYq7JFBjk6bGKbXJH2XWz7NSrtRehdAwI6R3mZRHpHOJCke4ydt+L5upegrVqfQ/BsttmzecKdTci3p42BF+bbvcFjtFtdCzNupppcflq5Nan3vA3TbLT5l6w6WcqBjFKjUIxGIRBHbQwjH+JKVIQjOf+k+vy+4NnSzUWIm7JIU6GmyihZq6TQtJY9yqqStcIGmyZdJbhGMVWinUB9Wu18+bMjlcEm96XTqrfIkCHWV2gP4ulFKl1Jcya6ad1Xa2VYlfesqNXjFbbqc6y+H+Yp6zjbXJz6yQr0F+LaHWZvkplzJcpUr1khoLxGIRBL6xj7+zXMq17pD0qX5el8s895ptgx211cVbQgeZaJkhkho7xj9igSEaa+5+l0ma7VqZzPUHfVQYqQotJJTbYpw/yK7KZ9LbLnSm0dK73SU6SkgKFWnnS5kdqEZv8zT2htAFZsvH80LXGOsLF6uRr4NdImm1WfJVJpSwo3JNBWKxCALfOFIHU4XON0oBppgvt7uVesIY8zRobi+hLq4z2STpDFOAWbbZWVcTFOFtZbbIbKAlDveEhCSS5hilQi7XW2CadIZ4BT2kd61yofFmm4lmshumzlcaO9BA1LpX/n7mXtxolvwN8gOMVGPXFSsTKrejQCwWQeAbr0rZxyAFQjfJx2qlrvGiha62VAJJGyz2tK987i7vSO8sSes8YGdnG+sg3OEetbJZraef6qOlPpjiQtmdZYo9cZ8P1GhqhNAw6RzpNklvWGOkVmp0VSe7SyWNdqNFGuxhrFK1rrBYvnqZbg9jTBbFGnxkkl1VrFgZKpXYWSAWiyDQRA8nYoOr5WuRAYb4Z3MkFCjwlZZeNc9zlsismToJDQ5ygiGOwkK3qpLbhz70pMA0fayUywy19sQBekkIvC3lGIc5XgvdscUSTR1ghnZq3awa3XWV2ykeVqTSW57ysZTbHW+TKzwjX8earsBot4imG9bbLn+VilEuVCalUonGArFYBIEmrhRabLIoPjPaRIeo1Rpf+EQuSx1qX/Pdb72kAY7VTsLHxnvCGvnbZrOEarmNVyZ0i6H2sEJKR/tL2eRGz2uqv44YrFr+XnG6MU7XQw/1Er7yiGfk6yfmaGWgSaJKSDhOa7XyVSKJMg1KVGoqEItFEGikiyOEhonur96Vv194VYGu/kNCUmizPyv1gWhaaCNpkdwes0mZljoJ7W9ny93jIekUSaBKvQTe9aXs/lup7gZo8LVRlsvXsV6wl4EmiW61pKhKVEgZjnLpBWKxCAKNHOYwoQ56qfZdektvp7tYe6EV5htrseha64zOPpHLx0bb6A5tNfa5USZZI72kJOaY4lzTXCiJiT6Xy1bzzbdrfuIF7fX3tF0xwUXammKA5fJVKSG3QCwWQaCRjTbay+NetMF3ba65xttbaI01ds1ag7ygUH4e9FuXCQ1Rra/J3sY4W2V2v+Pt62C34BZJTDTRd6md32mjv6ftmo8M9qQiC7xnlD/afQKxWASBRqpU6+MGG/xtfOLbmyOQv/VGC42Wr9lK/cq5Umo8pMp3qdAcB+lrjl33rPWuUmwPh9idArFYBIEmzhRrrEqVv51pfmigOb6NLWaZZfcLxGIRBGLfM8fr7laTfD8FYrEIArHvmT8JfH8FYrEI/g8ntMbDoItCHgAAAABJRU5ErkJg\">\n```\n\nRandom samples from the MNIST dataset.\n:::\n:::\n\n\n## 🚧 Building the Network\n\nTo model the mapping from image inputs to labels will rely on a simple Multi-Layer Perceptron (MLP). A great Julia library for Deep Learning is `Flux.jl`. But wait ... doesn't `ConformalPrediction.jl` work with models trained in `MLJ.jl`? That's right, but fortunately there exists a `Flux.jl` interface to `MLJ.jl`, namely `MLJFlux.jl`. The interface is still in its early stages, but already very powerful and easily accessible for anyone (like myself) who is used to building Neural Networks in `Flux.jl`. \n\nIn `Flux.jl`, you could build an MLP for this task as follows,\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\nusing Flux\n\nmlp = Chain(\n    Flux.flatten,\n    Dense(prod((28,28)), 32, relu),\n    Dense(32, 10)\n)\n```\n:::\n\n\nwhere `(28,28)` is just the input dimension (28x28 pixel images). Since we have ten digits, our output dimension is ten.^[For a full tutorial on how to build an MNIST image classifier relying solely on `Flux.jl`, check out this [tutorial](https://fluxml.ai/Flux.jl/stable/tutorials/2021-01-26-mlp/).]\n\nWe can do the exact same thing in `MLJFlux.jl` as follows,\n\n::: {.cell execution_count=5}\n``` {.julia .cell-code}\nusing MLJFlux\n\nbuilder = MLJFlux.@builder Chain(\n    Flux.flatten,\n    Dense(prod(n_in), 32, relu),\n    Dense(32, n_out)\n)\n```\n:::\n\n\nwhere here we rely on the `@builder` macro to make the transition from `Flux.jl` to `MLJ.jl` as seamless as possible. Finally, `MLJFlux.jl` already comes with a number of helper functions to define plain-vanilla networks. In this case, we will use the `ImageClassifier` with our custom builder and cross-entropy loss:\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\nImageClassifier = @load ImageClassifier\nclf = ImageClassifier(\n    builder=builder,\n    epochs=10,\n    loss=Flux.crossentropy\n)\n```\n:::\n\n\nThe generated instance `clf` is a model (in the `MLJ.jl` sense) so from this point on we can rely on standard `MLJ.jl` workflows. For example, we can wrap our model in data to create a machine and then evaluate it on a holdout set as follows:\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\nmach = machine(clf, X, y)\n\nevaluate!(\n    mach,\n    resampling=Holdout(rng=123, fraction_train=0.8),\n    operation=predict_mode,\n    measure=[accuracy]\n)\n```\n:::\n\n\nThe accuracy of our very simple model is not amazing, but good enough for the purpose of this tutorial. For each image, our MLP returns a softmax output for each possible digit: 0,1,2,3,...,9. Since each individual softmax output is valued between zero and one, $y_k\\in(0,1)$, this is commonly interpreted as a probability: $y_k \\coloneqq p(y=k|X)$. Edge cases -- that is values close to either zero or one -- indicate high predictive certainty. But this is only a heuristic notion of predictive uncertainty [@angelopoulos2021gentle]. Next, we will turn this heuristic notion of uncertainty into a rigorous one using Conformal Prediction.\n\n## 🔥 Conformalizing the Network\n\nSince `clf` is a model, it is also compatible with our package: `ConformalPrediction.jl`. To conformalize our MLP, we therefore only need to call `conformal_model(clf)`. Since the generated instance `conf_model` is also just a model, we can still rely on standard `MLJ.jl` workflows. Below we first wrap it in data and then fit it. Aaaand ... we're done! Let's look at the results in the next section.\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\nusing ConformalPrediction\nconf_model = conformal_model(clf; method=:simple_inductive, coverage=.95)\nmach = machine(conf_model, X, y)\nfit!(mach)\n```\n:::\n\n\n## 📊 Results\n\n\n\n@fig-plots below presents the results. @fig-plots-1 displays highly certain predictions, now defined in the rigorous sense of Conformal Prediction: in each case, the conformal set (just beneath the image) includes only one label. \n\n@fig-plots-2 and @fig-plots-3 display increasingly uncertain predictions of set size two and three, respectively. They demonstrate that CP is well equipped to deal with samples characterized by high aleatoric uncertainty: digits four (4), seven (7) and nine (9) share certain similarities. So do digits five (5) and six (6) as well as three (3) and eight (8). These may be hard to distinguish from each other even after seeing many examples (and even for a human). It is therefore unsurprising to see that these digits often end up together in conformal sets. \n\n::: {#fig-plots .cell layout-nrow='3' execution_count=10}\n\n::: {.cell-output .cell-output-display}\n![Randomly selected prediction sets of size $|C|=1$.](index_files/figure-html/fig-plots-output-1.svg){#fig-plots-1}\n:::\n\n::: {.cell-output .cell-output-display}\n![Randomly selected prediction sets of size $|C|=2$.](index_files/figure-html/fig-plots-output-2.svg){#fig-plots-2}\n:::\n\n::: {.cell-output .cell-output-display}\n![Randomly selected prediction sets of size $|C|=3$.](index_files/figure-html/fig-plots-output-3.svg){#fig-plots-3}\n:::\n\nConformalized predictions from an image classifier.\n:::\n\n\n\n\n## 🧐 Evaluation\n\nTo evaluate the performance of conformal models, specific performance measures can be used to assess if the model is correctly specified and well-calibrated [@angelopoulos2021gentle]. We will look at this in some more detail in another post in the future. For now, just be aware that these measures are already available in `ConformalPrediction.jl` and we will briefly showcase them here.\n\nAs for many other things, `ConformalPrediction.jl` taps into the existing functionality of `MLJ.jl` for model evaluation. In particular, we will see below how we can use the generic `evaluate!` method on our machine. To assess the correctness of our conformal predictor, we can compute the empirical coverage rate using the custom performance measure `emp_coverage`. With respect to model calibration we will look at the model's conditional coverage. For adaptive, well-calibrated conformal models, conditional coverage is high. One general go-to measure for assessing conditional coverage is size-stratified coverage. The custom measure for this purpose is just called `size_stratified_coverage`, aliased by `ssc`. \n\nThe code below implements the model evaluation using cross-validation. The Simple Inductive Classifier that we used above is not adaptive and hence the attained conditional coverage is low compared to the overall empirical coverage, which is close to $0.95$, so in line with the desired coverage rate specified above.\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\n_eval = evaluate!(\n    mach,\n    resampling=CV(),\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n```\n\n::: {.cell-output .cell-output-display}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>PerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n┌───────────────────────────────────────────────────────────┬───────────┬───────\n│ measure                                                   │ operation │ meas ⋯\n├───────────────────────────────────────────────────────────┼───────────┼───────\n│ emp_coverage (generic function with 1 method)             │ predict   │ 0.95 ⋯\n│ size_stratified_coverage (generic function with 1 method) │ predict   │ 0.77 ⋯\n└───────────────────────────────────────────────────────────┴───────────┴───────\n<span class=\"ansi-cyan-fg\">                                                               3 columns omitted</span>\n</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEmpirical coverage: 0.951\nSSC: 0.771\n```\n:::\n:::\n\n\n\n\nWe can attain higher adaptivity (SSC) when using adaptive prediction sets:\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\nconf_model = conformal_model(clf; method=:adaptive_inductive, coverage=.95)\nmach = machine(conf_model, X, y)\nfit!(mach)\n_eval = evaluate!(\n    mach,\n    resampling=CV(),\n    operation=predict,\n    measure=[emp_coverage, ssc]\n)\nresults[:adaptive_inductive] = mach\ndisplay(_eval)\nprintln(\"Empirical coverage: $(round(_eval.measurement[1], digits=3))\")\nprintln(\"SSC: $(round(_eval.measurement[2], digits=3))\")\n```\n\n::: {.cell-output .cell-output-display}\n\n::: {.ansi-escaped-output}\n```{=html}\n<pre>PerformanceEvaluation object with these fields:\n  measure, operation, measurement, per_fold,\n  per_observation, fitted_params_per_fold,\n  report_per_fold, train_test_rows\nExtract:\n┌───────────────────────────────────────────────────────────┬───────────┬───────\n│ measure                                                   │ operation │ meas ⋯\n├───────────────────────────────────────────────────────────┼───────────┼───────\n│ emp_coverage (generic function with 1 method)             │ predict   │ 0.99 ⋯\n│ size_stratified_coverage (generic function with 1 method) │ predict   │ 0.94 ⋯\n└───────────────────────────────────────────────────────────┴───────────┴───────\n<span class=\"ansi-cyan-fg\">                                                               3 columns omitted</span>\n</pre>\n```\n:::\n\n:::\n\n::: {.cell-output .cell-output-stdout}\n```\nEmpirical coverage: 0.991\nSSC: 0.948\n```\n:::\n:::\n\n\nWe can also have a look at the resulting set size for both approaches using a custom `Plots.jl` recipe (fig-setsize). In line with the above, the spread is wider for the adaptive approach, which reflects that \"the procedure is effectively distinguishing between easy and hard inputs\" [@angelopoulos2021gentle].\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\nplt_list = []\nfor (_mod, mach) in results\n    push!(plt_list, bar(mach.model, mach.fitresult, X; title=String(_mod)))\nend\nplot(plt_list..., size=(800,300))\nplot(plt_list..., size=(800,300),bg_colour=:transparent)\n```\n\n::: {.cell-output .cell-output-display execution_count=16}\n![Distribution of set sizes for both approaches.](index_files/figure-html/fig-setsize-output-1.svg){#fig-setsize}\n:::\n:::\n\n\n## 🔁 Recap\n\nIn this short guide we have seen how easy it is to conformalize a deep learning image classifier in Julia using `ConformalPrediction.jl`. Almost any deep neural network trained in `Flux.jl` is compatible with `MLJ.jl` and can therefore be conformalized in just a few lines of code. This makes it remarkably easy to move uncertainty heuristics to rigorous predictive uncertainty estimates. We have also seen a sneak peek at performance evaluation of conformal predictors. Stay tuned for more!\n\n## 🎓 References\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {
      "include-in-header": [
        "<script src=\"https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.6/require.min.js\" integrity=\"sha512-c3Nl8+7g4LMSTdrm621y7kf9v3SDPnhxLNhcjFJbKECVnmZHTdo+IRO05sNLTH/D3vA6u1X32ehoLC7WFVdheg==\" crossorigin=\"anonymous\"></script>\n<script src=\"https://cdnjs.cloudflare.com/ajax/libs/jquery/3.5.1/jquery.min.js\" integrity=\"sha512-bLT0Qm9VnAYZDflyKcBaQ2gg0hSYNQrJ8RilYldYQ1FxQYoCLtUjuuRuZo+fjqhx/qtq/1itJ0C2ejDxltZVFg==\" crossorigin=\"anonymous\"></script>\n<script type=\"application/javascript\">define('jquery', [],function() {return window.jQuery;})</script>\n"
      ]
    }
  }
}