{
  "hash": "39661e41ccfa325c90ebc4ca03f87af2",
  "result": {
    "markdown": "---\ntitle: ECCCos from the Black Box\nsubtitle: Faithful Model Explanations through Energy-Based Conformal Counterfactuals\ndate: '2024-02-08'\ncategories:\n  - counterfactuals\n  - explainable AI\n  - algorithmic recourse\n  - Julia\ndescription: '*ECCCo* is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.'\nimage: www/intro.gif\ndraft: false\n---\n\n\n\n<div class=\"intro-gif\">\n  <figure>\n    <img src=\"www/intro.gif\">\n    <figcaption>Photo by <a href=\"https://unsplash.com/@jake_oates2000?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Jake Oates</a> on <a href=\"https://unsplash.com/photos/steel-wool-photography-MovsEr-Bgts?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a></figcaption>\n  </figure>\n</div> \n\nCounterfactual explanations offer an intuitive and straightforward way to explain opaque machine learning (ML) models. They work under the premise of perturbing inputs to achieve a desired change in the predicted output. There are typically many ways to achieve this, in other words, many different counterfactuals may yield the same desired outcome. A key challenge for researchers has therefore been to, firstly, define certain desirable characteristics of counterfactual explanations and, secondly, come up with efficient ways to achieve them.\n\nOne of the most important and studied characteristics of counterfactual explanations is *plausibility*. Broadly speaking, counterfactuals are considered plausible if they are indistinguishable from actual observed data in the target domain. Plausibility is positively associated with actionability, robustness [@artelt2021evaluating] and causal validity [@mahajan2019preserving]. To achieve plausibility, many existing approaches rely on surrogate models. This is straightforward but it also convolutes things further: it essentially reallocates the task of learning plausibile explanations for the data from the model itself to the surrogate.\n\nIn our AAAI 2024 paper, [Faithful Model Explanations through Energy-Based Conformal Counterfactuals](https://arxiv.org/abs/2312.10648) (*ECCCo*), we propose that we should not only look for explanations that please us but rather focus on generating counterfactuals that faithfully explain model behavior. It turns out that we can achieve both faithfulness and plausibility by relying solely on the model itself by leveraging recent advances in energy-based modelling and conformal prediction. We support this claim through extensive empirical studies and believe that *ECCCo* opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.\n\n::: {.callout-note}\n\nThis is a companion post to our recent AAAI 2024 paper co-authored with [Mojtaba Farmanbar](https://nl.linkedin.com/in/mfarmanbar), [Arie van Deursen](https://avandeursen.com/about/) and [Cynthia C. S. Liem](https://www.cynthialiem.com/). The paper is a more formal and detailed treatment of the topic and is available [here](https://arxiv.org/abs/2312.10648). This blog post is intentionally free of technical details and is meant to provide a high-level overview of the paper.\n\n:::\n\n## Pick your Poison {#sec-poison}\n\nThere are two major debates in the field of Explainable AI (XAI). The first one centers around the role of explainability in AI: do we even need explanations and if so, why? Some have argued that we need not care about explaining models as long as they produce reliable outcomes [@robbins2019misdirected]. Humans have not shied away from this in other domains either: doctors, for example, have prescribed aspirin for decades without understanding why it is a reliable drug for pain relief [@london2019artificial]. While this reasoning may rightfully apply in some cases, experiments conducted by my colleagues at TU Delft have shown that humans do not make better decisions if aided by a reliable and yet opaque model [@he2023how]. In these studies, subjects tended to ignore the AI model indicating that they did not trust its decisions. Beyond this, explainability comes with numerous advantages such as accountability, control, contestability and the potential for uncovering causal relationships. \n\n::: {.callout-note}\n\n## Intermezzo: Why Bother?\n\nHere is some food for thought: if we can blindly rely on the decisions made by an AI, why should we even bother to come up with explanations?\n\nI must confess I had never even seriously considered this as an option until attending [Stefan Buijsman](https://www.tudelft.nl/staff/s.n.r.buijsman/)'s recent talk at a Delft Design for Values [workshop](https://www.tudelft.nl/evenementen/2024/delft-ai/workshop-navigating-the-interplay-of-explainability-and-privacy-in-ai). It is an interesting critique of recent efforts towards XAI, especially considering that the field has so far struggled to produce satisfactory and robust results with meaningful real-world impact. Much like @he2023how find that blind reliance on reliable AI models does not seem to work in practice, numerous other studies have shown that explanations for AI models either fail to help users or even mislead them [@mittelstadt2019explaining,@alufaisan2021does,@lakkaraju2020how].\n\nSo, have all efforts toward explainable AI been in vain? Should we simply stop explaining black-box models altogether as proposed by @rudin2019stop? Having worked in this field for a little over 2 years now, I have personally grown more and more skeptical of certain trends. In particular, I think that the community has focused too much on finding explanations that please us independent of the model itself. This is a bit like applying a band-aid to a wound without first cleaning it. At best, plausible explanations for untrustworthy models provide a false sense of security. At worst, they can be used to manipulate and deceive. The AAAI paper presented in this post is very much born out of skepticism about this approach.\n\nNonetheless, I do not think all hope is lost for XAI. I strongly believe that there is a need for algorithmic recourse as long as we continue to deploy black-box models for automated decision-making. While I am fully supportive of the idea that we should always strive to use models that are as interpretable as possible, I also think that there are cases where this is not feasible or it is simply too convenient to use a black-box model instead. The aspirin example mentioned above is a striking example of this. But it is easy enough to stretch that example further to illustrate why explainability is important. What if the use of aspirin was prohibited for a small minority of people and there was a reliable, opaque model to decide who belonged to that group? If you were part of that group, would you not want to know why? Why should you have to endure headaches for the rest of your life while others do not?\n\nIn summary, I think that---like it or not---we do need to bother. \n\n:::\n\nThe second major debate is about what constitutes a good explanation, because, crucially, explanations are not unique: was your headache cured by the aspirin you took before going to sleep or sleep itself? Or a combination of both? This multiplicity of explanations arises almost naturally in the context of counterfactual explanations. Unless the combination of input features for which the model predicts the target class or value is unique, there is always more than one possible explanation. As an illustrative example, consider the counterfactuals presented in @fig-cf-example. All of these are valid explanations for turning a 'nine' into 'seven' according to the underlying classifier (a simple multi-layer perceptron). They are all valid in the sense the model predicts the target label with high probability in each case. The troubling part is that even though all of the generated counterfactuals provide valid explanations for why the model predicts 'seven' instead of 'nine', they all look very different. \n\n![Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using *Wachter* [@wachter2017counterfactual], *Schut* [@schut2021generating] and *REVISE* [@joshi2019realistic].](www/mnist_motivation.png){#fig-cf-example width=\"80%\"}\n\nSo, which explanations do we trust most? Which one would you choose to present to an audience to explain how the classifier decides which digit to predict? Arguably, the counterfactual on the far right looks most like a 'seven', so I am willing to bet that most people would simply choose that one. It is valid after all and it looks plausible, while the other two counterfactuals might just lead to awkward questions from particularly some of the more interrogative audience members. In any case, I mentioned earlier that more plausible explanations tend to also be more actionable and robust, so this seems fair game. The counterfactual produced by *REVISE* [@joshi2019realistic] is the poison we will pick---dump the rest and move on^[Considering how much I have cited @joshi2019realistic in the past I think it should go without saying that I very much like this paper, despite taking a critical stance on it here.]. Plausibility is all we need!\n\nI am exaggerating but I do think that as a community of researchers studying counterfactual explanations, we have become so absorbed by the pursuit of a few desiderata that we have forgotten that ultimately we are in the business of explaining models. Our primary mandate is to design tools that help us understand why models predict certain outcomes. How useful is a plausible, actionable, sparse, causally valid explanation in gaining that understanding, if there exist a whole bunch of other valid explanations that do not meet these desiderata? Have we significantly improved our understanding of the underlying classifier in @fig-cf-example and therefore established a higher degree of trust in the model, simply because we have found a plausible counterfactual? \n\nIn my mind, we most certainly have not. I would argue that the existence of a valid and plausible explanation merely serves to reassure us that the model is not entirely ignorant about meaningful representations in the data. But as long as entirely implausible counterfactuals are also valid according to the model, selectively relying only on the subset of plausible counterfactuals may lead to a wrong sense of trust in untrustworthy models. That is why in our paper we argue that explanations should be faithful first, and plausible second.\n\n## Faithful First, Plausible Second\n\nTo navigate the interplay between faithfulness and plausibility, we propose a way to generate counterfactuals that are consistent with what the model has learned about the data. In doing so, we can also achieve plausibility but only in case the model has learned something meaningful. \n\n### Faithful Counterfactuals\n\nWhen inquiring about what is \"consistent with what the model has learned about the data\", we are essentially asking about the model's posterior conditional distribution of the input data given the target output. It turns out that we can approximate that distribution using ideas relevant to energy-based modelling. In particular, we can use something called Stochastic Gradient Langevin Dynamics (SGLD) to sample from the model's posterior conditional distribution [@welling2011bayesian]. \n\nWithout going into too much detail here, the idea is to use the model's energy function to guide the sampling process. The energy function is a scalar function that assigns a value to each possible configuration of the input data. The lower the energy, the higher the likelihood corresponding to the configuration. This is a powerful tool: @grathwohl2020your, for example, use SGLD in this fashion to train hybrid models---joint-energy models (JEM)---that are trained to both classify and generate data. \n\n@fig-faithful illustrates this concept. It shows samples (yellow stars) drawn from the posterior of a simple JEM trained on linearly separable data. The contour shows the kernel density estimate (KDE) for the learned conditional distribution. Although it seems that the posterior is too sharp in this case, the learned conditional distribution is overall consistent with the data. \n\n::: {.column-margin}\n![Kernel Density Estimate (KDE) for the learned conditional distribution. Yellow stars indicate samples generated through Stochastic Gradient Langevin Dynamics for a joint energy model (JEM).](www/density_model.png){#fig-faithful}\n:::\n\nAlso shown in @fig-faithful is a single counterfactual path from the orange to the blue class. I have relied on the baseline approach proposed in @wachter2017counterfactual here using only a small penalty for the distance between the counterfactual and the original input. A truly faithful counterfactual, as we define it in our paper, would be one that we could expect to sample from the learned conditional distribution (with high probability)^[I have had an interesting chat with [Nico Potyka](https://profiles.cardiff.ac.uk/staff/potykan) and [Francesco Leofante](https://fraleo.github.io/), recently, where they rightly pointed out that this definition of faithfulness needs to be refined. In particular, one might wonder what constitutes a 'high probability' in this context. I think this is a very valid point and I am looking forward to discussing this further with them.]. Based on this notion, we would not characterize the counterfactual in @fig-faithful as faithful, but it also is not too far off. \n\nIt is easy to see how other desiderata may conflict with faithfulness. If I had penalized the distance between the counterfactual and the original input more, for example, then the counterfactual would have been less costly but also less faithful. This is the sort of trade-off between different desiderata that we always need to navigate carefully in the context of counterfactual explanations. As we will see next, the same also applies to plausibility but in a different way.\n\n### Plausible Counterfactuals\n\nIf you could follow the discussion so far, then you have already covered the trickiest concept in our paper. Plausibility can be defined much like we have done for faithfulness, but it is a bit more straightforward. In our paper, we broadly define plausible counterfactals as those that are indistinguishable from the observed data in the target domain. We already touched on this in @sec-poison when discussing the counterfactuals in @fig-cf-example. \n\n::: {.column-margin}\n![KDE for the conditional distribution based on observed data. Counterfactual path as in @fig-faithful.](www/density_true.png){#fig-plausible}\n:::\n\n@fig-plausible illustrates the same concept for the same JEM as in @fig-faithful. The KDE in @fig-plausible shows the conditional distribution based on the observed data. The counterfactual path is the same as in @fig-faithful. The counterfactual is plausible in this case since it is not indistinguishable from the observed data in the target domain.\n\nLooking at both @fig-faithful and @fig-plausible, it becomes evident why the interplay between faithfulness and plausibility need not necessary be a trade-off. In this case, the counterfactual is neither terribly unfaithful nor implausible. This is because the learned conditional distribution is broadly consistent with the observed data.\n\n## Our approach: *ECCCo*\n\nNow that we have covered the two major concepts in our paper, we can move on to our proposed approach for generating faithful counterfactuals: *ECCCo*. As the title of the paper suggests, *ECCCo* is an acronym for *E*nergy-*C*onstrained *C*onformal *Co*unterfactuals. We leverage ideas from energy-based modelling and conformal prediction, in particular from @grathwohl2020your and @stutz2022learning, respectively. Our proposed counterfactual generation process involves little to no overhead and is broadly applicable to any model that can be trained using stochastic gradient descent. Technical details can be found in the paper. For now, let us focus on the high-level idea.\n\n@fig-poc compares the counterfactual path generated by *Wachter* [@wachter2017counterfactual] to those generated by *ECCCo*, where we use ablation to remove the energy constraint---*ECCCo (no EBM)*---and the conformal prediction component---*ECCCo (no CP)*. In this case, the counterfactual generated by *Wachter* is neither faithful nor plausible. It does, however, minimize the distance between the counterfactual and the original input. \n\nThe counterfactual generated by *ECCCo (no EBM)* is deeper inside the blue class and has avoided points near the decision boundary on its path to its final destination. This is because *ECCCo (no EBM)* involves a penalty term for predictive uncertainty, which is high near the decision boundary. Intuitively, we would expect that avoiding regions of high predictive uncertainty in our counterfactual search should help with plausibility [@schut2021generating]. In this particular case, the final counterfactual is neither more faithful nor more plausible than the one generated by *Wachter*, but we have found that generally penalizing predictive uncertainty alone can help to generate more faithful and plausible counterfactuals.\n\nThe counterfactual generated by *ECCCo (no CP)* is more faithful than the one generated by *Wachter* and *ECCCo (no EBM)*. This is because the energy constraint induces counterfactuals that are more consistent with the learned conditional distribution (as in @fig-faithful). Since the model has learned something meaningful about the data, the counterfactual is also more plausible than the one generated by *Wachter* and *ECCCo (no EBM)* in this case. \n\nThe counterfactual path generated by *ECCCo* combines benefits from both the energy constraint and the conformal prediction component. It ends up in a region of low predictive uncertainty and is consistent with the learned conditional distribution. \n\n![Gradient fields and counterfactual paths for different generators.](www/poc_gradient_fields.png){#fig-poc}\n\n## References\n\n",
    "supporting": [
      "index_files"
    ],
    "filters": [],
    "includes": {}
  }
}