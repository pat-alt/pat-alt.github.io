{
  "hash": "a67a8291a4601e1c984ed922b33d0ca3",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ndraft: true\nexecute:\n  eval: false\ntitle: Train model\n---\n\n\n\n\n\n\n::: {#65945278 .cell execution_count=1}\n``` {.julia .cell-code}\n# Import libraries.\nusing Flux, Plots, Random, Statistics, LaplaceRedux\ntheme(:wong)\n# Number of points to generate.\nxs, y = toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);\n\n# Neural network:\nn_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(D, n_hidden, σ),\n    Dense(n_hidden, 1)\n)  \nλ = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()\n\n# Training:\nusing Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 200\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n```\n:::\n\n\n### Chart\n\n::: {#6859b5f7 .cell execution_count=2}\n``` {.julia .cell-code}\nRandom.seed!(123)\nshuffled_indices = randperm(200)\ndata = zip(xs[shuffled_indices], y[shuffled_indices]);\n```\n:::\n\n\n::: {#46918a20 .cell execution_count=3}\n``` {.julia .cell-code}\nxlim = (minimum(X[1,:]),maximum(X[1,:]))\nylim = (minimum(X[2,:]),maximum(X[2,:]))\nanim = Animation()\nfor N in 1:1:length(data)\n    la = laplace(nn, λ=λ, subset_of_weights=:last_layer)\n    fit!(la, Iterators.take(data,N))\n    Xᵢ = reduce(vcat,map(d -> reshape(d[1],1,length(d[1])),Iterators.take(data, N)))\n    yᵢ = map(d -> d[2],Iterators.take(data, N))\n    plt = plot_contour(Xᵢ,yᵢ,la;xlim=xlim,ylim=ylim,zoom=-2,clegend=false)\n    plt = plot(plt, axis=nothing,background_color=:transparent, size=(500,500))\n    frame(anim, plt)\nend\n```\n:::\n\n\n::: {#2c2893d3 .cell execution_count=4}\n``` {.julia .cell-code}\ngif(anim, \"www/intro.gif\", fps=50)\n```\n:::\n\n\n::: {#686e0826 .cell execution_count=5}\n``` {.julia .cell-code}\nxlim = (minimum(X[1,:]),maximum(X[1,:]))\nylim = (minimum(X[2,:]),maximum(X[2,:]))\nanim = Animation()\nfor N in 1:1:length(data)\n    la = laplace(nn, λ=λ, subset_of_weights=:last_layer)\n    fit!(la, Iterators.take(data,N))\n    Xᵢ = reduce(vcat,map(d -> reshape(d[1],1,length(d[1])),Iterators.take(data, N)))\n    yᵢ = map(d -> d[2],Iterators.take(data, N))\n    plt = plot_contour(Xᵢ,yᵢ,la;xlim=xlim,ylim=ylim,zoom=-2,clegend=false)\n    plt = plot(plt, axis=nothing,background_color=:transparent, size=(1200,400))\n    frame(anim, plt)\nend\ngif(anim, \"www/medium.gif\", fps=50)\n```\n:::\n\n\n",
    "supporting": [
      "intro_gif_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}