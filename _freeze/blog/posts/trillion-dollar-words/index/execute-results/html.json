{
  "hash": "cb97895c1d35bd6832f6730219f9b78b",
  "result": {
    "engine": "jupyter",
    "markdown": "---\ntitle: TrillionDollarWords.jl\nsubtitle: The Trillion Dollar Words dataset and model in Julia\ndate: '2024-02-18'\ncategories:\n  - llm\n  - mechanistic interpretability\n  - finance\n  - Julia\ndescription: >-\n  A short post introducing a small new Julia package that facilitates working with the Trillion Dollar Words dataset and model published in a recent ACL 2023 [paper](https://arxiv.org/abs/2305.07972).\nimage: intro.jpeg\ndraft: false\ncode-fold: show\n---  \n\n\n\n<div class=\"intro-gif\">\n  <figure>\n    <img src=\"intro.jpeg\">\n    <figcaption>Photo by <a href=\"https://unsplash.com/@neonbrand?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Kenny Eliason</a> on <a href=\"https://unsplash.com/photos/1-us-dollar-banknote-8fDhgAN5zG0?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash\">Unsplash</a></figcaption>\n  </figure>\n</div> \n\n\nIn a recent [post](../spurious-sparks/index.qmd), I questioned the idea that finding patterns in latent embeddings of models is indicative of AGI or even surprising. One of the models we investigate in our related [paper](https://arxiv.org/abs/2402.03962) [@altmeyer2024position] is the *FOMC-RoBERTa* model trained on the Trillion Dollar Words dataset, both of which were published by @shah2023trillion in a recent ACL 2023 paper: [Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis](https://arxiv.org/abs/2305.07972) [@shah2023trillion]. To run our experiments and facilitate working with the data and model in Julia, I have developed a small package: [TrillionDollarWords.jl](https://github.com/pat-alt/TrillionDollarWords.jl). This short post introduces the package and its basic functionality.\n\n## TrillionDollarWords.jl\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://pat-alt.github.io/TrillionDollarWords.jl/stable/)\n[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/TrillionDollarWords.jl/dev/)\n[![Build Status](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml?query=branch%3Amain)\n[![Coverage](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl)\n[![Code Style: Blue](https://img.shields.io/badge/code%20style-blue-4495d1.svg)](https://github.com/invenia/BlueStyle)\n\n[TrillionDollarWords.jl](https://github.com/pat-alt/TrillionDollarWords.jl) is a light-weight package that provides Julia useres easy access to the Trillion Dollar Words dataset and model [@shah2023trillion].\n\n::: {.callout-note}\n\n## Disclaimer  \n\nPlease note that I am not the author of the Trillion Dollar Words paper nor am I affiliated with the authors. The package was developed as a by-product of our research and is not officially endorsed by the authors of the paper. \n\n:::\n\nYou can install the package from Julia's general registry as follows:\n\n``` julia\nusing Pkg\nPkg.add(url=\"TrillionDollarWords.jl\")\n```\n\nTo install the development version, use the following command:\n\n``` julia\nusing Pkg\nPkg.add(url=\"https://github.com/pat-alt/TrillionDollarWords.jl\")\n```\n\n### Basic Functionality\n\nThe package provides the following functionality:\n\n- Load pre-processed data.\n- Load the model proposed in the paper. \n- Basic model inference: compute forward passes and layer-wise activations.\n- Download pre-computed activations for probing the model.\n\nThe latter two are particularly useful for downstream tasks related to [mechanistic interpretability](https://en.wikipedia.org/wiki/Large_language_model#Interpretation). In times of increasing scrutiny of AI models, it is important to understand how they work and what they have learned. Mechanistic interpretability is a promising approach to this end, as it aims to understand the model's internal representations and how they relate to the task at hand. As we make abundantly clear in our own [paper](https://arxiv.org/abs/2402.03962) [@altmeyer2024position], interpretability is not a silver bullet, but merely a step towards understanding, monitoring and improving AI models.\n\n### Loading the Data\n\nThe Trillion Dollar Words dataset is a collection of preprocessed sentences around 40,000 time-stamped sentences from meeting minutes, press conferences and speeches by members of the Federal Open Market Committee (FOMC) [@shah2023trillion]. The total sample period spans from January, 1996, to October, 2022. In order to train various rule-based models and large language models (LLM) to classify sentences as either ‘hawkish’, ‘dovish’ or ‘neutral’, they have manually annotated a subset of around 2,500 sentences. The best-performing model, a large BERT model with around 355 million parameters, was open-sourced on [HuggingFace](https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves). The authors also link the sentences to market data, which makes it possible to study the relationship between language and financial markets. While the authors of the paper did publish their data, much of it is unfortunately scattered across CSV and Excel files stored in a public GitHub [repo](https://github.com/gtfintechlab/fomc-hawkish-dovish). I have collected and merged that data, yielding a combined dataset with indexed sentences and additional metadata that may be useful for downstream tasks.\n\nThe entire dataset of all available sentences used in the paper can be loaded as follows:\n\n::: {#c3a49107 .cell execution_count=2}\n``` {.julia .cell-code}\nusing TrillionDollarWords\nload_all_sentences() |> show\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n38358×8 DataFrame\n   Row │ sentence_id  doc_id  date        event_type        label    sentence  ⋯\n       │ Int64        Int64   Date        String31          String7  String    ⋯\n───────┼────────────────────────────────────────────────────────────────────────\n     1 │           1       1  1996-01-30  meeting minutes   neutral  The Commi ⋯\n     2 │           2       1  1996-01-30  meeting minutes   neutral  Consumer\n     3 │           3       1  1996-01-30  meeting minutes   dovish   Slower gr\n     4 │           4       1  1996-01-30  meeting minutes   hawkish  The deman\n     5 │           5       1  1996-01-30  meeting minutes   neutral  The recen ⋯\n     6 │           6       1  1996-01-30  meeting minutes   neutral  Nonfarm p\n     7 │           7       1  1996-01-30  meeting minutes   hawkish  Job growt\n     8 │           8       1  1996-01-30  meeting minutes   hawkish  Elsewhere\n     9 │           9       1  1996-01-30  meeting minutes   neutral  The outpu ⋯\n    10 │          10       1  1996-01-30  meeting minutes   neutral  Recent in\n    11 │          11       1  1996-01-30  meeting minutes   hawkish  Incoming\n   ⋮   │      ⋮         ⋮         ⋮              ⋮             ⋮               ⋱\n 38349 │       38349      63  2015-09-17  press conference  dovish   monetary\n 38350 │       38350      63  2015-09-17  press conference  neutral  When we—w ⋯\n 38351 │       38351      63  2015-09-17  press conference  neutral  It’s one\n 38352 │       38352      63  2015-09-17  press conference  neutral  1 Chair Y\n 38353 │       38353      63  2015-09-17  press conference  neutral  It remain\n 38354 │       38354      63  2015-09-17  press conference  neutral  And, reme ⋯\n 38355 │       38355      63  2015-09-17  press conference  neutral  It is tru\n 38356 │       38356      63  2015-09-17  press conference  dovish   To me, th\n 38357 │       38357      63  2015-09-17  press conference  hawkish  And since\n 38358 │       38358      63  2015-09-17  press conference  neutral  There hav ⋯\n                                                3 columns and 38337 rows omitted\n```\n:::\n:::\n\n\nThe combined dataset is also available as a `DataFrame` and can be loaded as follows:\n\n::: {#5d34ae52 .cell execution_count=3}\n``` {.julia .cell-code}\nload_all_data() |> show\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n524395×11 DataFrame\n    Row │ sentence_id  doc_id  date        event_type       label    sentence  ⋯\n        │ Int64        Int64   Date        String31         String7  String    ⋯\n────────┼───────────────────────────────────────────────────────────────────────\n      1 │           1       1  1996-01-30  meeting minutes  neutral  The Commi ⋯\n      2 │           2       1  1996-01-30  meeting minutes  neutral  Consumer\n      3 │           3       1  1996-01-30  meeting minutes  dovish   Slower gr\n      4 │           4       1  1996-01-30  meeting minutes  hawkish  The deman\n      5 │           5       1  1996-01-30  meeting minutes  neutral  The recen ⋯\n      6 │           6       1  1996-01-30  meeting minutes  neutral  Nonfarm p\n      7 │           7       1  1996-01-30  meeting minutes  hawkish  Job growt\n      8 │           8       1  1996-01-30  meeting minutes  hawkish  Elsewhere\n      9 │           9       1  1996-01-30  meeting minutes  neutral  The outpu ⋯\n     10 │          10       1  1996-01-30  meeting minutes  neutral  Recent in\n     11 │          11       1  1996-01-30  meeting minutes  hawkish  Incoming\n   ⋮    │      ⋮         ⋮         ⋮              ⋮            ⋮               ⋱\n 524386 │       29435     125  2022-10-12  speech           hawkish  However,\n 524387 │       29436     125  2022-10-12  speech           hawkish  My genera ⋯\n 524388 │       29429     125  2022-10-12  speech           neutral  I will fo\n 524389 │       29430     125  2022-10-12  speech           hawkish  Inflation\n 524390 │       29431     125  2022-10-12  speech           neutral  At this p\n 524391 │       29432     125  2022-10-12  speech           hawkish  If we do  ⋯\n 524392 │       29433     125  2022-10-12  speech           hawkish  However,\n 524393 │       29434     125  2022-10-12  speech           hawkish  To bring\n 524394 │       29435     125  2022-10-12  speech           hawkish  However,\n 524395 │       29436     125  2022-10-12  speech           hawkish  My genera ⋯\n                                               6 columns and 524374 rows omitted\n```\n:::\n:::\n\n\nAdditional functionality for data loading is available (see [docs](https://www.patalt.org/TrillionDollarWords.jl/dev/)).\n\n### Loading the Model\n\nThe model can be loaded with or without the classifier head (below without the head). Under the hood, this function uses [Transformers.jl](https://github.com/chengchingwen/Transformers.jl) to retrieve the model from [HuggingFace](https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves). Any keyword arguments accepted by `Transformers.HuggingFace.HGFConfig` can also be passed. For example, to load the model without the classifier head and enable access to layer-wise activations, the following command can be used:\n\n::: {#8375363f .cell execution_count=4}\n``` {.julia .cell-code}\nload_model(; load_head=false, output_hidden_states=true) |> show\n```\n\n::: {.cell-output .cell-output-stdout}\n```\nBaselineModel(GPT2TextEncoder(\n├─ TextTokenizer(MatchTokenization(CodeNormalizer(BPETokenization(GPT2Tokenization, bpe = CachedBPE(BPE(50000 merges))), codemap = CodeMap{UInt8 => UInt16}(3 code-ranges)), 5 patterns)),\n├─ vocab = Vocab{String, SizedArray}(size = 50265, unk = <unk>, unki = 4),\n├─ codemap = CodeMap{UInt8 => UInt16}(3 code-ranges),\n├─ startsym = <s>,\n├─ endsym = </s>,\n├─ padsym = <pad>,\n├─ trunc = 256,\n└─ process = Pipelines:\n  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)\n  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)\n  ╰─ target[(token, segment)] := SequenceTemplate{String}(<s>:<type=1> Input:<type=1> </s>:<type=1> (</s>:<type=1> Input:<type=1> </s>:<type=1>)...)(target.token)\n  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(256))(target.token)\n  ╰─ target[token] := TextEncodeBase.trunc_or_pad(256, <pad>, tail, tail)(target.token)\n  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)\n  ╰─ target := (target.token, target.attention_mask)\n), HGFRobertaModel(Chain(CompositeEmbedding(token = Embed(1024, 50265), position = ApplyEmbed(.+, FixedLenPositionEmbed(1024, 514), Transformers.HuggingFace.roberta_pe_indices(1,)), segment = ApplyEmbed(.+, Embed(1024, 1), Transformers.HuggingFace.bert_ones_like)), DropoutLayer<nothing>(LayerNorm(1024, ϵ = 1.0e-5))), Transformer<24>(PostNormTransformerBlock(DropoutLayer<nothing>(SelfAttention(MultiheadQKVAttenOp(head = 16, p = nothing), Fork<3>(Dense(W = (1024, 1024), b = true)), Dense(W = (1024, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5), DropoutLayer<nothing>(Chain(Dense(σ = NNlib.gelu, W = (1024, 4096), b = true), Dense(W = (4096, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5))), Branch{(:pooled,) = (:hidden_state,)}(BertPooler(Dense(σ = NNlib.tanh_fast, W = (1024, 1024), b = true)))), Transformers.HuggingFace.HGFConfig{:roberta, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}}(:use_cache => true, :torch_dtype => \"float32\", :vocab_size => 50265, :output_hidden_states => true, :hidden_act => \"gelu\", :num_hidden_layers => 24, :num_attention_heads => 16, :classifier_dropout => nothing, :type_vocab_size => 1, :intermediate_size => 4096, :max_position_embeddings => 514, :model_type => \"roberta\", :layer_norm_eps => 1.0e-5, :id2label => Dict(0 => \"LABEL_0\", 2 => \"LABEL_2\", 1 => \"LABEL_1\"), :_name_or_path => \"roberta-large\", :hidden_size => 1024, :transformers_version => \"4.21.2\", :attention_probs_dropout_prob => 0.1, :bos_token_id => 0, :problem_type => \"single_label_classification\", :eos_token_id => 2, :initializer_range => 0.02, :hidden_dropout_prob => 0.1, :label2id => Dict(\"LABEL_1\" => 1, \"LABEL_2\" => 2, \"LABEL_0\" => 0), :pad_token_id => 1, :position_embedding_type => \"absolute\", :architectures => [\"RobertaForSequenceClassification\"]))\n```\n:::\n:::\n\n\n### Basic Model Inference\n\nUsing the model and data, layer-wise activations can be computed as below (here for the first 5 sentences). When called on a `DataFrame`, the `layerwise_activations` returns a data frame that links activations to sentence identifiers. This makes it possible to relate activations to market data by using the `sentence_id` key. Alternatively, `layerwise_activations` also accepts a vector of sentences.\n\n::: {#99d7242a .cell execution_count=5}\n``` {.julia .cell-code}\ndf = load_all_sentences()\nmod = load_model(; load_head=false, output_hidden_states=true)\nn = 5\nqueries = df[1:n, :]\nlayerwise_activations(mod, queries) |> show\n```\n\n::: {.cell-output .cell-output-stdout}\n```\n122880×4 DataFrame\n    Row │ sentence_id  activations  layer  activation_id \n        │ Int64        Float32      Int64  Int64         \n────────┼────────────────────────────────────────────────\n      1 │           1   0.202931        1              1\n      2 │           1  -0.00693996      1              2\n      3 │           1   0.12731         1              3\n      4 │           1  -0.0129803       1              4\n      5 │           1   0.122843        1              5\n      6 │           1   0.258675        1              6\n      7 │           1   0.0466324       1              7\n      8 │           1   0.0318548       1              8\n      9 │           1   1.18888         1              9\n     10 │           1  -0.0386651       1             10\n     11 │           1  -0.116031        1             11\n   ⋮    │      ⋮            ⋮         ⋮          ⋮\n 122871 │           5  -0.769513       24           1015\n 122872 │           5   0.834678       24           1016\n 122873 │           5   0.212098       24           1017\n 122874 │           5  -0.556661       24           1018\n 122875 │           5   0.0957697      24           1019\n 122876 │           5   1.04358        24           1020\n 122877 │           5   1.71445        24           1021\n 122878 │           5   1.162          24           1022\n 122879 │           5  -1.58513        24           1023\n 122880 │           5  -1.01479        24           1024\n                                      122859 rows omitted\n```\n:::\n:::\n\n\n### Probe Findings\n\nFor our own [research](https://arxiv.org/abs/2402.03962) [@altmeyer2024position], we have been interested in probing the model. This involves using linear models to estimate the relationship between layer-wise transformer embeddings and some outcome variable of interest [@alain2018understanding]. To do this, we first had to run a single forward pass for each sentence through the RoBERTa model and store the layerwise emeddings. As we have seen above, the package ships with functionality for doing just that, but to save others valuable GPU hours we have archived activations of the hidden state on the first entity token for each layer as [artifacts](https://github.com/pat-alt/TrillionDollarWords.jl/releases/tag/activations_2024-01-17). To download the last-layer activations in an interactive Julia session, for example, users can proceed as follows:\n\n``` julia\nusing LazyArtifacts\n\njulia> artifact\"activations_layer_24\"\n```\n\nWe have found that despite the small sample size, the *FOMC-RoBERTa* model appears to have distilled useful representations for downstream tasks that it was not explicitly trained for. @fig-rmse-pca-128 below shows the average out-of-sample root mean squared error for predicting various market indicators from layer activations. Consistent with findings in related work [@alain2018understanding], we find that performance typically improves for layers closer to the final output layer of the transformer model. The measured performance is at least on par with baseline autoregressive models. For more information on this, see also my other recent [post](../spurious-sparks/index.qmd).\n\n![Out-of-sample root mean squared error (RMSE) for the linear probe plotted against *FOMC-RoBERTa*'s $n$-th layer for different indicators. The values correspond to averages computed across cross-validation folds, where we have used an expanding window approach to split the time series.](https://raw.githubusercontent.com/pat-alt/TrillionDollarWords.jl/11-activations-for-cls-head/dev/juliacon/rmse_pca_128.png){#fig-rmse-pca-128}\n\n## Intended Purpose and Goals\n\nI hope that this small package may be useful to members of the Julia community who are interested in the interplay between Economics, Finance and Artificial Intelligence. It should serve as a good starting point for the following ideas:\n\n- Fine-tune additional models on the classification task or other tasks of interest.\n- Further model probing, e.g. using other market indicators not discussed in the original paper.\n- Improve and extend the label annotations. \n\nAny contributions are very much welcome.\n\n## References\n\n",
    "supporting": [
      "index_files/figure-html"
    ],
    "filters": [],
    "includes": {}
  }
}