{
  "hash": "051e6a590c38fcde6af31a39a5c13db4",
  "result": {
    "engine": "julia",
    "markdown": "---\ntitle: Counterfactual Reasoning and Probabilistic Methods for Trustworthy AI\nsubtitle: Go/No Go Meeting 2022\nauthor: Patrick Altmeyer\nformat: \n  revealjs:\n    logo: /www/images/delft_logo.png\n    footer: |\n      Counterfactual Reasoning and Probabilistic Methods for Trustworthy AI with Applications in Finance -- Patrick Altmeyer\n    self-contained: true\n    smaller: true\n    scrollable: true\n    preview-links: auto\n    slide-number: true\n    transition: slide\n    background-transition: fade\n    fig-align: center\nengine: julia\nexecute:\n  eval: false\n  echo: true\ndraft: true\n---\n\n\n\n## Overview\n\n:::{.incremental}\n- Trustworthy AI üîÆ ... and how I think about it\n- Looking back üö©\n- The Road Ahead üéØ \n- Questions ‚ùì\n:::\n\n# Trustworthy AI üîÆ {background-image=\"/www/images/trust.jpeg\" background-color=\"black\"}\n\n## The Problem with Today's AI\n\n> From **human** to **data-driven** decision-making ...\n\n:::{.incremental}\n\n- Black-box models like deep neural networks are being deployed virtually everywhere.\n- Includes safety-critical and public domains: health care, autonomous driving, finance, ... \n- More likely than not that your loan or employment application is handled by an algorithm.\n\n::: \n\n. . .\n\n> ... where black boxes are recipe for disaster.\n\n:::{.incremental}\n- We have no idea what exactly we're cooking up ...\n    - Have you received an automated rejection email? Why didn't you \"mEet tHe sHoRtLisTiNg cRiTeRia\"? üôÉ\n- ... but we do know that some of it is junk. \n:::\n\n. . .\n\n![Adversarial attacks on deep neural networks. Source: @goodfellow2014explaining](/www/images/panda.png){#fig-panda width=50%}\n\n## Towards Trustworthy AI  {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nCounterfactual Reasoning\n:::\n:::\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack style=\"text-align: center;\"}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nCounterfactual Reasoning\n:::\n:::\n\n#### Current Standard in ML\n\nWe typically want to maximize the likelihood of observing $\\mathcal{D}_n$ under given parameters [@murphy2022probabilistic]:\n\n$$\n\\theta^* = \\arg \\max_{\\theta} p(\\mathcal{D}_n|\\theta)\n$$ {#eq-mle}\n\nCompute an MLE (or MAP) point estimate $\\hat\\theta = \\mathbb{E} \\theta^*$ and use **plugin** approximation for prediction:\n\n$$\np(y|x,\\mathcal{D}_n) \\approx p(y|x,\\hat\\theta)\n$$ {#eq-plugin}\n\n- In an ideal world we can just use parsimonious and interpretable models like GLM [@rudin2019stop], for which in many cases we can rely on asymptotic properties of $\\theta$ to quantify uncertainty.\n- In practice these models often have performance limitations.\n- Black-box models like deep neural networks are popular, but they are also the very opposite of parsimonious.\n\n#### Objective\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack style=\"text-align: center;\"}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nCounterfactual Reasoning\n:::\n:::\n\n#### Objective\n\n. . .\n\n> [...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. \n> [@wilson2020case]\n\nIn this setting it is often crucial to treat models probabilistically!\n\n$$\np(y|x,\\mathcal{D}_n) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D}_n)d\\theta\n$$ {#eq-bma}\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; align-items: center;\"}\nCounterfactual Reasoning\n:::\n:::\n\n> We can now make predictions -- great! But do we know how the predictions are actually being made?\n\n. . .\n\n#### Objective\n\nWith the model trained for its task, we are interested in understanding how its predictions change in response to input changes.\n\n$$\n\\nabla_x p(y|x,\\mathcal{D}_n;\\hat\\theta)\n$$ {#eq-ce-objective}\n\n:::{.incremental}\n- Counterfactual reasoning (in this context) boils down to simple questions: what if $x$ (factual) $\\Rightarrow$ $x\\prime$ (counterfactual)?\n- By strategically perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.\n- Counterfactual Explanations always have full fidelity by construction (as opposed to surrogate explanations, for example).\n:::\n\n. . .\n\n> Important to realize that we are keeping $\\hat\\theta$ constant!\n\n# Looking back üö© {background-image=\"/www/images/looking_back.jpeg\" background-color=\"black\"}\n\n## Some achievements ...\n\n1. Three presentations at JuliaCon 2022:\n    - [Explaining Black-Box Models through Counterfactuals](https://pretalx.com/juliacon-2022/talk/HU8FVH/)^[The relevant Julia package repository can be found here: [https://github.com/juliatrustworthyai/CounterfactualExplanations.jl](https://github.com/juliatrustworthyai/CounterfactualExplanations.jl). The repository that will be used to generate the relevant [JuliaCon Proceedings](https://proceedings.juliacon.org/) paper can be found here: [https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/tree/paper](https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/tree/paper).]\n    - [Effortless Bayesian Deep Learning through Laplace Redux](https://pretalx.com/juliacon-2022/talk/Z7MXFS/)^[The relevant Julia package repository can be found here: [https://github.com/juliatrustworthyai/LaplaceRedux.jl](https://github.com/juliatrustworthyai/LaplaceRedux.jl).]\n    - [Julia and Quarto: A Match Made in Heaven?](https://www.youtube.com/watch?v=9L5Oy3oGoTU)\n2. IEEE SaTML 2022 submission \"Endogenous Macrodynamics in Algorithmic Recourse\" (under review)\n    - The submitted paper can be found [here](https://anonymous.4open.science/r/AlgorithmicRecourseDynamics/paper/paper.pdf).\n    - The code for the companion Julia package can be found [here](https://anonymous.4open.science/r/AlgorithmicRecourseDynamics/README.md).\n3. Supervision of student Research Project\n\n## [Explaining Black-Box Models through Counterfactuals](https://pretalx.com/juliacon-2022/talk/HU8FVH/)\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable) [![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev) [![Build Status](https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/juliatrustworthyai/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain) [![Coverage](https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/juliatrustworthyai/CounterfactualExplanations.jl)\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n`CounterfactualExplanations.jl` is a package for generating Counterfactual Explanations (CE) and Algorithmic Recourse (AR) for black-box algorithms. Both CE and AR are related tools for explainable artificial intelligence (XAI). While the package is written purely in Julia, it can be used to explain machine learning algorithms developed and trained in other popular programming languages like Python and R. See below for short introduction and other resources or dive straight into the [docs](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/dev).\n\n![Turning a nine (9) into a four (4).](/www/images/MNIST_9to4.png)\n\n:::\n\n:::{.column width=\"50%\"}\n\n![A sad üê± on its counterfactual path to its cool dog friends.](https://raw.githubusercontent.com/juliatrustworthyai/CounterfactualExplanations.jl/main/docs/src//www/images/recourse_laplace.gif)\n\n::: \n\n::::\n\n## [Effortless Bayesian Deep Learning through Laplace Redux](https://pretalx.com/juliacon-2022/talk/Z7MXFS/)\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://juliatrustworthyai.github.io/LaplaceRedux.jl/stable) [![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://juliatrustworthyai.github.io/LaplaceRedux.jl/dev) [![Build Status](https://github.com/juliatrustworthyai/LaplaceRedux.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/juliatrustworthyai/LaplaceRedux.jl/actions/workflows/CI.yml?query=branch%3Amain) [![Coverage](https://codecov.io/gh/juliatrustworthyai/LaplaceRedux.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/juliatrustworthyai/LaplaceRedux.jl)\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n`LaplaceRedux.jl` (formerly `BayesLaplace.jl`) is a small package that can be used for effortless Bayesian Deep Learning and Logistic Regression trough Laplace Approximation. It is inspired by this Python [library](https://aleximmer.github.io/Laplace/index.html#setup) and its companion [paper](https://arxiv.org/abs/2106.14806).\n\n![Plugin Approximation (left) and Laplace Posterior (right) for simple artificial neural network.](/www/images/posterior_predictive_mlp.png)\n\n:::\n\n:::{.column width=\"50%\"}\n![Simulation of changing posteriour predictive distribution. Image by author.](https://raw.githubusercontent.com/juliatrustworthyai/LaplaceRedux.jl/main/dev/resources/juliacon22//www/images/intro.gif){width=\"70%\"}\n:::\n\n::::\n\n## [Endogenous Macrodynamics in AR](https://anonymous.4open.science/r/AlgorithmicRecourseDynamics/README.md) - motivation\n\n::: {.panel-tabset}\n\n### Motivation\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n> **TLDR**: We find that standard implementation of various SOTA approaches to AR can induce substantial domain and model shifts. We argue that these dynamics indicate that individual recourse generates hidden external costs and provide mitigation strategies.\n\n**Description**: In this work we investigate what happens if Algorithmic Recourse is actually implemented by a large number of individuals. The chart below illustrates what we mean by Endogenous Macrodynamics in Algorithmic Recourse: (a) we have a simple linear classifier trained for binary classification where samples from the negative class (y=0) are marked in blue and samples of the positive class (y=1) are marked in orange; (b) the implementation of AR for a random subset of individuals leads to a noticable domain shift; (c) as the classifier is retrained we observe a corresponding model shift [@upadhyay2021robust]; (d) as this process is repeated, the decision boundary moves away from the target class.\n\n:::\n\n:::{.column width=\"50%\"}\n![Proof of concept: repeated implementation of AR leads to domain and model shifts.](/www/images/poc.png){width=\"70%\"}\n\n> We argue that these shifts should be considered as an expected external cost of individual recourse and call for a paradigm shift from individual to collective recourse in these types of situations. \n\n![Mitigation strategies.](/www/images/mitigation.png){width=\"70%\"}\n\n:::\n::::\n\n### Findings\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n![Results for synthetic data.](/www/images/synthetic_results.png){width=\"70%\"}\n\n:::\n\n:::{.column width=\"50%\"}\n\n![Mitigation strategies applied to synthetic data.](/www/images/mitigation_synthetic_results.png){width=\"70%\"}\n\n:::\n\n::::\n\n:::\n\n# The Road Ahead üéØ {background-image=\"/www/images/looking_ahead.jpeg\"}\n\n## LaplaCE: Realistic and Surrogate-Free Counterfactual Explanations\n\n::: {.panel-tabset}\n\n### Motivating Example\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Good VAE\n\n. . .\n\n> The results in @fig-mnist-latent look great!\n\n![Turning a nine (9) into a four (4) using REVISE. It appears that the VAE is well-specified in this case.](/www/images/mnist_9to4_latent.png){#fig-mnist-latent width=\"80%\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### Bad VAE\n\n. . .\n\n> But things can also go wrong ...\n\nThe VAE used to generate the counterfactual in @fig-latent-fail is not expressive enough.\n\n![Turning a seven (7) into a nine (9) using REVISE with a weak VAE.](/www/images/mnist_7to9_latent.png){#fig-latent-fail width=\"60%\"}\n\n. . .\n\n> The counterfactual in @fig-wachter-fail is also valid ... what to do?\n\n![Turning a seven (7) into a nine (9) using generic search.](/www/images/mnist_7to9_wachter.png){#fig-wachter-fail width=\"60%\"}\n:::\n\n::::\n\n### Idea üí°\n\n> **TLDR**: Using Linearized Laplace Redux we can compute predictive uncertainty estimates for any neural network [@daxberger2021laplace]. By minimizing predictive uncertainty we can create realistic counterfactual explanations [@schut2021generating] without the need for surrogate generative models. \n\n**Description**: We propose LaplaCE: an effortless way to produce realistic Counterfactual Explanations for deep neural networks (DNN) using Laplace Approximation (LA). To address the need for realistic counterfactuals, existing work has primarily relied on separate generative models to learn the data generating process (e.g. @joshi2019realistic). While this an effective way to produce plausible and model-agnostic counterfactual explanations, it not only introduces an significant engineering overhead, but also reallocates the task of creating realistic model explanations from the model itself to the surrogate generative model. Recent work has shown that there is no need for any of this when working with probabilistic models that explicitly quantify their own uncertainty. Unfortunately, most models used in practice still do not fulfill that basic requirement, in which case we would like to have a way to quantify predictive uncertainty in a post-hoc fashion. Recent work on Bayesian Deep Learning has shown that LA can be used effectively in this context. By leveraging this finding we show that it is possible to generate realistic counterfactual explanations, without the need to restrict the class of models or rely on a separate generative model. \n\n**Potential venues**^[Likely submission deadline in brackets.]: FAccT (Dec '22), AIES (March '23), NeurIPS (May '23), SaTML (Sep '23)\n\n:::\n\n## \"Crack my code\" - can XAI teach users black-box behaviour? \n\n> **TLDR**: Using a gamified experiment we test if SOTA XAI methods can actually help users to understand the workings of a black-box model. \n\n**Description**: Do Counterfactual Explanations actually help users to understand the workings of a black-box model? In this work we investigate this question through gamified experiments. The idea is to set up an experiment as follows:\n\n1. Fit a neural network on a number of synthetic features with two classes and no inherent meaning.\n2. Generate a random sample from any of the two classes and show it to the user.\n3. Let user predict the class and compare it to actual prediction by neural network.\n4. Show user XAI explanation and reward user if their guess matches actual prediction.\n5. Repeat 1-4 many times. \n\nIf the XAI method is useful, the discrepancy between user guesses and neural network prediction should diminish over time. This project idea is inspired by an AIES 2022 paper that employs a similar framework [@dai2022counterfactual].\n\n**Potential venues**: AIES (March '23), JuliaCon (April '23)\n\n## Not your Typical Black-Box: the Dutch Childcare Benefits Scandal through the Lens of Counterfactual Explanations\n\n> **TLDR**: The automated decision-making system used by the Dutch tax authorities is opaque not because of its complexity, but rather by design. The goal of this work is to explore ways to explain such black-boxes through counterfactuals.\n\n**Description**: The Dutch childcare benefits scandal involved involved false fraud allegations based on an automated decision-making system (ADMS) used by the tax authorities. The ADMS was essentially a collection of spreadsheets containing hard-coded rules. The sheer quantity of spreadsheets has made it difficult for experts (Cynthia) to disentangle the inners workings and hence understand the behaviour of the ADMS. We can think of this a non-conventional black-box that is opaque not because of its complexity, but rather by design. We believe that these types of ADMS are still widely prevalent in industry and therefore should be considered as a different kind of threat to AI integrity. The goal of this work is to explore ways to explain such black-boxes through counterfactuals. This is a challenging and ambitious task, but a few strategies come to mind: 1) use brute force to search counterfactuals; 2) use a Growing Spheres [@laugel2017inverse] to generate counterfactuals; 3) derive a decision tree from the spreadsheets and generate counterfactuals for the tree.\n\n**Potential venues**: FAccT 2024\n\n## Counterfactual Explanations for Credit Risk Monitoring in Central Banks\n\n::::{.columns}\n:::{.column width=\"50%\"}\n**Description**: This fall I will give a seminar about Counterfactual Explanations and Algorithmic Recourse at the Bank of England. Bank researchers are interested in applying CE and AR to their bank risk prediction models.\n\n**Potential venues**: \n\n1. Blog post applying recent findings to sovereign default risk dataset. \n2. Contribution to BoE Staff Working Paper. \n\n:::\n:::{.column width=\"50%\"}\n![Source: Paul Fiedler on [Unsplash](https://unsplash.com/photos/M_S7pim3Hwg)](/www/images/banks.jpeg)\n:::\n::::\n\n## Counterfactual Explanations for Regression Problems\n\n::::{.columns}\n:::{.column width=\"50%\"}\n\n**Description**: The literature on Counterfactual Explanations almost exclusively focuses on classification problems. In Finance and Economics, however, the overwhelming majority of problems involve regression. Hence it is perhaps not altogether surprising that practitioners and researchers in these fields are largely unfamiliar with the CE and instead typically rely on surrogate explanations like LIME and SHAP to explain black-box models. Using @spooner2021counterfactual as a potential starting point, I would be interested in exploring how state-of-the-art CE approaches can be applied to regression problems. \n\n**Potential venues**: -\n\n:::\n:::{.column width=\"50%\"}\n\n![Source: @spooner2021counterfactual)](/www/images/regression.png)\n\n:::\n::::\n\n## Other plans\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n#### Priorities\n\n- Masters student supervision.\n- Contribute and/or participate in TU Delft Summer School on XAI.\n- Proposal for Google Summer of Code.\n- Increased co-operation with ING.\n\n#### Side projects\n\n- More blog post implementations of @murphy2022probabilistic. \n- Revise master's work on Deep VAR. \n\n:::\n\n:::{.column width=\"50%\"}\n![Photo by [Ivan Diaz](https://unsplash.com/@ivvndiaz?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)](/www/images/launch.jpeg){width=\"70%\"}\n:::\n\n::::\n\n# Questions ‚ùì\n\n# Hiddens {visibility=\"hidden\"}\n\n## [Endogenous Macrodynamics in Algorithmic Recourse](https://anonymous.4open.science/r/AlgorithmicRecourseDynamics/README.md) - findings (real world) {visibility=\"hidden\"}\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n![](/www/images/real_world_results.png)\n\n:::\n\n:::{.column width=\"50%\"}\n\n![](/www/images/mitigation_real_world_results.png)\n\n:::\n\n::::\n\n## References\n\n",
    "supporting": [
      "presentation_gonogo_files"
    ],
    "filters": [],
    "includes": {}
  }
}