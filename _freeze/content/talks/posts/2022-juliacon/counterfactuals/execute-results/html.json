{
  "hash": "a5fb8a7f6ffab8305faab615317ec191",
  "result": {
    "markdown": "---\ntitle: Explaining Black-Box Models through Counterfactuals\nsubtitle: JuliaCon 2022\nauthor: Patrick Altmeyer\nformat: \n  revealjs:\n    logo: /www/images/delft_logo.png\n    footer: |\n      Explaining Black-Box Models through Counterfactuals -- JuliaCon 2022 -- Patrick Altmeyer\n    self-contained: true\n    smaller: true\n    scrollable: true\n    preview-links: auto\n    slide-number: true\n    transition: slide\n    background-transition: fade\n    fig-align: center\nbibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib\nexecute:\n  eval: false\n  echo: true\n---\n\n\n\n## Overview\n\n:::{.incremental}\n- The Problem with Black Boxes ⬛\n    - What are black-box models? Why do we need explainability?\n- Enter: Counterfactual Explanations 🔮\n    - What are they? What are they not?\n- Counterfactual Explanations in Julia (and beyond!) 📦\n    - Introducing: [`CounterfactualExplanations.jl`](https://github.com/pat-alt/CounterfactualExplanations.jl){preview-link=\"false\"}\n    - Package architecture\n    - Usage examples - what can it do?\n- Goals and Ambitions 🎯\n    - Future developments - where can it go?\n    - Contributor's guide\n:::\n\n# The Problem with Black Boxes ⬛\n\n## Short Lists, Pandas and Gibbons\n\n> From **human** to **data-driven** decision-making ...\n\n:::{.incremental}\n\n- Black-box models like deep neural networks are being deployed virtually everywhere.\n- Includes safety-critical and public domains: health care, autonomous driving, finance, ... \n- More likely than not that your loan or employment application is handled by an algorithm.\n\n::: \n\n. . .\n\n> ... where black boxes are recipe for disaster.\n\n:::{.incremental}\n- We have no idea what exactly we're cooking up ...\n    - Have you received an automated rejection email? Why didn't you \"mEet tHe sHoRtLisTiNg cRiTeRia\"? 🙃\n- ... but we do know that some of it is junk. \n:::\n\n. . .\n\n![Adversarial attacks on deep neural networks. Source: @goodfellow2014explaining](/www/images/panda.png){#fig-panda width=50%}\n\n## \"Weapons of Math Destruction\"\n\n::::{.columns}\n\n:::{.column width=\"70%\"}\n\n> “You cannot appeal to (algorithms). They do not listen. Nor do they bend.”\n>\n> — Cathy O'Neil in [*Weapons of Math Destruction*](https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction), 2016\n\n![Cathy O’Neil. Source: Cathy O’Neil a.k.a. [mathbabe](https://mathbabe.org/contact/).](/www/images/cathy.webp){#fig-cathy width=60%}\n\n:::\n\n:::{.column width=\"30%\"}\n\n:::{.incremental}\n- If left unchallenged, these properties of black-box models can create undesirable dynamics in automated decision-making systems:\n  - Human operators in charge of the system have to rely on it blindly.\n  - Individuals subject to the decisions generally have no way to challenge their outcome.\n:::\n\n:::\n\n::::\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nCounterfactual Reasoning\n:::\n:::\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack style=\"text-align: center;\"}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nCounterfactual Reasoning\n:::\n:::\n\n#### Current Standard in ML\n\nWe typically want to maximize the likelihood of observing $\\mathcal{D}_n$ under given parameters [@murphy2022probabilistic]:\n\n$$\n\\theta^* = \\arg \\max_{\\theta} p(\\mathcal{D}_n|\\theta)\n$$ {#eq-mle}\n\nCompute an MLE (or MAP) point estimate $\\hat\\theta = \\mathbb{E} \\theta^*$ and use **plugin** approximation for prediction:\n\n$$\np(y|x,\\mathcal{D}_n) \\approx p(y|x,\\hat\\theta)\n$$ {#eq-plugin}\n\n- In an ideal world we can just use parsimonious and interpretable models like GLM [@rudin2019stop], for which in many cases we can rely on asymptotic properties of $\\theta$ to quantify uncertainty.\n- In practice these models often have performance limitations.\n- Black-box models like deep neural networks are popular, but they are also the very opposite of parsimonious.\n\n\n#### Objective\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack style=\"text-align: center;\"}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nCounterfactual Reasoning\n:::\n:::\n\n#### Objective\n\n. . .\n\n> [...] deep neural networks are typically very underspecified by the available data, and [...] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. \n> [@wilson2020case]\n\nIn this setting it is often crucial to treat models probabilistically!\n\n$$\np(y|x,\\mathcal{D}_n) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D}_n)d\\theta\n$$ {#eq-bma}\n\n. . .\n\n> Probabilistic models covered briefly today. More in my other talk on Laplace Redux ...\n\n## Towards Trustworthy AI {auto-animate=true auto-animate-easing=\"ease-in-out\"}\n\n::: {.r-hstack}\n::: {data-id=\"box3\" style=\"background: #389826; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nGround Truthing\n:::\n\n::: {data-id=\"box2\" style=\"background: #cb3c33; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; opacity: 0.5;\"}\nProbabilistic Models\n:::\n\n::: {data-id=\"box1\" style=\"background: #9558b2; width: 150px; height: 150px; border-radius: 50%; display: grid; place-content: center; text-align: center; align-items: center;\"}\nCounterfactual Reasoning\n:::\n:::\n\n> We can now make predictions -- great! But do we know how the predictions are actually being made?\n\n. . .\n\n#### Objective\n\nWith the model trained for its task, we are interested in understanding how its predictions change in response to input changes.\n\n$$\n\\nabla_x p(y|x,\\mathcal{D}_n;\\hat\\theta)\n$$ {#eq-ce-objective}\n\n:::{.incremental}\n- Counterfactual reasoning (in this context) boils down to simple questions: what if $x$ (factual) $\\Rightarrow$ $x\\prime$ (counterfactual)?\n- By strategically perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.\n- Counterfactual Explanations always have full fidelity by construction (as opposed to surrogate explanations, for example).\n:::\n\n. . .\n\n> Important to realize that we are keeping $\\hat\\theta$ constant!\n\n# Enter: Counterfactual Explanations 🔮\n\n## A Framework for Counterfactual Explanations\n\n> Even though [...] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the “black box”.\n> [@wachter2017counterfactual]\n\n\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n#### Framework\n\n. . .\n \nObjective originally proposed by @wachter2017counterfactual is as follows\n\n$$\n\\min_{x\\prime \\in \\mathcal{X}} h(x\\prime) \\ \\ \\ \\mbox{s. t.} \\ \\ \\ M(x\\prime) = t\n$$ {#eq-obj}\n\nwhere $h$ relates to the complexity of the counterfactual and $M$ denotes the classifier.\n\n. . .\n\nTypically this is approximated through regularization:\n\n$$\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) + \\lambda h(x\\prime)\n$$ {#eq-solution} \n\n:::\n\n:::{.column width=\"50%\"}\n\n#### Intuition\n\n. . .\n\n![A cat performing gradient descent in the feature space à la @wachter2017counterfactual.](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_mlp.gif){#fig-cat-mlp}\n\n:::\n::::\n\n\n## Counterfactuals ... as in Adversarial Examples?\n\n. . .\n\n> Yes and no! \n\nWhile both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.\n\n. . .\n\n> Effective counterfactuals should meet certain criteria ✅\n\n- **closeness**: the average distance between factual and counterfactual features should be small (@wachter2017counterfactual)\n- **actionability**: the proposed feature perturbation should actually be actionable (@ustun2019actionable, @poyiadzi2020face)\n- **plausibility**: the counterfactual explanation should be plausible to a human (@joshi2019realistic)\n- **unambiguity**: a human should have no trouble assigning a label to the counterfactual (@schut2021generating)\n- **sparsity**: the counterfactual explanation should involve as few individual feature changes as possible (@schut2021generating)\n- **robustness**: the counterfactual explanation should be robust to domain and model shifts (@upadhyay2021robust)\n- **diversity**: ideally multiple diverse counterfactual explanations should be provided (@mothilal2020explaining)\n- **causality**: counterfactual explanations reflect the structural causal model underlying the data generating process (@karimi2020algorithmic, @karimi2021algorithmic)\n\n## Counterfactuals ... as in Causal Inference?\n\n> NO!\n\n::::{.columns}::::\n:::{.column width='60%'}\n**Causal inference**: counterfactuals are thought of as unobserved states of the world that we would like to observe in order to establish causality.\n\n- The only way to do this is by actually interfering with the state of the world: $p(y|do(x),\\theta)$. \n- In practice we can only move some individuals to the counterfactual state of the world and compare their outcomes to a control group. \n- Provided we have controlled for confounders, properly randomized, ... we can estimate an average treatment effect: $\\hat\\theta$. \n\n**Counterfactual Explanations**: involves perturbing features **after** some model has been trained.\n\n- We end up comparing **modeled outcomes** $p(y|x,\\hat\\phi)$ and $p(y|x\\prime,\\hat\\phi)$ for individuals.\n- We have **not** magically solved causality.\n\n:::\n:::{.column width='40%'}\n<blockquote class=\"twitter-tweet\"><p lang=\"en\" dir=\"ltr\">The number of ostensibly pro data scientists confusing themselves into believing that &quot;counterfactual explanations&quot; capture real-world causality is just staggering🤦‍♀️. Where do we go from here? How can a community that doesn&#39;t even understand what&#39;s already known make advances?</p>&mdash; Zachary Lipton (\\@zacharylipton) <a href=\"https://twitter.com/zacharylipton/status/1538952312781168640?ref_src=twsrc%5Etfw\">June 20, 2022</a></blockquote> <script async src=\"https://platform.twitter.com/widgets.js\" charset=\"utf-8\"></script>\n:::\n::::\n\n\n## Probabilistic Methods for Counterfactual Explanations\n\nWhen people say that counterfactuals should look **realistic** or **plausible**, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:\n\n$$\nx\\prime \\sim p(x)\n$$\n\nBut how do we estimate $p(x)$? Two probabilistic approaches ...\n\n. . .\n\n::: {.panel-tabset}\n\n### APPROACH 1: use the model itself\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n@schut2021generating note that by maximizing predictive probabilities $\\sigma(M(x\\prime))$ for **probabilistic** models $M\\in\\mathcal{\\widetilde{M}}$ one implicitly minimizes **epistemic** and **aleotoric** uncertainty.\n\n$$\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) \\ \\ \\ , \\ \\ \\ M\\in\\mathcal{\\widetilde{M}}\n$$ {#eq-bayes} \n\n:::\n\n:::{.column width=\"50%\"}\n![A cat performing gradient descent in the feature space à la @schut2021generating](https://raw.githubusercontent.com/pat-alt/CounterfactualExplanations.jl/main/docs/src/www/recourse_laplace.gif){#fig-cat-laplace width=\"70%\"}\n:::\n\n::::\n\n### APPROACH 2: use some generative model\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\nInstead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model [@joshi2019realistic].\n\n$$\nz\\prime = \\arg \\min_{z\\prime}  \\ell(M(dec(z\\prime)),t) + \\lambda h(x\\prime) \n$$ {#eq-latent} \n\nand \n\n$$x\\prime = dec(z\\prime)$$\n\nwhere $dec(\\cdot)$ is the decoder function.\n:::\n\n:::{.column width=\"50%\"}\n![Counterfactual (yellow) generated through latent space search (right panel) following @joshi2019realistic. The corresponding counterfactual path in the feature space is shown in the left panel.](/www/images/example_3d.png){#fig-latent-3d width=\"80%\"}\n:::\n\n::::\n\n:::\n\n# Counterfactual Explanations in Julia (and beyond!)\n\n## Limited Software Availability  \n\n> Work currently scattered across different GitHub repositories ...\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n:::{.incremental}\n\n- Only one unifying Python library: CARLA [@pawelczyk2021carla].\n  - Comprehensive and (somewhat) extensible.\n  - But not language-agnostic and some desirable functionality not supported.\n  - Also not composable: each generator is treated as different class/entity.\n- Both R and Julia lacking any kind of implementation. \n\n:::\n:::\n\n:::{.column width=\"50%\"}\n![Photo by [Volodymyr Hryshchenko](https://unsplash.com/@lunarts?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/rubiks-cube?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).](/www/images/software.jpeg)\n:::\n\n::::\n\n\n## Enter: `CounterfactualExplanations.jl` 📦\n\n[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/stable) [![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/CounterfactualExplanations.jl/dev) [![Build Status](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/CounterfactualExplanations.jl/actions/workflows/CI.yml?query=branch%3Amain) [![Coverage](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/CounterfactualExplanations.jl)\n\n> ... until now!\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n:::{.incremental}\n- A unifying framework for generating Counterfactual Explanations.\n- Built in Julia, but essentially language agnostic:\n    - Currently supporting explanations for differentiable models built in Julia (e.g. Flux) and torch (R and Python).\n- Designed to be easily extensible through dispatch.\n- Designed to be composable allowing users and developers to combine different counterfactual generators.\n:::\n:::\n\n:::{.column width=\"50%\"}\n![Photo by [Denise Jans](https://unsplash.com/@dmjdenise?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/s/photos/rubiks-cube?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText).](/www/images/software2.jpeg){width=80%}\n:::\n\n::::\n\n. . .\n\n> Julia has an edge with respect to Trustworthy AI: it's open-source, uniquely transparent and interoperable 🔴🟢🟣\n\n# Package Architecture\n\n> Modular, composable, scalable! \n\n## Overview\n\n![Overview of package architecture. Modules are shown in red, structs in green and functions in blue.](/www/images/pkg_architecture.png){#fig-architecture width=\"70%\"}\n\n## Generators \n\n::: {.cell execution_count=2}\n``` {.julia .cell-code}\nusing CounterfactualExplanations, Plots, GraphRecipes\nplt = plot(AbstractGenerator, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))\nsavefig(plt, joinpath(www_path,\"generators.png\"))\n```\n:::\n\n\n![Type tree for `AbstractGenerator`.](/www/images/generators.png){#fig-generators width=\"60%\"}\n\n## Models\n\n::: {.cell execution_count=3}\n``` {.julia .cell-code}\nplt = plot(AbstractFittedModel, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))\nsavefig(plt, joinpath(www_path,\"models.png\"))\n```\n:::\n\n\n![Type tree for `AbstractFittedModel`.](/www/images/models.png){#fig-models width=\"60%\"}\n\n# Basic Usage\n\n## A simple example\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n1. Load and prepare some toy data.\n2. Select a random sample.\n3. Generate counterfactuals using different approaches.\n\n::: {.cell execution_count=4}\n``` {.julia .cell-code}\n# Data:\nusing Random\nRandom.seed!(123)\nN = 100\nusing CounterfactualExplanations\nxs, ys = toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Randomly selected factual:\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\n```\n:::\n\n\n:::\n\n:::{.column width=\"50%\"}\n\n\n\n![Synthetic data.](/www/images/example_data.png){#fig-data}\n:::\n\n::::\n\n## Generic Generator\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n#### Code \n\n. . .\n\nWe begin by instantiating the fitted model ...\n\n::: {.cell execution_count=6}\n``` {.julia .cell-code}\n# Model\nw = [1.0 1.0] # estimated coefficients\nb = 0 # estimated bias\nM = LogisticModel(w, [b])\n```\n:::\n\n\n. . .\n\n... then based on its prediction for $x$ we choose the opposite label as our target ...\n\n::: {.cell execution_count=7}\n``` {.julia .cell-code}\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n```\n:::\n\n\n. . .\n\n... and finally generate the counterfactual.\n\n::: {.cell execution_count=8}\n``` {.julia .cell-code}\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n#### Output\n\n. . .\n\n> ... et voilà!\n\n![Counterfactual path (left) and predicted probability (right) for `GenericGenerator`. The contour (left) shows the predicted probabilities of the classifier (Logistic Regression).](/www/images/example_generic.gif){#fig-generic}\n:::\n\n::::\n\n## Greedy Generator\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n#### Code \n \n. . .\n\nThis time we use a Bayesian classifier ...\n\n::: {.cell execution_count=10}\n``` {.julia .cell-code}\nusing LinearAlgebra\nΣ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\nμ = hcat(b, w)\nM = BayesianLogisticModel(μ, Σ)\n```\n:::\n\n\n. . .\n\n... and once again choose our target label as before ...\n\n::: {.cell execution_count=11}\n``` {.julia .cell-code}\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n```\n:::\n\n\n. . .\n\n... to then finally use greedy search to find a counterfactual.\n\n::: {.cell execution_count=12}\n``` {.julia .cell-code}\n# Counterfactual search:\nparams = GreedyGeneratorParams(\n  δ = 0.5,\n  n = 10\n)\ngenerator = GreedyGenerator(;params=params)\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n#### Output\n\n. . .\n\n> In this case the Bayesian approach yields a similar outcome.\n\n![Counterfactual path (left) and predicted probability (right) for `GreedyGenerator`. The contour (left) shows the predicted probabilities of the classifier (Bayesian Logistic Regression).](/www/images/example_greedy.gif){#fig-greedy}\n:::\n\n::::\n\n## REVISE Generator\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n#### Code \n\nUsing the same classifier as before we can either use the specific `REVISEGenerator` ...\n\n::: {.cell execution_count=14}\n``` {.julia .cell-code}\n# Counterfactual search:\ngenerator = REVISEGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n```\n:::\n\n\n. . .\n\n... or realize that that REVISE [@joshi2019realistic] just boils down to generic search in a latent space:\n\n::: {.cell execution_count=15}\n``` {.julia .cell-code}\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator,\n  latent_space=true\n)\n```\n:::\n\n\n\n\n:::\n\n::: {.column width=\"60%\"}\n#### Output\n\n. . . \n\n> We have essentially combined latent search with a probabilistic classifier (as in @antoran2020getting). \n\n![Counterfactual path (left) and predicted probability (right) for `REVISEGenerator`.](/www/images/example_revise.gif){#fig-greedy}\n:::\n\n::::\n\n## MNIST - Latent Space Search\n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n\n#### Good VAE\n\n. . .\n\nLoading pre-trained classifiers and VAE ...\n\n::: {.cell execution_count=17}\n``` {.julia .cell-code}\nX, ys = mnist_data() \nmodel = mnist_model() # simple MLP\n```\n:::\n\n\n. . .\n\n... instantiating model and attaching VAE.\n\n::: {.cell execution_count=18}\n``` {.julia .cell-code}\nM = FluxModel(model, likelihood=:classification_multi)\ncounterfactual_data = CounterfactualData(X,ys)\nvae = mnist_vae()\ncounterfactual_data.generative_model = vae\n```\n:::\n\n\n. . .\n\n> The results in @fig-mnist-latent look great!\n\n![Turning a nine (9) into a four (4) using REVISE. It appears that the VAE is well-specified in this case.](/www/images/mnist_9to4_latent.png){#fig-mnist-latent width=\"80%\"}\n\n:::\n\n::: {.column width=\"50%\"}\n\n#### Bad VAE\n\n. . .\n\n> But things can also go wrong ...\n\nThe VAE used to generate the counterfactual in @fig-latent-fail is not expressive enough.\n\n![Turning a seven (7) into a nine (9) using REVISE with a weak VAE.](/www/images/mnist_7to9_latent.png){#fig-latent-fail width=\"60%\"}\n\n. . .\n\n> The counterfactual in @fig-wachter-fail is also valid ... what to do?\n\n![Turning a seven (7) into a nine (9) using generic search.](/www/images/mnist_7to9_wachter.png){#fig-wachter-fail width=\"60%\"}\n:::\n\n::::\n\n# Customization\n\n## Custom Models - Deep Ensemble\n\n. . . \n\nLoading the pre-trained deep ensemble ...\n\n::: {.cell execution_count=19}\n``` {.julia .cell-code}\nensemble = mnist_ensemble() # deep ensemble\n```\n:::\n\n\n. . .\n\n**Step 1**: add composite type as subtype of `AbstractFittedModel`.\n\n::: {.cell execution_count=20}\n``` {.julia .cell-code}\nstruct FittedEnsemble <: Models.AbstractFittedModel\n    ensemble::AbstractArray\nend\n```\n:::\n\n\n. . .\n\n**Step 2**: dispatch `logits` and `probs` methods for new model type.\n\n::: {.cell execution_count=21}\n``` {.julia .cell-code}\nusing Statistics\nimport CounterfactualExplanations.Models: logits, probs\nlogits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3)\nprobs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3)\nM = FittedEnsemble(ensemble)\n```\n:::\n\n\n. . .\n\n> Results for a simple deep ensemble also look convincing!\n\n![Turning a nine (9) into a four (4) using generic (Wachter) and greedy search for MLP and deep ensemble.](/www/images/MNIST_9to4.png){#fig-mnist-schut width=\"80%\"}\n\n## Custom Models - Interoperability\n\nAdding support for `torch` models was easy! Here's how I implemented it for `torch` classifiers trained in R.\n\n. . .\n\n:::: {.columns}\n\n::: {.column width=\"40%\"}\n\n#### Source code\n\n. . .\n\n**Step 1**: add composite type as subtype of `AbstractFittedModel`\n\n> Implemented [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/models/differentiable/R.jl#L10){preview-link=\"false\"}.\n\n**Step 2**: dispatch `logits` and `probs` methods for new model type.\n\n> Implemented [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/models/differentiable/R.jl#L21){preview-link=\"false\"}.\n\n. . .\n\n**Step 3**: add gradient access.\n\n> Implemented [here](https://github.com/pat-alt/CounterfactualExplanations.jl/blob/19795f547d87d561d5906d7966e6fd1022fa8ceb/src/generators/gradient_based/functions.jl#L22){preview-link=\"false\"}.\n\n:::\n\n::: {.column width=\"40%\"}\n\n#### Unchanged API\n\n. . . \n\n\n\n::: {.cell execution_count=23}\n``` {.julia .cell-code}\nM = RTorchModel(model)\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n```\n:::\n\n\n\n\n![Counterfactual path (left) and predicted probability (right) for `GenericGenerator` and `RTorchModel`.](/www/images/example_interop.gif){#fig-interop}\n\n:::\n\n::::\n\n## Custom Generators\n\nIdea 💡: let's implement a generic generator with dropout!\n\n. . . \n\n:::: {.columns}\n\n::: {.column width=\"50%\"}\n#### Dispatch\n\n. . .\n\n**Step 1**: create a subtype of `AbstractGradientBasedGenerator` (adhering to some basic rules).\n\n::: {.cell execution_count=25}\n``` {.julia .cell-code}\n# Constructor:\nabstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end\nstruct DropoutGenerator <: AbstractDropoutGenerator\n    loss::Symbol # loss function\n    complexity::Function # complexity function\n    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints \n    λ::AbstractFloat # strength of penalty\n    ϵ::AbstractFloat # step size\n    τ::AbstractFloat # tolerance for convergence\n    p_dropout::AbstractFloat # dropout rate\nend\n```\n:::\n\n\n. . . \n\n**Step 2**: implement logic for generating perturbations.\n\n::: {.cell execution_count=26}\n``` {.julia .cell-code}\nimport CounterfactualExplanations.Generators: generate_perturbations, ∇\nusing StatsBase\nfunction generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::State)\n    𝐠ₜ = ∇(generator, counterfactual_state.M, counterfactual_state) # gradient\n    # Dropout:\n    set_to_zero = sample(1:length(𝐠ₜ),Int(round(generator.p_dropout*length(𝐠ₜ))),replace=false)\n    𝐠ₜ[set_to_zero] .= 0\n    Δx′ = - (generator.ϵ .* 𝐠ₜ) # gradient step\n    return Δx′\nend\n```\n:::\n\n\n:::\n\n::: {.column width=\"50%\"}\n#### Unchanged API\n\n. . .\n\n::: {.cell execution_count=27}\n``` {.julia .cell-code}\n# Instantiate:\nusing LinearAlgebra\ngenerator = DropoutGenerator(\n    :logitbinarycrossentropy,\n    norm,\n    nothing,\n    0.1,\n    0.1,\n    1e-5,\n    0.5\n)\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n```\n:::\n\n\n\n\n![Counterfactual path (left) and predicted probability (right) for custom `DropoutGenerator` and `RTorchModel`.](/www/images/example_dropout.gif){#fig-dropout}\n\n:::\n\n::::\n\n# Goals and Ambitions 🎯\n\n## JuliaCon 2022 and beyond\n\n::::{.columns}\n\n:::{.column width=\"50%\"}\n\n:::{.fragment .semi-fade-out fragment-index=4}\n#### To JuliaCon ...\n\n:::{.fragment .strike fragment-index=1}\nDevelop package, register and submit to [JuliaCon 2022](https://juliacon.org/2022/).\n:::\n\n:::{.fragment .strike fragment-index=2}\nNative support for deep learning models (`Flux`, `torch`).\n:::\n\n:::{.fragment .strike fragment-index=3}\nAdd latent space search.\n:::\n:::\n\n#### ... and beyond\n\n. . .\n\n- Add more generators:\n  - DiCE [@mothilal2020explaining]\n  - ROAR [@upadhyay2021robust]\n  - MINT [@karimi2021algorithmic]\n\n. . .\n\n- Add support for more models: \n  - `MLJ`, `GLM`, ...\n  - Non-differentiable\n\n. . .\n\n- Enhance preprocessing functionality.\n\n. . .\n\n- Extend functionality to regression problems.\n\n. . .\n\n- Use `Flux` optimizers.\n\n. . .\n\n- ...\n:::\n\n:::{.column width=\"50%\"}\n![Photo by [Ivan Diaz](https://unsplash.com/@ivvndiaz?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)](/www/images/launch.jpeg){width=\"70%\"}\n:::\n\n::::\n\n## More Resources 📚\n\n::::{.columns}\n\n:::{.column width=\"60%\"}\n> Read on ...\n\n- Blog post introducing CE: [[TDS](https://towardsdatascience.com/individual-recourse-for-black-box-models-5e9ed1e4b4cc), [homepage](https://www.paltmeyer.com/blog/posts/individual-recourse-for-black-box-models/)].\n- Blog post introducing package: [[TDS](https://towardsdatascience.com/a-new-tool-for-explainable-ai-65834e757c28), [homepage](https://www.paltmeyer.com/blog/posts/a-new-tool-for-explainable-ai/)].\n- Package [docs](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/) with lots of examples.\n\n> ... or get involved! 🤗\n\n- [Contributor's Guide](https://www.paltmeyer.com/CounterfactualExplanations.jl/dev/contributing/)\n\n:::\n\n:::{.column width=\"40%\"}\n\n<img src=\"/www/images/profile.jpg\" height=\"auto\" width=\"250\" style=\"border-radius:50%; display: block; margin-left: auto; margin-right: auto;\">\n\n<div style=\"text-align: center;\">\n  <p style=\"display: inline; vertical-align: middle\"> \n    <a href=\"https://www.linkedin.com/in/patrick-altmeyer-a2a25494/\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n      <font style=\"\">\n        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/d0fc399dee4218d1e0e0399b8947acab.png\" alt=\"LinkedIn (Personal)\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n      </font>\n    </a>\n    <a href=\"https://twitter.com/paltmey\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n      <font style=\"\">\n        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/3949237f892004c237021ac9e3182b1d.png\" alt=\"Twitter\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n      </font>\n    </a>\n    <a href=\"https://github.com/pat-alt\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n      <font style=\"\">\n        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/47f4eb2d0082a8a3611d614b75a09db8.png\" alt=\"Github\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n      </font>\n    </a>\n    <a href=\"https://medium.com/@patrick.altmeyer\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n      <font style=\"\">\n        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/175f49662614345cb7dbb95fce3f88af.png\" alt=\"Medium\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n      </font>\n    </a>\n  </p>\n</div>\n\n<img src=\"/www/images/qr.png\" height=\"auto\" width=\"100\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n:::\n\n::::\n\n# Hidden {visibility=\"hidden\"}\n\n## Explainable AI (XAI) {visibility=\"hidden\"}\n\n- *interpretable* = inherently interpretable model, no extra tools needed (GLM, decision trees, rules, ...)  [@rudin2019stop]\n- *explainable* = inherently not interpretable model, but explainable through XAI\n\n#### Post-hoc Explainability:\n- Local **surrogate explainers** like LIME and Shapley: useful and popular, but ... \n    - ... can be easily fooled [@slack2020fooling]\n    - ... rely on reasonably interpretable features.\n    - ... rely on the concept of fidelity.\n- **Counterfactual explanations** explain how inputs into a system need to change for it to produce different decisions. \n    - Always full-fidelity, since no proxy involved. \n    - Intuitive interpretation and straight-forward implemenation.\n    - Works well with Bayesian models. Clear link to Causal Inference. \n    - Does not rely on interpretable features.\n- Realistic and actionable changes can be used for the purpose of **algorithmic recourse**.\n\n## Feature Constraints {visibility=\"hidden\"}\n\n::: {.panel-tabset}\n\n### Domain constraint\n\n![](/www/images/mutability_domain_2.gif)\n\n### Code\n\nMutability constraints can be added at the preprocessing stage:\n\n```{.julia}\ncounterfactual_data = CounterfactualData(X,ys';domain=[(-Inf,Inf),(-Inf,-0.5)])\n```\n\n:::\n\n## Research Topics (1) - Student Project {visibility=\"hidden\"}\n\n> What happens once AR has actually been implemented? 👀\n\n:::{.incremental}\n- Towards robust AR: protection against exogenous domain and model shifts [@upadhyay2021robust]\n- What about endogenous model shifts?\n:::\n\n![](/www/images/bayesian.gif){fig-align=\"center\" width=800px} \n\n## Research Topics (2) {visibility=\"hidden\"}\n\n:::{.incremental}\n- An effortless way to incorporate model uncertainty (w/o need for expensive generative model): *Laplace Redux*.\n- Counterfactual explanations for time series data.\n- Is CE really more intuitive? Could run a user-based study like in @kaur2020interpreting.\n- More ideas form your side? 🤗\n:::\n\n## References \n\n",
    "supporting": [
      "counterfactuals_files/figure-revealjs"
    ],
    "filters": [],
    "includes": {}
  }
}