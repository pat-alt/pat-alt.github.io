---
title: Prediction Intervals for any Regression Model
subtitle: Conformal Prediction in Julia --- Part 3
date: '2022-12-12'
categories:
  - probabilistic programming
  - uncertainty
  - conformal prediction
  - regression
  - Julia
description: >-
  This third post introduces conformal regression by going through a standard machine learning workflow using [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/) and [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl).
image: www/intro.gif
jupyter: julia-1.10
draft: false
---

```{julia}
#| echo: false

using Pkg; Pkg.activate("blog/posts/conformal-regression")
using ConformalPrediction
using Distributions
using Markdown
using MLJBase
using MLJModels
using NearestNeighborModels: KNNRegressor
using Plots
using Random
using TaijaPlotting
Random.seed!(2022)
theme(:wong)
www_path = "blog/posts/conformal-regression/www"
```

<div class="intro-gif">
  <figure>
    <img src="www/intro.gif">
    <figcaption>Conformal Prediction intervals for different<br>coverage rates. As coverage grows, so does<br>the width of the prediction interval.</figcaption>
  </figure>
</div>

This is the third (and for now final) part of a series of posts that introduce Conformal Prediction in Julia using [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl). The first [post](../conformal-prediction/index.qmd) introduced Conformal Prediction for supervised classification tasks: we learned that conformal classifiers produce set-valued predictions that are guaranteed to include the true label of a new sample with a certain probability. In the second [post](../conformal-image-classifier/) we applied these ideas to a more hands-on example: we saw how easy it is to use [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl) to conformalize a Deep Learning image classifier. 

In this post, we will look at regression models instead, that is supervised learning tasks involving a continuous outcome variable. Regression tasks are as ubiquitous as classification tasks. For example, we might be interested in using a machine learning model to predict house prices or the inflation rate of the Euro or the parameter size of the next large language model. In fact, many readers may be more familiar with regression models than classification, in which case it may also be easier for you to understand Conformal Prediction (CP) in this context. 

## üìñ Background

Before we start, let's briefly recap what CP is all about. Don't worry, we're not about to deep-dive into methodology. But just to give you a high-level description upfront:

> Conformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions.
>
> --- @angelopoulos2021gentle ([arXiv](https://arxiv.org/pdf/2107.07511.pdf))

Intuitively, CP works under the premise of turning heuristic notions of uncertainty into rigorous uncertainty estimates through repeated sampling or the use of dedicated calibration data. 

In what follows we will explore what CP can do by going through a standard machine learning workflow using [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/) and [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl). There will be less focus on how exactly CP works, but references will point you to additional resources. 

:::{.callout-tip}
## Interactive Version

This post is also available as a fully interactive [`Pluto.jl`](https://github.com/fonsp/Pluto.jl) üéà notebook hosted on [binder](https://mybinder.org/): [![Binder](https://mybinder.org/badge_logo.svg)](https://binder.plutojl.org/v0.19.12/open?url=https%253A%252F%252Fraw.githubusercontent.com%252Fpat-alt%252FConformalPrediction.jl%252Fmain%252Fdocs%252Fpluto%252Fintro.jl){target="_blank"}

In my own experience, this may take some time to load, certainly long enough to get yourself a hot beverage ‚òï or first read on here. But I promise you that the wait is worth it!
:::

## üìà Data

Most machine learning workflows start with data. For illustrative purposes we will work with synthetic data. The helper function below can be used to generate some regression data. 

```{julia}
function get_data(;N=1000, xmax=3.0, noise=0.5, fun::Function=fun(X) = X * sin(X))
    # Inputs:
    d = Distributions.Uniform(-xmax, xmax)
    X = rand(d, N)
    X = MLJBase.table(reshape(X, :, 1))

    # Outputs:
    Œµ = randn(N) .* noise
    y = @.(fun(X.x1)) + Œµ
    y = vec(y)
    return X, y
end
```

@fig-data illustrates our observations (dots) along with the ground-truth mapping from inputs to outputs (line). We have defined that mapping $f: \mathcal{X} \mapsto \mathcal{Y}$ as follows:

```{julia}
f(X) = X * cos(X)
```


```{julia}
#| output: true
#| echo: false
#| fig-cap: "Some synthetic regression data. Observations are shown as dots. The ground-truth mapping from inputs to outputs is shown as a dashed line."
#| label: fig-data

xmax = 5
N = 1000
X, y = get_data(N=N, xmax=xmax, fun=f)
scatter(X.x1, y, label="Observed data")
xrange = range(-xmax, xmax, length=50)
plot!(
    xrange,
    @.(f(xrange)),
    lw=4,
    label="Ground truth",
    ls=:dash,
    colour=:black,
)
```

## üèãÔ∏è Model Training using [`MLJ`](https://alan-turing-institute.github.io/MLJ.jl/dev/)

[`ConformalPrediction.jl`]((https://github.com/juliatrustworthyai/ConformalPrediction.jl)) is interfaced to [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/) [@blaom2020mlj]: a comprehensive Machine Learning Framework for Julia. `MLJ.jl` provides a large and growing suite of popular machine learning models that can be used for supervised and unsupervised tasks. Conformal Prediction is a model-agnostic approach to uncertainty quantification, so it can be applied to any common supervised machine learning model. 

The interface to `MLJ.jl` therefore seems natural: any (supervised) `MLJ.jl` model can now be conformalized using `ConformalPrediction.jl`. By leveraging existing `MLJ.jl` functionality for common tasks like training, prediction and model evaluation, this package is light-weight and scalable. Now let's see how all of that works ...

To start with, let's split our data into a training and test set:

```{julia}
train, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)
```

Now let's define a model for our regression task:

```{julia}
Model = @load KNNRegressor pkg = NearestNeighborModels
model = Model()
```

:::{.callout-tip}
## Have it your way!

Think this dataset is too simple? Wondering why on earth I'm not using XGBoost for this task? In the interactive [version](https://binder.plutojl.org/v0.19.12/open?url=https%253A%252F%252Fraw.githubusercontent.com%252Fpat-alt%252FConformalPrediction.jl%252Fmain%252Fdocs%252Fpluto%252Fintro.jl){target="_blank"} of this post you have full control over the data and the model. Try it out!
:::

Using standard `MLJ.jl` workflows let us now first train the unconformalized model. We first wrap our model in data:

```{julia}
mach_raw = machine(model, X, y)
```

Then we fit the machine to the training data:

```{julia}
MLJBase.fit!(mach_raw, rows=train, verbosity=0)
```

@fig-point below shows the resulting point predictions for the test data set:

```{julia}
#| output: true
#| echo: false
#| fig-cap: Point predictions for our machine learning model.
#| label: fig-point

Xtest = MLJBase.matrix(selectrows(X, test))
ytest = y[test]
yÃÇ = MLJBase.predict(mach_raw, Xtest)
scatter(vec(Xtest), vec(ytest), label="Observed")
_order = sortperm(vec(Xtest))
plot!(vec(Xtest)[_order], vec(yÃÇ)[_order], lw=4, label="Predicted")
plot!(
    xrange,
    @.(f(xrange)),
    lw=2,
    ls=:dash,
    colour=:black,
    label="Ground truth",
)
```

How is our model doing? It's never quite right, of course, since predictions are estimates and therefore uncertain. Let's see how we can use Conformal Prediction to express that uncertainty.

## üî• Conformalizing the Model

We can turn our `model` into a conformalized model in just one line of code:

```{julia}
conf_model = conformal_model(model)
```

By default `conformal_model` creates an Inductive Conformal Regressor (more on this below) when called on a `<:Deterministic` model. This behaviour can be changed by using the optional `method` key argument.

To train our conformal model we can once again rely on standard `MLJ.jl` workflows. We first wrap our model in data:

```{julia}
mach = machine(conf_model, X, y)
```

Then we fit the machine to the data:

```{julia}
MLJBase.fit!(mach, rows=train, verbosity=0)
```

Now let us look at the predictions for our test data again. The chart below shows the results for our conformalized model. Predictions from conformal regressors are range-valued: for each new sample the model returns an interval $(y_{\text{lb}},y_{\text{ub}})\in\mathcal{Y}$ that covers the test sample with a user-specified probability $(1-\alpha)$, where $\alpha$ is the expected error rate. This is known as the **marginal coverage guarantee** and it is proven to hold under the assumption that training and test data are exchangeable. 

```{julia}
#| output: true
#| echo: false
#| fig-cap: Prediction intervals for our conformalized machine learning model.
#| label: fig-interval

coverage = 0.9
_conf_model = conformal_model(model, coverage=coverage)
_mach = machine(_conf_model, X, y)
MLJBase.fit!(_mach, rows=train, verbosity=0)
plot(_mach.model, _mach.fitresult, Xtest, ytest, zoom=0, observed_lab="Test points")
plot!(
    xrange,
    @.(f(xrange)),
    lw=2,
    ls=:dash,
    colour=:black,
    label="Ground truth",
)
```

```{julia}
#| echo: false

bg_color = :white
coverages = vcat(range(0.5,1.0,length=50),range(1.0,0.5,length=50))
anim = @animate for _cov in coverages 
    _conf_model = conformal_model(model, coverage=_cov)
    _mach = machine(_conf_model, X, y)
    MLJBase.fit!(_mach, rows=train, verbosity=0)
    plot(
      _mach.model, _mach.fitresult, Xtest, ytest, zoom=0, observed_lab="Test points",
      axis=([],false), legend=false, bg_color=bg_color, bg_color_inside=bg_color,
      size = (300, 300),
      dpi = 300,
    )
end
gif(anim, joinpath(www_path,"intro.gif"), fps=25)
```

```{julia}
#| echo: false

anim = @animate for _cov in coverages 
    _conf_model = conformal_model(model, coverage=_cov)
    _mach = machine(_conf_model, X, y)
    MLJBase.fit!(_mach, rows=train, verbosity=0)
    plot(
      _mach.model, _mach.fitresult, Xtest, ytest, zoom=0, observed_lab="Test points",
      axis=([],false), legend=false, bg_color=bg_color, bg_color_inside=bg_color,
      size = (900, 300),
      dpi = 300,
    )
end
gif(anim, joinpath(www_path,"medium.gif"), fps=25)
```

Intuitively, a higher coverage rate leads to larger prediction intervals: since a larger interval covers a larger subspace of $\mathcal{Y}$, it is more likely to cover the true value.

I don't expect you to believe me that the marginal coverage property really holds. In fact, I couldn't believe it myself when I first learned about it. If you like mathematical proofs, you can find one in this [tutorial](https://arxiv.org/pdf/2107.07511.pdf), for example. If you like convincing yourself through empirical observations, read on below ...

## üßê Evaluation

To verify the marginal coverage property empirically we can look at the empirical coverage rate of our conformal predictor (see Section 3 of the [tutorial](https://arxiv.org/pdf/2107.07511.pdf) for details). To this end our package provides a custom performance measure `emp_coverage` that is compatible with `MLJ.jl` model evaluation workflows. In particular, we will call `evaluate!` on our conformal model using `emp_coverage` as our performance metric. The resulting empirical coverage rate should then be close to the desired level of coverage.

```{julia}
#| output: true

model_evaluation =
    evaluate!(_mach, operation=MLJBase.predict, measure=emp_coverage, verbosity=0)
println("Empirical coverage: $(round(model_evaluation.measurement[1], digits=3))")
println("Coverage per fold: $(round.(model_evaluation.per_fold[1], digits=3))")
```

```{julia}
#| echo: false
#| output: true

ncal = Int(conf_model.train_ratio * N)
if model_evaluation.measurement[1] < coverage
    Markdown.parse(
        """
    > ‚ùå‚ùå‚ùå Oh no! We got an empirical coverage rate that is slightly lower than desired ü•≤ ... what's happened? 
    
    The coverage property is "marginal" in the sense that the probability is averaged over the randomness in the data. For most purposes a large enough calibration set size (``n>1000``) mitigates that randomness enough. Depending on your choices above, the calibration set may be quite small (set to $ncal), which can lead to **coverage slack** (see Section 3 in the [tutorial](https://arxiv.org/pdf/2107.07511.pdf)).
    """
    )
else
  Markdown.parse(
    """
    > ‚úÖ ‚úÖ ‚úÖ  Great! We got an empirical coverage rate that is slightly higher than desired üòÅ ... but why isn't it exactly the same? 
    
    In most cases it will be slightly higher than desired, since ``(1-\\alpha)`` is a lower bound. But note that it can also be slightly lower than desired. That is because the coverage property is "marginal" in the sense that the probability is averaged over the randomness in the data. For most purposes a large enough calibration set size (``n>1000``) mitigates that randomness enough. Depending on your choices above, the calibration set may be quite small (set to $ncal), which can lead to **coverage slack** (see Section 3 in the [tutorial](https://arxiv.org/pdf/2107.07511.pdf)).
    """
  )
end
```


### *So what's happening under the hood?*
		
Inductive Conformal Prediction (also referred to as Split Conformal Prediction) broadly speaking works as follows:

1. Partition the training into a proper training set and a separate calibration set
2. Train the machine learning model on the proper training set.
3. Using some heuristic notion of uncertainty (e.g., absolute error in the regression case), compute nonconformity scores using the calibration data and the fitted model.
4. For the given coverage ratio compute the corresponding quantile of the empirical distribution of nonconformity scores.
5. For the given quantile and test sample $X_{\text{test}}$, form the corresponding conformal prediction set like so: $C(X_{\text{test}})=\{y:s(X_{\text{test}},y) \le \hat{q}\}$

## üîÉ Recap

This has been a super quick tour of [`ConformalPrediction.jl`](https://github.com/juliatrustworthyai/ConformalPrediction.jl). We have seen how the package naturally integrates with [`MLJ.jl`](https://alan-turing-institute.github.io/MLJ.jl/dev/), allowing users to generate rigorous predictive uncertainty estimates for any supervised machine learning model. 

### *Are we done?*

Quite cool, right? Using a single API call we are able to generate rigorous prediction intervals for all kinds of different regression models. Have we just solved predictive uncertainty quantification once and for all? Do we even need to bother with anything else? Conformal Prediction is a very useful tool, but like so many other things, it is not the final answer to all our problems. In fact, let's see if we can take CP to its limits.

The helper function to generate data from above takes an optional argument `xmax`. By increasing that value, we effectively expand the domain of our input. Let's do that and see how our conformal model does on this new out-of-domain data.

```{julia}
#| output: true
#| echo: false
#| fig-cap: Prediction intervals for our conformalized machine learning model applied to out-of-domain data.
#| label: fig-ood

xmax_ood = xmax+5
Xood, yood = get_data(N=N, xmax=xmax_ood, fun=f)
plot(_mach.model, _mach.fitresult, Xood, yood, zoom=0, observed_lab="Test points")
xood_range = range(-xmax_ood, xmax_ood, length=50)
plot!(
    xood_range,
    @.(f(xood_range)),
    lw=2,
    ls=:dash,
    colour=:black,
    label="Ground truth",
)
```

> Whooooops ü§ï ... looks like we're in trouble: in @fig-ood the prediction intervals do not cover out-of-domain test samples well. What happened here?
	
By expanding the domain of out inputs, we have violated the exchangeability assumption. When that assumption is violated, the marginal coverage property does not hold. But do not despair! There are ways to deal with this.

## üìö Read on

If you are curious to find out more, be sure to read on in the [docs](https://www.patalt.org/ConformalPrediction.jl/stable/). There are also a number of useful resources to learn more about Conformal Prediction, a few of which I have listed below:

- *A Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification* by Angelopoulos and Bates ([2022](https://arxiv.org/pdf/2107.07511.pdf)).
- *Awesome Conformal Prediction* repository by Manokhin ([2022](https://github.com/valeman/awesome-conformal-prediction))
- **MAPIE**: a comprehensive Python [library](https://mapie.readthedocs.io/en/latest/index.html) for conformal prediction.
- My previous two blog posts.

Enjoy!

## References