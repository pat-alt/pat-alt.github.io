{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Spurious Sentience\n",
        "subtitle: On the Unsurprising Finding of Patterns in Latent Spaces\n",
        "date: '2023-10-06'\n",
        "categories:\n",
        "  - spurious sentience\n",
        "  - artificial intelligence\n",
        "  - llm\n",
        "description: This is an opinionated post that questions the idea that the finding of patterns in latent embeddings of artificial neural networks is surprising.\n",
        "image: www/intro.gif\n",
        "execute:\n",
        "  code-fold: true\n",
        "draft: true\n",
        "---"
      ],
      "id": "95ba085c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| echo: false\n",
        "\n",
        "# Set up:\n",
        "BLOG_DIR = \"blog/posts/spurious-sentience\"\n",
        "using Pkg; Pkg.activate(BLOG_DIR)\n",
        "\n",
        "# Packages:\n",
        "using CSV\n",
        "using DataFrames\n",
        "using Dates\n",
        "using Flux\n",
        "using LinearAlgebra\n",
        "using Metal\n",
        "using Plots\n",
        "using Plots.PlotMeasures\n",
        "using Printf\n",
        "using TidierData\n",
        "\n",
        "# Utility functions:\n",
        "include(joinpath(\"$(pwd())\", BLOG_DIR, \"utils.jl\"))"
      ],
      "id": "6432dd11",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We humans are prone to seek patterns everywhere. Meaningful patterns have often proven to help us make sense of our past, navigate our presence and predict the future. Our society is so invested in finding patterns that today it seems we are more willing than ever to outsource this task to an Artificial Intelligence (AI): an omniscient oracle that leads us down the right path. Unfortunately, history has shown time and again that patterns are double-edged swords: if we attribute the wrong meaning to them, they may lead us nowhere at all, or worse, they may lead us down the dark roads. \n",
        "\n",
        "In statistics, misleading patterns are referred to as **spurious relationships**: purely associational relationships between two or more variables that are not causally related to each other at all. The world is full of these and as good as we as species may be at recognizing patterns, we typically have a much harder time discerning spurious relationships from causal ones. Despite new and increased momentum in scientific fields concerned with causal inference and discovery, I am also willing to go out on a limb and claim that we are not about to finally reach the top of Judea Pearl's Causal Ladder through the means of Causal AI.\n",
        "\n",
        "I agree with the premise that in a world full of spurious relationships, causal reasoning is our only remedy. But I am very sceptical of claims that AI will magically provide that remedy. This leads me to the title and topic of this post: **spurious sentience** - patterns exhibited by artificial intelligence that may hint at sentience but are really just reflections of the data used to train them. The article is written in response to a recent paper and claims by one of the authors, Max Tegmark, that revealed structure in the latent embeddings of Llama 2 should finally have us believe that LLMs are more than just parrots. Since this is an opinionated post, I feel that I should start with a few disclaimers:\n",
        "\n",
        "1. I take no issue with the methodological ideas that form the foundation of the article in question: on the contrary, I think that mechanistic interpretability is an interesting and important toolkit that can help us better understand the intrinsics and behaviour of opaque artificial intelligences.\n",
        "2. The visualizations are intriguing, the code is open-sourced and the findings are interesting. \n",
        "3. I am surprised that people are surprised by the findings: if we agree that LLMs exhibit strong capabilities that can only be connected to the patterns observed in the data they were trained with, then where exactly do you expect this information to be stored if not in the parameters of the model?^[I would be very surprised---concerned even---if our search for patterns in latent spaces of capable LLMs revealed nothing at all.]\n",
        "4. I therefore do take issue with the way that these findings are being overblown by people with clout. Perhaps the parrot metaphor should not be taken too literally either, but if anything the paper's findings seem to support the notion that LLMs are remarkably capable of memorizing explicit and implicit knowledge contained in text. \n",
        "\n",
        "## Patterns in Latent Spaces and How to Find Them\n",
        "\n",
        "To demonstrate the claim that observing patterns in latent spaces should not generally surprise us, we will now go through a couple of simple examples. \n",
        "\n",
        "### Example: Principal Component Analysis\n",
        "\n",
        "In response to the claims made by Tegmark, numerous commentators on social media have pointed out that even the simplest of models can exhibit structure in their latent spaces. One of the most popular and illustrative examples I remember from my time at the Bank of England is yield curve decomposition through PCA. The yield curve is a popular tool for investors and economists to gauge the health of the economy. It plots the yields of bonds against their maturities. The slope of the yield curve is often used as a predictor of future economic activity: a steep yield curve is associated with a growing economy, while a flat or inverted yield curve is associated with a contracting economy. \n",
        "\n",
        "To understand this better, let us go on a quick detour into economics and look at actual yield curves observed in the US during the Global Financial Crisis (GFC). @fig-gfc-1 shows the yield curve of US Treasury bonds on 27 February 2007, which according to [CNN](https://money.cnn.com/2007/02/27/markets/markets_0630/index.htm?cnn=yes) was a \"brutal day on Wall Street\".^[The [data](https://home.treasury.gov/resource-center/data-chart-center/interest-rates/TextView?type=daily_treasury_yield_curve&field_tdr_date_value_month=202310) is taken from the US Department of the Treasury.] This followed [reports](https://money.cnn.com/2007/02/26/news/economy/greenspan/index.htm?postversion=2007022609) on the previous day of former Federal Reserve Chairman Alan Greenspan's warning that the US economy was at risk of a recession. The yield curve was inverted with a sharp negative spread between the 10-year and 3-month yields, indicative of the market's expectation of a recession.\n",
        "\n",
        "@fig-gfc-2 shows the corresponding yield curve during the aftermath of the GFC on 20 April 2009. On that day the influential Time Magazine [reported](https://content.time.com/time/business/article/0,8599,1890560,00.html) that the \"Banking Crisis is Over\". The yield curve was steeply sloped with a positive spread between the 10-year and 3-month yields, indicative of the market's expectation of a recovery. The overall level of the yield curve was still very low though, indicative of the fact that US economy had not fully recovered at that point.\n"
      ],
      "id": "b6912555"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Yield curve of US Treasury bonds.\n",
        "#| fig-subcap:\n",
        "#|   - 'Onset of GFC: 27 February 2007.'\n",
        "#|   - 'Aftermath of GFC: 20 April 2009.'\n",
        "#| label: fig-gfc\n",
        "#| output: true\n",
        "#| layout-ncol: 2\n",
        "\n",
        "df = CSV.read(joinpath(BLOG_DIR, \"data/ust_yields.csv\"), DataFrame) |>\n",
        "    x -> @pivot_longer(x, -Date) |>\n",
        "    x -> @mutate(x, variable=to_year(variable)) |>\n",
        "    x -> @mutate(x, year=Dates.year(Date)) |>\n",
        "    x -> @mutate(x, quarter=Dates.quarter(Date)) |>\n",
        "    x -> @mutate(x, Date=Dates.format(Date, \"yyyy-mm-dd\")) |>\n",
        "    x -> @arrange(x, Date) |>\n",
        "    x -> @fill_missing(x, \"down\")\n",
        "ylims = extrema(skipmissing(df.value))\n",
        "\n",
        "# Peak-crisis:\n",
        "onset_date = \"2007-02-27\"\n",
        "plt_df = df[df.Date .== onset_date, :]\n",
        "plt = plot(\n",
        "    plt_df.variable, plt_df.value;\n",
        "    label=\"\", color=:blue,\n",
        "    xlabel=\"Maturity (years)\", ylabel=\"Yield (%)\",\n",
        ")\n",
        "scatter!(\n",
        "    plt_df.variable, plt_df.value;\n",
        "    label=\"\", color=:blue, alpha=0.5,\n",
        "    ylims=(0,6)\n",
        ")\n",
        "display(plt)\n",
        "\n",
        "# Post-crisis:\n",
        "aftermath_date = \"2009-04-20\"\n",
        "plt_df = df[df.Date .== aftermath_date, :]\n",
        "plt = plot(\n",
        "    plt_df.variable, plt_df.value;\n",
        "    label=\"\", color=:blue,\n",
        "    xlabel=\"Maturity (years)\", ylabel=\"Yield (%)\",\n",
        ")\n",
        "scatter!(\n",
        "    plt_df.variable, plt_df.value;\n",
        "    label=\"\", color=:blue, alpha=0.5,\n",
        "    ylims=(0,6)\n",
        ")\n",
        "display(plt)"
      ],
      "id": "fig-gfc",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Of course, US Treasuries are not the only bonds that are traded in the market. To get a more complete picture of the economy, analysts might therefore be interested in looking at the yield curves of other bonds as well. In particular, we might be interested in predicting economic growth based on the yield curves of many different bonds. The problem with that idea is that it is cursed by high dimensionality: we would end up modelling a single variable of interest (economic growth) with a large number of predictors (the yields of many different bonds). To deal with the curse of high dimensionality it can be useful to decompose the yield curves into sets of principal components. \n",
        "\n",
        "To compute the principal components we can decompose the matrix of yields $\\mathbf{Y}$ into a product of its singular vectors and values: $\\mathbf{Y}=\\mathbf{U}\\Sigma\\mathbf{V}^{\\prime}$. I will not go into the details here, because Professor Gilbert Strang has already done a much better job than I ever could in his [Linear Algebra lectures](https://www.youtube.com/watch?v=mBcLRGuAFUk). To put this into the broader context of the article, however, let us simply refer to $\\mathbf{U}$, $\\Sigma$ and $\\mathbf{V}^{\\prime}$ as latent embeddings of the yield curve (they are latent because they are not directly observable).\n",
        "\n",
        "@fig-pca-1 shows the first two principal components of the yield curves of US Treasury bonds over time. Vertical stalks indicate the key dates during the onset and aftermath of the crisis, which we discussed above. For both components, we can observe some marked shifts between the two dates - but can we attribute any meaning to these shifts? It turns out we can: for comparison, @fig-pca-2 shows the average level and spread of the yield curves over time. The first principal component is strongly correlated with the level of the yield curve, while the second principal component is strongly correlated with the spread of the yield curve. To put it in AI-lingo:\n",
        "\n",
        "> The estimated latent embeddings of the yield curve are characterized by patterns observed in the data. \n"
      ],
      "id": "4a28d4da"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| fig-cap: Comparison of observed data and latent embeddings of the US Treasury yield curve.\n",
        "#| fig-subcap:\n",
        "#|   - Principal components of the yield curve.\n",
        "#|   - Average level and spread of the yield curve.\n",
        "#| label: fig-pca\n",
        "#| output: true\n",
        "\n",
        "# PCA:\n",
        "df_wide = @select(df, Date, variable, value) |>\n",
        "    x -> @pivot_wider(x, names_from = variable, values_from = value) |>\n",
        "    x -> dropmissing(x)\n",
        "X = @select(df_wide, -Date) |> Matrix\n",
        "U, Î£, V = svd(X)\n",
        "dates = Date.(df_wide.Date)\n",
        "tick_years = Date.(unique(Dates.year.(dates)))\n",
        "date_tick = Dates.format.(tick_years, \"yyyy\")\n",
        "n_pc = 2\n",
        "plt_pc = plot(\n",
        "    dates,\n",
        "    .-U[:,1:n_pc],\n",
        "    label=[\"PC $i\" for i in 1:n_pc] |> permutedims,\n",
        "    size=(1000, 300),\n",
        "    ylims=(-0.015,0.03),\n",
        "    legend=:topright\n",
        ")\n",
        "plot!(xticks=(tick_years,date_tick), xtickfontsize=6, yaxis=(formatter=y->@sprintf(\"%.2f\",y)))\n",
        "vline!(Date.([onset_date]), ls=:solid, color=:black, label=\"Onset of GFC\")\n",
        "vline!(Date.([aftermath_date]), ls=:dash, color=:black, label=\"Aftermath of GFC\")\n",
        "\n",
        "# Level:\n",
        "df_level = @group_by(df, Date) |>\n",
        "    x -> @mutate(x, level=sum(value)/length(value)) |>\n",
        "    x -> @ungroup(x) |>\n",
        "    x -> @select(x, Date, level)\n",
        "\n",
        "# Spreads:\n",
        "df_spread = @filter(df, variable==0.25 || variable==10) |>\n",
        "    x -> @select(x, -(year:quarter)) |>\n",
        "    x -> @mutate(x, variable=ifelse(variable==0.25,\"short\",\"long\")) |>\n",
        "    x -> @pivot_wider(x, names_from=variable, values_from=value) |>\n",
        "    x -> @mutate(x, spread=long-short) |>\n",
        "    x -> @select(x, Date, spread)\n",
        "\n",
        "# Plot:\n",
        "plt_mat = @full_join(df_level, df_spread) |> \n",
        "    dropmissing |> \n",
        "    unique |>\n",
        "    x -> @select(x, -Date) |>\n",
        "    Matrix\n",
        "plt_obs = plot(\n",
        "    dates,\n",
        "    plt_mat,\n",
        "    label=[\"Level\" \"Spread\"],\n",
        "    size=(1000, 300),\n",
        "    ylims=(-3,9),\n",
        "    legend=:topright,\n",
        "    ylab=\"Yield (%)\"\n",
        ")\n",
        "plot!(xticks=(tick_years,date_tick), xtickfontsize=6, yaxis=(formatter=y->@sprintf(\"%.2f\",y)))\n",
        "vline!(Date.([onset_date]), ls=:solid, color=:black, label=\"Onset of GFC\")\n",
        "vline!(Date.([aftermath_date]), ls=:dash, color=:black, label=\"Aftermath of GFC\")\n",
        "\n",
        "plot(plt_pc, plt_obs, layout=(2,1), size=(1000, 400), left_margin=5mm, bottom_margin=5mm)"
      ],
      "id": "fig-pca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Not convinced? Let us use $\\mathbf{Y}=\\mathbf{U}\\Sigma\\mathbf{V}^{\\prime}$ in true autoencoder fashion to reconstruct yield curves from principal components. Let $z_1$ denote the first principal component and consider the following: we keep all other $M-1$ principal components fixed at zero where $M$ denotes the total number of maturities; next we traverse the latent space by varying the value of $z_1$ over a fixed grid of length $K$ each time storing the full vector $\\mathbf{z}$; finally, we vertically concatenate the vectors and end up with a matrix $\\mathbf{Z}$ of dimension $(K \\times M)$. To reconstruct yields, we simply multiply $Z$ by the singular values and right singular vectors: $\\mathbf{Y}=\\mathbf{Z}\\Sigma\\mathbf{V}^{\\prime}$. \n",
        "\n",
        "@fig-pca-anim shows the result of this exercise in the left panel. As we can see, our generated yield curves shift vertically as we traverse the latent space. The right panel of @fig-pca-anim shows the result of a similar exercise, but this time we keep the first principal component fixed at zero and vary the second principal component. This time the slope of our generated yield curves shifts as we traverse the latent space. \n"
      ],
      "id": "e4ecc392"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "n_vals = 50\n",
        "pc1_range = range(extrema(U[:,1])..., length=n_vals)\n",
        "pc2_range = range(extrema(U[:,2])..., length=n_vals)\n",
        "Z_1 = [[pc1, 0, zeros(size(U, 2)-2)...] for pc1 in pc1_range] |> x -> reduce(vcat, x')\n",
        "Y_1 = Z_1 * diagm(Î£) * V'\n",
        "Z_2 = [[0, pc2, zeros(size(U, 2)-2)...] for pc2 in pc2_range] |> x -> reduce(vcat, x')\n",
        "Y_2 = Z_2 * diagm(Î£) * V'\n",
        "anim = @animate for i in 1:n_vals\n",
        "    # Level shifts:\n",
        "    y = Y_1[i,:]\n",
        "    p1 = plot(\n",
        "        unique(df.variable), y;\n",
        "        label=\"\", color=:blue,\n",
        "        xlabel=\"Maturity (years)\", ylabel=\"Yield (%)\",\n",
        "        title=\"PC1: $(round(collect(pc1_range)[i], digits=5))\",\n",
        "        ylims=extrema(Y_1)\n",
        "    )\n",
        "    scatter!(\n",
        "        unique(df.variable), y;\n",
        "        label=\"\", color=:blue, alpha=0.5,\n",
        "    )\n",
        "    # Spread shifts:\n",
        "    y = Y_2[i,:]\n",
        "    p2 = plot(\n",
        "        unique(df.variable), y;\n",
        "        label=\"\", color=:blue,\n",
        "        xlabel=\"Maturity (years)\", ylabel=\"Yield (%)\",\n",
        "        title=\"PC2: $(round(collect(pc2_range)[i], digits=5))\",\n",
        "        ylims=extrema(Y_2)\n",
        "    )\n",
        "    scatter!(\n",
        "        unique(df.variable), y;\n",
        "        label=\"\", color=:blue, alpha=0.5,\n",
        "    )\n",
        "    plot(p1, p2, layout=(1,2), size=(1000, 400), left_margin=5mm, bottom_margin=5mm)\n",
        "end\n",
        "gif(anim, joinpath(BLOG_DIR, \"www/pc_anim.gif\"), fps=5)"
      ],
      "id": "9ae2f8a0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "![Yield curve decomposition through PCA.](www/pc_anim.gif){#fig-pca-anim}\n",
        "\n",
        "### Example: Deep Learning\n",
        "\n",
        "So far we have considered simple matrix decomposition. You might argue that principal components are not really latent embeddings in the traditional sense of deep learning. To address this, let us now consider a simple deep-learning example. \n",
        "\n",
        "[Data](https://fred.stlouisfed.org/series/GDPC1).\n"
      ],
      "id": "cf9739e8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_gdp = CSV.read(joinpath(BLOG_DIR, \"data/gdp.csv\"), DataFrame) |>\n",
        "    x -> @rename(x, Date=DATE, gdp=GDPC1) |>\n",
        "    x -> @mutate(x, gdp_l1=lag(gdp)) |>\n",
        "    x -> @mutate(x, growth=log(gdp)-log(gdp_l1)) |>\n",
        "    x -> @select(x, Date, growth) |>\n",
        "    x -> @mutate(x, year=Dates.year(Date)) |>\n",
        "    x -> @mutate(x, quarter=Dates.quarter(Date)) |>\n",
        "    x -> @filter(x, year <= 2018)\n",
        "\n",
        "df_yields_qtr = @group_by(df, year, quarter, variable) |>\n",
        "    x -> @mutate(x, value=mean(value)) |>\n",
        "    x -> @ungroup(x) |>\n",
        "    x -> @select(x, -Date) |>\n",
        "    unique\n",
        "\n",
        "df_all = @inner_join(df_gdp, df_yields_qtr, (year, quarter)) |> \n",
        "    x -> @pivot_wider(x, names_from=variable, values_from=value) |>\n",
        "    dropmissing\n",
        "\n",
        "y = df_all.growth |> \n",
        "    x -> Float32.(x)\n",
        "X = @select(df_all, -(Date:quarter)) |> \n",
        "    Matrix |>\n",
        "    x -> Float32.(x)\n",
        "\n",
        "# Plot:\n",
        "p_gdp = plot(\n",
        "    df_all.Date, y;\n",
        "    label=\"\", color=:blue,\n",
        "    size=(800,200),\n",
        "    ylabel=\"GDP Growth (%)\"\n",
        ")\n",
        "p_yields = plot(\n",
        "    df_all.Date, X;\n",
        "    label=\"\", color=:blue,\n",
        "    ylabel=\"Yield (%)\",\n",
        "    legend=:bottomright,\n",
        "    alpha=0.5,\n",
        "    size=(800,400)\n",
        ")\n",
        "plot(p_gdp, p_yields, layout=(2,1), size=(800, 600), left_margin=5mm)"
      ],
      "id": "f17f9d53",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dl = Flux.MLUtils.DataLoader((X', y), batchsize=1, shuffle=true) |> gpu\n",
        "input_dim = size(X,2)\n",
        "n_pc = input_dim\n",
        "n_hidden = 24\n",
        "encoder = Flux.Chain(\n",
        "    Dense(input_dim => n_hidden, tanh),\n",
        "    Dense(n_hidden => n_pc, tanh),\n",
        ") |> gpu\n",
        "decoder = Flux.Chain(\n",
        "    Dense(n_pc => n_hidden, tanh),\n",
        "    Dense(n_hidden => input_dim, tanh),\n",
        ") |> gpu\n",
        "model = Flux.Chain(\n",
        "    encoder,\n",
        "    decoder,\n",
        "    Dense(input_dim, 1),\n",
        ") |> gpu\n",
        "loss(yhat, y) = Flux.mse(yhat, y)\n",
        "opt = Adam()\n",
        "opt_state = Flux.setup(opt, model)\n",
        "for epoch in 1:1000\n",
        "    Flux.train!(model, dl, opt_state) do m, x, y\n",
        "        loss(m(x), y)\n",
        "    end\n",
        "end\n",
        "yhat = model(X')' |> cpu\n",
        "plt = plot(yhat, label=\"Predicted\", color=:red)\n",
        "plot!(y, label=\"Actual\", color=:blue)\n",
        "display(plt)\n",
        "\n",
        "# plot(backbone(X')', label=\"Latent Embeddings\")"
      ],
      "id": "d28a6d22",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Example: Variational Autoencoder\n",
        "\n",
        "\n",
        "\n",
        "## ðŸŽ“ References"
      ],
      "id": "5121ffb0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "julia-1.9",
      "language": "julia",
      "display_name": "Julia 1.9.3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}