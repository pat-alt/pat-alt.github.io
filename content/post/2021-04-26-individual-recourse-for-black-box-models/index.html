---
title: "Individual recourse for Black Box Models"
subtitle: "Explained through a tale of üê±s and üê∂s"
author: "Patrick Altmeyer"
date: "2021-04-27"
output:
  blogdown::html_page:
      toc: false
bibliography: ../../../bib.bib
link-citations: true
categories: [Data Science, Trustworth AI]
tags: [gradient descent, classification, individual recourse]
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<blockquote>
<p>‚ÄúYou cannot appeal to [algorithms]. They do not listen. Nor do they bend.‚Äù</p>
<p>‚Äî Cathy O‚ÄôNeil</p>
</blockquote>
<p><img src ="www/toy.gif" style="float:right; margin-left: auto; margin-right: auto; width:200px; height:200px;"></p>
<p>In her popular book <a href="https://en.wikipedia.org/wiki/Weapons_of_Math_Destruction">Weapons of Math Destruction</a> Cathy O‚ÄôNeil presents the example of public school teacher Sarah Wysocki, who lost her job after a teacher evaluation algorithm had rendered her redundant <span class="citation">(<a href="#ref-o2016weapons" role="doc-biblioref">O‚Äôneil 2016</a>)</span>. Sarah was highly popular among her peers, supervisors and students.</p>
<p>This post looks at a novel algorithmic solution to the problem that individuals like Sarah, who are faced with an undesirable outcome, should be provided with means to revise that outcome. The literature commonly refers to this as <em>individual recourse</em>. One of the first approaches towards individual recourse was proposed by <span class="citation"><a href="#ref-ustun2019actionable" role="doc-biblioref">Ustun, Spangher, and Liu</a> (<a href="#ref-ustun2019actionable" role="doc-biblioref">2019</a>)</span>. In a recent follow-up paper, <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> propose a methodology coined <code>REVISE</code>, which extends the earlier approach in at least three key ways:</p>
<ol style="list-style-type: decimal">
<li><code>REVISE</code> provides a framework that avoids suggesting an unrealistic set of changes by imposing a threshold likelihood on the revised attributes.</li>
<li>It is applicable to a broader class of models including Black Box classifiers and structural causal models.</li>
<li>It can be used to detect poorly defined proxies and biases.</li>
</ol>
<p>For a detailed discussion of these points you may check out this <a href="paper_presentation.pdf">slide deck</a> or consult the paper directly (freely available on <a href="https://deepai.org/publication/towards-realistic-individual-recourse-and-actionable-explanations-in-black-box-decision-making-systems">DeepAI</a>). Here, we will abstract from some of these complications and instead look at an application of a slightly simplified version of <code>REVISE</code>. This should help us to first build a good intuition. Readers interested in the technicalities and code may find all of this in the annex below.</p>
<div id="from-to" class="section level2">
<h2>From üê± to üê∂</h2>
<p>We will explain <code>REVISE</code> through a short tale of cats and dogs. The protagonist of this tale is Kitty üê±, a young cat that identifies as a dog. Unfortunately, Kitty is not very tall and her tail, though short for a cat, is longer than that of the average dog (Figure <a href="#fig:density">1</a>).</p>
<div class="figure" style="text-align: center"><span id="fig:density"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/density-1.png" alt="Empirical distributions of simulated data set describing cats and dogs. Vertical stalks represent Kitty's attribute values." width="864" />
<p class="caption">
Figure 1: Empirical distributions of simulated data set describing cats and dogs. Vertical stalks represent Kitty‚Äôs attribute values.
</p>
</div>
<p>Much to her dismay, Kitty has been recognized as a cat by a linear classifier <span class="math inline">\(g_n(X)\)</span> that we trained through stochastic gradient descent using the data on animals‚Äô height and tail length. Once again interested readers may find technical details and code in the annex below. Figure <a href="#fig:class">2</a> shows the resulting linear separation in the attribute space with the decision boundary in solid black and Kitty‚Äôs location indicated by a red circle. Can we provide individual recourse to Kitty?</p>
<div class="figure" style="text-align: center"><span id="fig:class"></span>
<img src="{{< blogdown/postref >}}index_files/figure-html/class-1.png" alt="Linear separation of cats and dogs in the 2-dimensional attribute space with the decision boundary of the fitted classifier in solid black. Kitty's location is indicated by a red circle." width="672" />
<p class="caption">
Figure 2: Linear separation of cats and dogs in the 2-dimensional attribute space with the decision boundary of the fitted classifier in solid black. Kitty‚Äôs location is indicated by a red circle.
</p>
</div>
<p>Let‚Äôs see if and how we can apply <code>REVISE</code> to Kitty‚Äôs problem. The following summary should give you some flavour of how the algorithm works:</p>
<ol style="list-style-type: decimal">
<li>Initialize <span class="math inline">\(\mathbf{x}_i&#39;^{(0)}\)</span>, that is the attributes that will be revised recursively. Kitty‚Äôs original attributes seem like a reasonable place to start.</li>
<li>Through gradient descent recursively revise <span class="math inline">\(\mathbf{x}_i&#39;^{(t)}\)</span> until <span class="math inline">\(g_n(\mathbf{x}_i&#39;^{(T)})=\)</span>üê∂. At this point <span class="math inline">\(T\)</span> the descent terminates since for these revised attributes the classifier labels Kitty as a dog.</li>
<li>Return <span class="math inline">\(\delta_i=\mathbf{x}_i&#39;^{(T)}-\mathbf{x}_i\)</span>, that is the individual recourse for Kitty.</li>
</ol>
<p>Figure <a href="#fig:revise">3</a> illustrates what happens when this approach is applied to Kitty‚Äôs problem. The different panels show the results for different values of a regularization parameter <span class="math inline">\(\lambda\)</span> that governs the trade-off between achieving the desired label switch and keeping the distance between the original (<span class="math inline">\(\mathbf{x}_i\)</span>) and revised (<span class="math inline">\(\mathbf{x}_i&#39;\)</span>) attributes small. In all but one case, <code>REVISE</code> converges: a decrease in tail length along with an increase in height eventually allows Kitty to cross the decision boundary. In other words, we have successfully turned Kitty into a dog - at least in the eyes of the linear classifier!</p>
<p>We also observe that as we increase <span class="math inline">\(\lambda\)</span> for a fixed learning rate, <code>REVISE</code> takes longer to converge. This should come as no surprise, since higher values of <span class="math inline">\(\lambda\)</span> lead to greater regularization with respect to the penalty we place on the distance that Kitty has to travel. When we penalize too much (<span class="math inline">\(\lambda=10\)</span>), Kitty never reaches the decision boundary, because she is reluctant to change her characteristics beyond a certain point. While not visible to the naked eye, in this particular example <span class="math inline">\(\lambda=0.001\)</span> corresponds to the best choice among the candidate values.</p>
<div class="figure" style="text-align: center"><span id="fig:revise"></span>
<img src="www/revise.gif" alt="The simplified `REVISE` algorithm in action: how Kitty crosses the decision boundary by changing her attributes. Regularization with respect to the distance penalty increases from top left to bottom right."  />
<p class="caption">
Figure 3: The simplified <code>REVISE</code> algorithm in action: how Kitty crosses the decision boundary by changing her attributes. Regularization with respect to the distance penalty increases from top left to bottom right.
</p>
</div>
</div>
<div id="discussion" class="section level2">
<h2>Discussion</h2>
<p>While hopefully Kitty‚Äôs journey has provided you with some useful intuition, the story is of course very silly. Even if your cat ever seems to signal that she wants to be a dog, helping her cross that decision boundary will be tricky. Some attributes are simply immutable or very difficult to change, which <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> do not fail to account for in their framework. Their proposed methodology offers a simple and ingenious approach towards providing individual recourse. Instead of concerning ourselves with Black Box interpretability, why not simply provide remedy in case things go wrong?</p>
<p>To some extent that idea has its merit. As this post has hopefully shown, <code>REVISE</code> is straight-forward to understand and readily applicable. It could be a very useful tool to provide individual recourse in many real-world applications. As the implementation of our simplified version of <code>REVISE</code> demonstrates, researchers should also find it relatively easy to develop the methodology further and tailor it to specific use cases. The simpler version here, for example, may be useful in settings where the dimensionality is relatively small and one can reasonably model the distribution of attributes without the need for generative models.</p>
<p>Still, you may be wondering: if the original classifier is based on poorly defined rules and proxies, then what good does <code>REVISE</code> really do? Going back to the example of high-school teacher Sarah Wysocki, one of the key attributes determining teachers‚Äô evaluations was their students‚Äô performance. Realizing this, some teachers took the shortest route to success by artificially inflating their students‚Äô test scores. That same course of action may well have been suggested by <code>REVISE</code>. As <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span> demonstrate, this very property of <code>REVISE</code> may actually proof useful in detecting weaknesses of decision making systems before setting them loose (key contribution 3).</p>
<p>Nonetheless, the example above also demonstrates that approaches like <code>REVISE</code>, useful as they may be, tend to provide solutions for very particular problems. In reality data-driven decision making systems are often subject to many different problems and hence research on trustworthy AI will need to tackle the issue from various angles. A few places to start include the question of dealing with data that is inherently biased, improving ad-hoc and post-hoc model interpretability and continuing efforts around causality-inspired AI.</p>
</div>
<div id="references" class="section level2">
<h2>References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-joshi2019towards" class="csl-entry">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù</span> <em>arXiv Preprint arXiv:1907.09615</em>.
</div>
<div id="ref-o2016weapons" class="csl-entry">
O‚Äôneil, Cathy. 2016. <em>Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy</em>. Crown.
</div>
<div id="ref-ustun2019actionable" class="csl-entry">
Ustun, Berk, Alexander Spangher, and Yang Liu. 2019. <span>‚ÄúActionable Recourse in Linear Classification.‚Äù</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 10‚Äì19.
</div>
</div>
</div>
<div id="annex" class="section level2">
<h2>Annex</h2>
<p>In my blog posts I aim to implement interesting ideas from scratch even if that sometimes means that things need to undergo some sort of simplification. The benefit of this approach is that the experience is educationally rewarding - both for myself and hopefully also for readers. The first two sections of this annex show how <code>REVISE</code> and linear classification can be implemented in R. The final section just shows how the synthetic data was generated. To also inspect the code that generates the visualizations and everything else, you can find the source code of this file on <a href="https://github.com/pat-alt/patalt/blob/master/content/post/2021-04-26-individual-recourse-for-black-box-models/index.Rmd">GitHub</a>.</p>
<div id="linear-classifier" class="section level3">
<h3>Linear classifier</h3>
<p>Linear classification is implemented through stochastic gradient descent (SGD) with Hinge loss</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \ell(-\mathbf{w}^T\mathbf{x}_i y_i)&amp;=(1-\mathbf{w}^T\mathbf{x}_i y_i)_+ \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\mathbf{w}\)</span> is a coefficient vector, <span class="math inline">\(\mathbf{x}_i\)</span> is the attribute vector of individual <span class="math inline">\(i\)</span> and <span class="math inline">\(y_i\)</span> is the individual‚Äôs outcome. Since we apply SGD in order to minimize the loss function <span class="math inline">\(\ell\)</span> by varying <span class="math inline">\(\mathbf{w}\)</span>, we need an expression for its gradient with respect to <span class="math inline">\(\mathbf{w}\)</span>, which is given by:</p>
<p><span class="math display" id="eq:hinge">\[\begin{equation} 
\begin{aligned}
&amp;&amp; \nabla_{\mathbf{W}} \left( \ell(-\mathbf{w}^T\mathbf{x}_i y_i) \right) &amp;= \begin{cases} -\mathbf{x}_i y_i &amp; \text{if} \ \ \ \mathbf{w}^T\mathbf{x}_i y_i \le 1\\ 0 &amp; \text{otherwise} \end{cases} \\
\end{aligned}
\tag{1}
\end{equation}\]</span></p>
<p>The code below uses this analytical solution to perform SGD over <span class="math inline">\(T\)</span> iterations or as long as updates yield feasible parameter values. As the final vector of coefficients the function returns <span class="math inline">\(\mathbf{\bar{w}}= \frac{1}{T} \sum_{t=1}^{T} \mathbf{w}_t\)</span>. Denoting the optimal coefficient vector as <span class="math inline">\(\mathbf{w}^*\)</span>, it can be shown that under certain conditions <span class="math inline">\(\ell(\mathbf{\bar{w}})\rightarrow\ell(\mathbf{w}^*)\)</span> as <span class="math inline">\(T\rightarrow\infty\)</span>.</p>
<pre class="r"><code>#&#39; Stochastic gradient descent
#&#39;
#&#39; @param X Feature matrix.
#&#39; @param y Vector containing training labels.
#&#39; @param eta Learning rate.
#&#39; @param n_iter Maximum number of iterations.
#&#39; @param w_init Initial parameter values.
#&#39; @param save_steps Boolean checking if coefficients should be saved at each step.
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @author Patrick Altmeyer
linear_classifier &lt;- function(X,y,eta=0.001,n_iter=1000,w_init=NULL,save_steps=FALSE) {
  # Initialization: ----
  n &lt;- nrow(X) # number of observations
  d &lt;- ncol(X) # number of dimensions
  if (is.null(w_init)) {
    w &lt;- matrix(rep(0,d)) # initialize coefficients as zero...
  } else {
    w &lt;- matrix(w_init) # ...unless initial values have been provided.
  }
  w_avg &lt;- 1/n_iter * w # initialize average coefficients
  iter &lt;- 1 # iteration count
  if (save_steps) {
    steps &lt;- data.table(iter=0, w=c(w), d=1:d) # if desired, save coefficient at each step
  } else {
    steps &lt;- NA
  }
  feasible_w &lt;- TRUE # to check if coefficients are finite, non-nan, ...
  # Surrogate loss:
  l &lt;- function(X,y,w) {
    x &lt;- (-1) * crossprod(X,w) * y
    pmax(0,1 + x) # Hinge loss
  }
  grad &lt;- function(X,y,w) {
    X %*% ifelse(crossprod(X,w) * y&lt;=1,-y,0) # Gradient of Hinge loss
  }
  # Stochastic gradient descent: ----
  while (feasible_w &amp; iter&lt;n_iter) {
    t &lt;- sample(1:n,1) # random draw
    X_t &lt;- matrix(X[t,])
    y_t &lt;- matrix(y[t])
    v_t &lt;- grad(X_t,y_t,w) # compute estimate of gradient
    # Update:
    w &lt;- w - eta * v_t # update coefficient vector
    feasible_w &lt;- all(sapply(w, function(i) !is.na(i) &amp; is.finite(i))) # check if feasible
    if (feasible_w) {
      w_avg &lt;- w_avg + 1/n_iter * w # update average
    }
    if (save_steps) {
      steps &lt;- rbind(steps, data.table(iter=iter, w=c(w), d=1:d))
    }
    iter &lt;- iter + 1 # increase counter
  }
  # Output: ----
  output &lt;- list(
    X = X,
    y = matrix(y),
    coefficients = w_avg,
    eta = eta,
    n_iter = n_iter,
    steps = steps
  )
  class(output) &lt;- &quot;classifier&quot; # assign S3 class
  return(output)
}

# Methods: ----
print.classifier &lt;- function(classifier) {
  print(&quot;Coefficients:&quot;)
  print(classifier$coefficients)
}
print &lt;- function(classifier) {
  UseMethod(&quot;print&quot;)
}

predict.classifier &lt;- function(classifier, newdata=NULL, discrete=TRUE) {
  if (!is.null(newdata)) {
    fitted &lt;- newdata %*% classifier$coefficients # out-of-sampple prediction
  } else {
    fitted &lt;- classifier$X %*% classifier$coefficients # in-sample fit
  }
  if (discrete) {
    fitted &lt;- sign(fitted) # map to {-1,1}
  }
  return(fitted)
}
predict &lt;- function(classifier, newdata=NULL, discrete=TRUE) {
  UseMethod(&quot;predict&quot;)
}</code></pre>
</div>
<div id="revise-simplified" class="section level3">
<h3><code>REVISE</code> (simplified)</h3>
<p>As flagged above, we are looking at a slightly simplified version of the algorithm presented in <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>. In particular, the approach here does not incorporate the threshold on the likelihood nor does it account for immutable attributes.</p>
<p>Let <span class="math inline">\(y\in\{-1,1\}\)</span> be a binary outcome variable, <span class="math inline">\(X\in\mathbb{R}^d\)</span> a feature matrix containing individuals‚Äô attributes and <span class="math inline">\(g_n(X)\)</span> a corresponding data-dependent classifier. Suppose <span class="math inline">\(y_i=-1\)</span> (the negative outcome) for some individual characterized by attributes <span class="math inline">\(\mathbf{x}_i\)</span>. Then we want to find <span class="math inline">\(\mathbf{x}_i&#39;\)</span> closest to <span class="math inline">\(\mathbf{x}_i\)</span> such that the classifier assigns the positive outcome <span class="math inline">\(g(\mathbf{x}_i^{&#39;})=1\)</span>. In order to do so, we use gradient descent with Hinge loss <span class="math inline">\(\ell\)</span> to minimize the following function</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; \min_{\mathbf{x}_i^{&#39;}}&amp; \ \ell(g_n(\mathbf{x}_i^{&#39;}),1) + \lambda d(\mathbf{x}_i^{&#39;},\mathbf{x}_i) \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(d=||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||\)</span> denotes the Euclidean distance. Note that this time we take the coefficient vector defining <span class="math inline">\(g_n\)</span> as given and instead vary the attributes. In particular, we will perform gradient descent steps as follows</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; {\mathbf{x}_i^{&#39;}}^t&amp;\leftarrow {\mathbf{x}_i^{&#39;}}^{t-1} + \eta \nabla_{{\mathbf{x}_i^{&#39;}}} \left( \ell(g_n(\mathbf{x}_i^{&#39;}),1) + \lambda d(\mathbf{x}_i^{&#39;},\mathbf{x}_i)  \right)  \\
\end{aligned}
\]</span></p>
<p>where <span class="math inline">\(\eta\)</span> is the learning rate. The descent step is almost equivalent to the one described in <span class="citation"><a href="#ref-joshi2019towards" role="doc-biblioref">Joshi et al.</a> (<a href="#ref-joshi2019towards" role="doc-biblioref">2019</a>)</span>, but here we greatly simplify things by optimizing directly in the attribute space instead of a latent space. The gradient of the loss function looks very similar to <a href="#eq:hinge">(1)</a>. With respect to the Euclidean distance partial derivatives are of the following form:</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp;  \frac{\partial ||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||}{\partial {x_i&#39;}^{(d)}}  &amp;= \frac{{x_i&#39;}^{(d)}-{x_i}^{(d)}}{||\mathbf{x}_i^{&#39;}-\mathbf{x}_i||} \\
\end{aligned}
\]</span></p>
<p>The code that implements this optimization follows below.</p>
<pre class="r"><code>#&#39; REVISE algoritm - a simplified version
#&#39;
#&#39; @param classifier The fitted classifier.
#&#39; @param x_star Attributes of individual seeking individual recourse.
#&#39; @param eta Learning rate.
#&#39; @param lambda Regularization parameter.
#&#39; @param n_iter Maximum number of operations.
#&#39; @param save_steps Boolean indicating if intermediate steps should be saved.
#&#39;
#&#39; @return
#&#39; @export
#&#39;
#&#39; @author Patrick Altmeyer
revise.classifier &lt;- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {
  # Initialization: ----
  d &lt;- length(x_star) # number of dimensions
  if (!is.null(names(x_star))) {
    d_names &lt;- names(x_star) # names of attributes, if provided
  } else {
    d_names &lt;- sprintf(&quot;X%i&quot;, 1:d)
  }
  w &lt;- classifier$coefficients # coefficient vector
  x &lt;- x_star # initialization of revised attributes
  distance &lt;- 0 # initial distance from starting point
  converged &lt;- predict(classifier, newdata = x)[1,1]==1 # positive outcome?
  iter &lt;- 1 # counter
  if (save_steps) {
    steps &lt;- data.table(iter=1, x=x, d=d_names) # save intermediate steps, if desired
  } else {
    steps &lt;- NA
  }
  # Gradients:
  grad &lt;- function(x,y,w) {
    w %*% ifelse(crossprod(x,w) * y&lt;=1,-y,0) # gradient of Hinge loss with respect to X
  }
  grad_dist &lt;- function(x,x_star) {
    d &lt;- length(x_star)
    distance &lt;- dist(matrix(cbind(x_star,x),nrow=d,byrow = T))
    matrix((x-x_star) / distance) # gradient of Euclidean distance with respect to X
  }
  # Gradient descent: ----
  while(!converged &amp; iter&lt;n_iter) {
    if (distance!=0) {
      x &lt;- c(x - eta * (grad(x=matrix(x),y=1,w) + lambda * grad_dist(x,x_star))) # gradient descent step
    } else {
      x &lt;- c(x - eta * grad(x=matrix(x),y=1,w)) # gradient with respect to distance not defined at zero
    }
    converged &lt;- predict(classifier, newdata = x)[1,1]==1 # positive outcome?
    iter &lt;- iter + 1 # update counter
    if (save_steps) {
      steps &lt;- rbind(steps, data.table(iter=iter, x=x, d=d_names))
    }
    distance &lt;- dist(matrix(cbind(x_star,x),nrow=d,byrow = T)) # update distance
  }
  # Output: ----
  if (converged) {
    revise &lt;- x - x_star
  } else {
    revise &lt;- NA
  }
  output &lt;- list(
    x_star = x_star,
    revise = revise,
    classifier = classifier,
    steps = steps,
    lambda = lambda,
    distance = distance,
    mean_distance = mean(abs(revise))
  )
  return(output)
}

revise &lt;- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {
  UseMethod(&quot;revise&quot;)
}</code></pre>
</div>
<div id="simulated-data" class="section level3">
<h3>Simulated data</h3>
<p>The synthetic data describing cats and dogs was generated as follows:</p>
<pre class="r"><code>sim_data &lt;- function(n=100,averages,noise=0.1) {
  d &lt;- ncol(averages)
  y &lt;- 2*(rbinom(n,1,0.5)-0.5) # generate binary outcome: 1=dog, -1=cat
  X &lt;- as.matrix(averages[(y+1)/2+1,]) # generate attributes conditional on y
  dogs &lt;- y==1 # boolean index for dogs
  cats &lt;- y==-1 # boolean index for cats
  X[cats,] &lt;- X[cats,] + 
    matrix(rnorm(sum(cats)*d),nrow=sum(cats)) %*% diag(noise*averages[2,]) # add noise for y=1 (cats)
  X[dogs,] &lt;- X[dogs,] + 
    matrix(rnorm(sum(dogs)*d),nrow=sum(dogs)) %*% diag(noise*averages[2,]) # add noise for y=1 (dogs)
  return(list(X=X,y=y))
}</code></pre>
</div>
</div>
