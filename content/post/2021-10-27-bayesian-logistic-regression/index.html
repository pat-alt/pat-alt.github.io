---
title: Bayesian Logistic Regression
author: Patrick Altmeyer
date: '2021-10-27'
slug: bayesian-logistic-regression
categories: [Data Science]
tags: [bayes, logit]
subtitle: From scratch in Julia Language
summary: ''
authors: []
lastmod: '2021-10-27T08:57:47+02:00'
featured: no
output:
  blogdown::html_page:
      toc: false
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: ../../../bib.bib
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<div id="the-ground-truth" class="section level2">
<h2>The ground truth</h2>
<p>In this post we will work with a synthetic data set <span class="math inline">\(\mathcal{D}\)</span> composed of <span class="math inline">\(N\)</span> binary labels <span class="math inline">\(y_n\in\{0,1\}\)</span> and corresponding feature vectors <span class="math inline">\(\mathbf{x}_n\in \mathbb{R}^D\)</span>. Working with synthetic data has the benefit that we have control over the <strong>ground truth</strong> that generates our data. In particular, we will assume that the binary labels <span class="math inline">\(y_n\)</span> are generated by a logistic regression model</p>
<p><span class="math display" id="eq:logreg">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; p(y_n|\mathbf{x}_n;\mathbf{w})&amp;\sim\text{Ber}(y_n|\sigma(\mathbf{w}^T\mathbf{x}_n)) \\
\end{aligned}
\tag{1}
\end{equation}
\]</span></p>
<p>where <span class="math inline">\(\sigma(a)=1/(1+e^{-a})\)</span> is the <strong>sigmoid</strong> or <strong>logit</strong> function <span class="citation">(<a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy 2022</a>)</span>. We let <span class="math inline">\(\mathbf{w}=\begin{bmatrix} 0 \\ 0.5 \\ -1 \\ \end{bmatrix}\)</span> define the true coefficients.</p>
<p><img src="{{< blogdown/postref >}}index_files/figure-html/unnamed-chunk-4-1.png" width="480" style="display: block; margin: auto;" /></p>
</div>
<div id="the-maths" class="section level2">
<h2>The maths</h2>
<p>Using the synthetic data <span class="math inline">\(\mathcal{D}\)</span> we will now estimate the logistic regression model <a href="#eq:logreg">(1)</a> that generates them. Estimation usually boils down to finding the vector of parameters <span class="math inline">\(\hat{\mathbf{w}}\)</span> that maximizes the likelihood of observing <span class="math inline">\(\mathcal{D}\)</span> under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set <span class="math inline">\(\mathcal{D}=\{x_m:m=1,...,M\}\)</span>. We are then typically interested in how accurate these model predictions are. This sums up how many automated decision-making systems operate in practice these days: for example, a company that is interested speeding up its recruitment process could train a binary classifier on historical data to automatically assign labels (<span class="math inline">\(\hat{y}_m \in \{\text{short-listed, rejected}\}\)</span>) to new applications <span class="math inline">\(\mathbf{x}_m\)</span>. Does this seem like a sound approach to automated decision-making? Haven’t we forgot anything? While I have mentioned prediction <strong>accuracy</strong> above, there was no mentioning of assessing the classifier’s <strong>uncertainty</strong> around its predictions. The predicted labels <span class="math inline">\(y_m\)</span> are merely point estimates based on the learned model parameters, which are random variables! Focusing merely on prediction accuracy and ignoring uncertainty altogether installs a false confidence in automated decision-making systems. Any <strong>trustworthy</strong> approach to learning from data should at the very least be transparent about its own uncertainty.</p>
<p>How then, can we estimate uncertainty around model parameters and predictions? Frequentist methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example <a href="https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture26.pdf">here</a> for the case of logistic regression). In Bayesian statistics and machine learning we instead use the <strong>posterior distribution</strong> over model parameters to quantify uncertainty. This approach to uncertainty quantification is known as <strong>Bayesian Inference</strong> because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on <strong>prior</strong> knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as <em>un</em>scientific. However, frequentist methods come with their own assumption and pitfalls (see for example <span class="citation"><a href="#ref-murphy2012machine" role="doc-biblioref">Murphy</a> (<a href="#ref-murphy2012machine" role="doc-biblioref">2012</a>)</span>) for a discussion). Without diving further into this argument, let us now see how Bayesian logistic regression can be implemented.</p>
<div id="problem-setup" class="section level3">
<h3>Problem setup</h3>
<p>The starting point for Bayesian Logistic Regression is <strong>Bayes’ Theorem</strong>:</p>
<p><span class="math display" id="eq:posterior">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; p(\mathbf{w}|\mathcal{D})&amp;\propto p(\mathcal{D}|\mathbf{w})p(\mathbf{w}) \\
\end{aligned}
\tag{2}
\end{equation}
\]</span>
Formally, this says that the posterior distribution of parameters <span class="math inline">\(\mathbf{w}\)</span> is proportional to the product of the likelihood of observing <span class="math inline">\(\mathcal{D}\)</span> given <span class="math inline">\(\mathbf{w}\)</span> and the prior density of <span class="math inline">\(\mathbf{w}\)</span>. Applied to our context this can intuitively be understood as follows: our posterior beliefs around <span class="math inline">\(\mathbf{w}\)</span> are formed by both our prior beliefs and the evidence we observe. Yet another way to this is that maximising <a href="#eq:posterior">(2)</a> with respect to <span class="math inline">\(\mathbf{w}\)</span> corresponds to maximum likelihood estimation regularized by prior beliefs.</p>
<p>Under the assumption that individual label-feature pairs are <strong>independently</strong> and <strong>identically</strong> distributed, their joint likelihood is simply the product over their individual densities. The prior beliefs around <span class="math inline">\(\mathbf{w}\)</span> are at our discretion. In practice they may be derived from previous experiments. Here we will use a zero-mean spherical Gaussian prior for reasons explained further below. To sum this up we have</p>
<p><span class="math display" id="eq:prior">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; p(\mathcal{D}|\mathbf{w})&amp; \sim \prod_{n=1}^N p(y_n|\mathbf{x}_n;\mathbf{w})\\
&amp;&amp; p(\mathbf{w})&amp; \sim \mathcal{N} \left( \mathbf{w} | \mathbf{w}_0, \Sigma_0 \right) \\
\end{aligned}
\tag{3}
\end{equation}
\]</span></p>
<p>with <span class="math inline">\(\mathbf{w}_0=\mathbf{0}\)</span> and <span class="math inline">\(\Sigma_0=\sigma^2\mathbf{I}\)</span>. Plugging this into Bayes’ rule we finally have</p>
<p><span class="math display">\[
\begin{aligned}
&amp;&amp; p(\mathbf{w}|\mathcal{D})&amp;\propto\prod_{n=1}^N \text{Ber}(y_n|\sigma(\mathbf{w}^T\mathbf{x}_n))\mathcal{N} \left( \mathbf{w} | \mathbf{w}_0, \Sigma_0 \right) \\
\end{aligned}
\]</span></p>
<p>Unlike with linear regression there are no closed-form analytical solutions to estimating or maximising this posterior, but fortunately accurate approximations do exist <span class="citation">(<a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy 2022</a>)</span>. One of the simplest approaches called <strong>Laplace Approximation</strong> is straight-forward to implement and not computationally expensive. It can be shown that under the assumption of a Gaussian prior, the posterior of logistic regression is also approximately Gaussian: in particular, it approximately follows a Gaussian distribution centered around the <strong>maximum a posteriori</strong> (MAP) estimate <span class="math inline">\(\hat{\mathbf{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})\)</span> and with a covariance matrix equal to the inverse Hessian evaluated at the mode <span class="math inline">\(\hat{\Sigma}=(\mathbf{H}(\hat{\mathbf{w}}))^{-1}\)</span>. With that in mind, finding <span class="math inline">\(\hat{\mathbf{w}}\)</span> seems like a natural next step.</p>
</div>
<div id="solving-the-problem" class="section level3">
<h3>Solving the problem</h3>
<p>In practice we do not maximize the likelihood <span class="math inline">\(p(\mathbf{w}|\mathcal{D})\)</span> directly. Instead we minimize the negative log likelihood, which is equivalent and easier to implement. In <a href="#eq:likeli">(4)</a> below I have denoted the negative log likelihood as <span class="math inline">\(\ell(\mathbf{w})\)</span> indicating that this is the <strong>loss function</strong> we aim to minimize. The following two lines in <a href="#eq:likeli">(4)</a> show the gradient and Hessian - so the first- and second-order derivatives of <span class="math inline">\(\ell\)</span> with respect to <span class="math inline">\(\mathbf{w}\)</span> - where <span class="math inline">\(\mathbf{H}_0=\Sigma_0^{-1}\)</span> and <span class="math inline">\(\mu_n=\sigma(\mathbf{w}^T\mathbf{x}_n)\)</span>. To understand how exactly the gradient and Hessian are derived see for example chapter 10 in <span class="citation"><a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy</a> (<a href="#ref-murphy2022probabilistic" role="doc-biblioref">2022</a>)</span>.<a href="#fn1" class="footnote-ref" id="fnref1"><sup>1</sup></a>.</p>
<p><span class="math display" id="eq:likeli">\[
\begin{equation} 
\begin{aligned}
&amp;&amp; \ell(\mathbf{w})&amp;=- \sum_{n=1}^{N} [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0) \\
&amp;&amp; \nabla_{\mathbf{w}}\ell(\mathbf{w})&amp;= \sum_{n=1}^{N} (\mu_n-y_n) \mathbf{x}_n + \mathbf{H}_0(\mathbf{w}-\mathbf{w}_0) \\
&amp;&amp; \nabla^2_{\mathbf{w}}\ell(\mathbf{w})&amp;= \sum_{n=1}^{N} (\mu_n-y_n) \left( \mu_n(1-\mu_n) \mathbf{x}_n \mathbf{x}_n^T \right) + \mathbf{H}_0\\
\end{aligned}
\tag{4}
\end{equation}
\]</span></p>
<p>SIDENOTE</p>
<p>Note how earlier I mentioned that maximising the posterior likelihood corresponds to a regularized version of MLE…</p>
<p>Since minimizing this loss function is a convex optimization problem we have many efficient algorithms to choose from in order to solve this problem. With the Hessian at hand it seems natural to use a second-order method, because incorporating information about the curvature of the loss function generally leads to faster convergence. Here we will implement Newton’s method in line with the presentation in chapter 8 of <span class="citation"><a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy</a> (<a href="#ref-murphy2022probabilistic" role="doc-biblioref">2022</a>)</span>.</p>
</div>
<div id="posterior-predictive" class="section level3">
<h3>Posterior predictive</h3>
<p>With the posterior distribution over parameters <span class="math inline">\(p(\mathbf{w}|\mathcal{D})\)</span> at hand we have the necessary ingredients to estimate the posterior predictive distribution <span class="math inline">\(p(y=1|\mathbf{x}, \mathcal{D})\)</span>. This estimated distribution can then be used to quantify the uncertainty around our predictions. …</p>
</div>
</div>
<div id="the-code" class="section level2">
<h2>The code</h2>
<p>We now have all the necessary ingredients to code Bayesian Logistic Regression up from scratch. While in practice we would usually want to rely on existing packages that have been properly tested, I often find it very educative and rewarding to program algorithms from the bottom up. You will see that Julia’s syntax so closely resembles the mathematical formulas we have seen above, that going from maths to code is incredibly straight-forward. Seeing those formulas and algorithms then actually doing their magic is quite fun! The code chunk below, for example, shows the implementation of the loss function and its derivatives from <a href="#eq:likeli">(4)</a> above. Take a moment to go through the code line-by-line and try to understand what it does. Isn’t it amazing how closely the code resembles the actual equations?</p>
<pre class="julia"><code># Loss:
function 𝓁(w,w_0,H_0,X,y)
    N = length(y)
    D = size(X)[2]
    μ = sigmoid(w,X)
    Δw = w-w_0
    l = - ∑( y[n] * log(μ[n]) + (1-y[n]) * log(1-μ[n]) for n=1:N) + 1/2 * Δw&#39;H_0*Δw
    return l
end

# Gradient:
function ∇𝓁(w,w_0,H_0,X,y)
    N = length(y)
    μ = sigmoid(w,X)
    Δw = w-w_0
    g = ∑((μ[n]-y[n]) * X[n,:] for n=1:N)
    return g + H_0*Δw
end
    
# Hessian:
function ∇∇𝓁(w,w_0,H_0,X,y)
    N = length(y)
    μ = sigmoid(w,X)
    H = ∑(μ[n] * (1-μ[n]) * X[n,:] * X[n,:]&#39; for n=1:N)
    return H + H_0
end</code></pre>
<p>Aside from the optimization routine this is essentially all there is to coding up Bayesian Logistic Regression from scratch in Julia Language. If you are curious to see the full source code in detail you can check out this <a href="https://colab.research.google.com/github/pat-alt/pat-alt.github.io/blob/logitbayes/content/post/2021-10-27-bayesian-logistic-regression/toy_example.ipynb">interactive notebook</a>. Now let us finally turn back to our synthetic data and see how Bayesian Logistic Regression can help us understand the uncertainty around our model predictions.</p>
</div>
<div id="the-estimates" class="section level2">
<h2>The estimates</h2>
<p><img src="www/posterior.png" width="750" style="display: block; margin: auto;" /></p>
<p><img src="www/predictive.png" width="750" style="display: block; margin: auto;" /></p>
<!-- ### MCMC using `rstan` -->
<!-- Now let us see if wee can improve upon the Laplace Approximation by instead relying on Monte Carlo methods.  -->
<!-- ```{r, echo=FALSE} -->
<!-- library(rstan) -->
<!-- options(mc.cores = parallel::detectCores()) -->
<!-- rstan_options(auto_write = TRUE) -->
<!-- ``` -->
<!-- ```{r, message=FALSE, eval=FALSE} -->
<!-- sigma <- 10 # uncertainty around prior -->
<!-- n_iter <- 100 # default is 2000 -->
<!-- model_data <- list( -->
<!--   N = nrow(X), -->
<!--   K = ncol(X), -->
<!--   X = as.matrix(X), -->
<!--   y = y, -->
<!--   sigma = sigma -->
<!-- ) -->
<!-- fit <- stan( -->
<!--   file = 'logit.stan',  -->
<!--   data = model_data, -->
<!--   iter = n_iter, -->
<!--   seed = seed -->
<!-- ) -->
<!-- ``` -->
</div>
<div id="references" class="section level2">
<h2>References</h2>
<p>For a great reference regarding the theory covered in this post see <span class="citation"><a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy</a> (<a href="#ref-murphy2022probabilistic" role="doc-biblioref">2022</a>)</span> (chapters 4 and 10) and <span class="citation"><a href="#ref-bishop2006pattern" role="doc-biblioref">Bishop</a> (<a href="#ref-bishop2006pattern" role="doc-biblioref">2006</a>)</span> (chapter 8). The section on <code>rstan</code> draws heavily on a great <a href="https://www.r-bloggers.com/2020/02/bayesian-logistic-regression-with-stan/">blog post</a> and also the <a href="https://mc-stan.org/docs/2_21/stan-users-guide/index.html#overview">Stan User’s Guide</a>.</p>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bishop2006pattern" class="csl-entry">
Bishop, Christopher M. 2006. <em>Pattern Recognition and Machine Learning</em>. springer.
</div>
<div id="ref-murphy2012machine" class="csl-entry">
Murphy, Kevin P. 2012. <em>Machine Learning: A Probabilistic Perspective</em>. MIT press.
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry">
———. 2022. <em>Probabilistic Machine Learning: An Introduction</em>. MIT Press.
</div>
</div>
</div>
<div class="footnotes">
<hr />
<ol>
<li id="fn1"><p>Note that the author works with the negative log likelihood scaled by the sample size<a href="#fnref1" class="footnote-back">↩︎</a></p></li>
</ol>
</div>
