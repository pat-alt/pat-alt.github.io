---
title: Bayesian Logistic Regression
author: Patrick Altmeyer
date: '2021-10-27'
slug: bayesian-logistic-regression
categories: [Data Science]
tags: [bayes, logit]
subtitle: ''
summary: ''
authors: []
lastmod: '2021-10-27T08:57:47+02:00'
featured: no
output:
  blogdown::html_page:
      toc: false
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: ../../../bib.bib
---

```{r setup, include=FALSE}
rm(list=ls())
knitr::opts_chunk$set(
  echo = FALSE, 
  message = FALSE, 
  fig.align='center'
)
library(data.table)
library(ggplot2)
library(gganimate)
library(ggimage)
seed <- 2021
theme_set(theme_minimal())

# Get helper functions:
utils <- list.files("../utils/")
invisible(
  lapply(
    utils,
    function(i) {
      source(file = file.path("../utils",i))
    }
  )
)
```

## Ground truth

```{r}
w <- as.matrix(c(0,0.5,-1))
seed <- 2021
```

In this post we will work with a synthetic data set $\mathcal{D}$ composed of $N$ binary labels $y_n$ and feature samples $\mathbf{x}_n$. Working with synthetic data has the benefit that we have control over the **ground truth** that generates our data. In particular, we will assume that the binary labels $y_n$ are generated by a logistic regression model

$$
\begin{equation} 
\begin{aligned}
&& p(y|\mathbf{x};\mathbf{w})&=\text{Ber}(y|\sigma(\mathbf{w}^T\mathbf{x})) \\
\end{aligned}
(\#eq:logreg)
\end{equation}
$$

where $\sigma(a)=1/(1+e^a)$ is the **sigmoid** or **logit** function [@murphy2022probabilistic]. We let $\mathbf{w}=`r matrix2latex(w)`$ define the true coefficients. 

```{r}
logit <- function(w,X) {
  
  if (dim(X)[2]!=dim(w)[1]) {
    X <- cbind(1,as.matrix(X)) # add 1 for constant/bias
  } else {
    X <- as.matrix(X)
  }
  
  logit <- 1 / (1 + exp(-1 * (X %*% w)))
  
  return(logit)
}
```


```{r gauss-mix}
gauss_mix <- function(n, mu, sigma, weights) {
  u <- runif(n) # to determine which distribution to sample from
  d <- ncol(mu) # how many variables
  X <- t(
    sapply(
      u,
      function(u) {
        idx_dist <- which(cumsum(weights) > u)[1]
        X <- sapply(
          1:d, 
          function(i) {
            rnorm(1,mu[idx_dist,i],sigma[idx_dist,i])
          }
        )
        matrix(X,ncol=d)
      }
    )
  )
  return(X)
}
```


```{r mixture}
# Parameters
n <- 1000
weights <- c(0.5,0.5)
mu <- rbind(c(50,50),c(100,20))
noise <- 0.25
sigma <- noise * mu
# Generate mixture:
X <- gauss_mix(n,mu,sigma,weights)
```

```{r}
# True Bernoulli probabilities based on synthetic features and true coefficients:
p <- logit(w,X)
# True labels based on synthetic features and true coefficients:
y <- rbinom(n,1,p)
```

```{r density, fig.height=4, fig.width=9, fig.cap="Empirical distributions of simulated data set describing cats and dogs. Vertical stalks represent Kitty's attribute values."}
dt <- data.table(y,X)
fwrite(dt, "data/cats_dogs.csv")
dt <- melt(dt, id.vars = "y")
dt[,y:=as.factor(y)]
levels(dt$y) <- c("Cat", "Dog")
dt_plot <- copy(dt)
levels(dt_plot$variable) <- c("Height", "Tail")
ggplot(data=dt_plot, aes(x=value)) +
  geom_density(alpha=0.25, aes(fill=factor(y))) +
  facet_wrap(~variable,scales="free") +
  scale_fill_manual("Outcome:", values = c("orange","brown")) +
  labs(
    x=NULL,
    y="Conditional density"
  )
```


## Background

Using the synthetic data $\mathcal{D}$ we will now estimate the logistic regression model \@ref(eq:logreg) that generates them. Estimation essentially boils down to finding the vector of parameters $\hat{\mathbf{w}}$ that maximizes the likelihood of observing $\mathcal{D}$ under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set $\mathcal{D}=\{x_m:m=1,...,M\}$. We are then typically interested in how accurate these model predictions are. This sums up how many automated decision-making systems operate in practice these days: for example, a company that is interested speeding up its recruitment process could train a binary classifier on historical data to automatically assign labels ($\hat{y}_m \in \{\text{short-listed, rejected}\}$) to new applications $\mathbf{x}_m$. Does this seem like a sound approach to automated decision-making? Haven't we forgot anything? While I have mentioned prediction **accuracy** above, there was no mentioning of assessing the classifier's **uncertainty** around its predictions. The predicted labels $y_m$ are merely point estimates based on the learned model parameters, which are random variables! Focusing merely on prediction accuracy and ignoring uncertainty altogether installs a false confidence in automated decision-making systems. Any **trustworthy** approach to learning from data should at the very least be transparent about its own uncertainty. 

How then, can we estimate uncertainty aorund model parameters and predictions? Frequentist methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example [here](https://web.stanford.edu/class/archive/stats/stats200/stats200.1172/Lecture26.pdf) for the case of logistic regression). In Bayesian statistics and machine learning we instead use the **posterior distribution** over model parameters to quantify uncertainty. This approach to uncertainty quantification is known as **Bayesian Inference** because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on **prior** knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as *un*scientific. However, frequentist methods come with their own assumption and pitfalls (see for example @murphy2012machine) for a discussion). Without diving further into this argument, let us now see how Bayesian logistic regression can be implemented.

Unlike with linear regression there are no closed-form analytical solutions - not for our point estimate $\hat{\mathbf{w}}$ and much less for its posterior distribution $p(\mathbf{w}|\mathcal{D})$ [@murphy2022probabilistic]. Fortunately though, there are many approximate solutions to this problem, some of which we will treat here.

### Laplace Approximation

One of the simplest approaches called **Laplace Approximation** is straight-forward to implement and can be summarized as follows:

#### Step 1

Assume a Gaussian prior over model parameters, e.g. a spherical prior: let $\mathbf{w}_0$ denote the vector of prior means and $\Sigma=\mathbf{H}_0^{-1}=\sigma^2\mathbf{I}$ the prior covariance matrix. The $\mathbf{H}_0^{-1}$ indicates that in case of a multi-variate Gaussian distribution the covariance matrix is equivalent to the inverse Hessian. Formally, we assume:

$$
\begin{equation} 
\begin{aligned}
&& p(\mathbf{w})&\sim \mathcal{N} (\mathbf{w_0},\mathbf{H}_0^{-1}) \\
\end{aligned}
(\#eq:prior)
\end{equation}
$$

#### Step 2

Use an optimization algorithm to compute the **maximum a posterior** (MAP) estimate $\hat{\mathbf{w}}$ as well as an estimate of the posterior uncertainty. The latter is our main quantity of interest after all and in the Gaussian setting corresponds to inverse Hessian of the posterior likelihood. Since maximising the likelihood (or equivalently minimizing the negative log likelihood) in this setting is a convex optimization problem [@murphy2022probabilistic] we have many efficient algorithms to choose from. Here we will implement one of the simplest: stochastic gradient descent (SGD). To avoid overfitting, we will also add a Ridge penalty that penalizes the $\ell^2$-norm of our estimated parameters, i.e. $||\mathbf{w}||_2^2=\mathbf{w}^T\mathbf{w}$. Let $\lambda$ denote a hyperparameter that governs how strictly we regularize and let $\mu_n=\sigma(\mathbf{w}^T\mathbf{x}_n)$. Then all the important ingredients for our optimization problem can be summarized as follows:

$$
\begin{equation} 
\begin{aligned}
&& \mathcal{L}(\mathbf{w})&=- \frac{1}{N} \sum_{n=1}^{N} [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \lambda\mathbf{w}^T\mathbf{w} \\
&& \nabla_{\mathbf{w}}\mathcal{L}(\mathbf{w})&= \frac{1}{N} \sum_{n=1}^{N} (\mu_n-y_n) \mathbf{x}_n + \mathbf{H}_0^T(\mathbf{w}-\mathbf{w}_0)+2\lambda \mathbf{w} \\
&& \nabla^2_{\mathbf{w}}\mathcal{L}(\mathbf{w})&= \frac{1}{N} \sum_{n=1}^{N} \left[ \mu_n(1-\mu_n) \mathbf{x}_n \mathbf{x}_n^T \right]   + \mathbf{H}_0+2\lambda \mathbf{I} \\
\end{aligned}
(\#eq:likeli)
\end{equation}
$$

#### Step 3

With the posterior distribution over parameters $p(\mathbf{w}|\mathcal{D})$ at hand we have the necessary ingredients to estimate the posterior predictive distribution $p(y|\mathbf{x}, \mathcal{D})$. This estimated distribution can then be used to quantify the uncertainty around our predictions. ...

#### Implementation in Julia (from scratch)

```{r}
library(JuliaCall)
```


```{julia}
include("julia/bayes_logreg.jl");
using CSV
using DataFrames
df = CSV.read("data/cats_dogs.csv", DataFrame);
```

```{julia}
y = df[:,:y];
N = length(y);
X = Matrix(df[:,Not(:y)]);
X = [ones(N) X]; # add for constant
d = size(X)[2]; # number of features
σ = 10; # noise
w_0 = zeros(d);
H_0 = UniformScaling(1/(σ^2));
```

```{julia}
w_map = sgd(X,y,∇,∇∇,w_0,H_0,1.0,10000,1.0,0.001)
```

### MCMC using `rstan`

Now let us see if wee can improve upon the Laplace Approximation by instead relying on Monte Carlo methods. 

```{r, echo=FALSE}
library(rstan)
options(mc.cores = parallel::detectCores())
rstan_options(auto_write = TRUE)
```

```{r, message=FALSE, eval=FALSE}
sigma <- 10 # uncertainty around prior
n_iter <- 100 # default is 2000
model_data <- list(
  N = nrow(X),
  K = ncol(X),
  X = as.matrix(X),
  y = y,
  sigma = sigma
)
fit <- stan(
  file = 'logit.stan', 
  data = model_data,
  iter = n_iter,
  seed = seed
)
```

## References

For a great reference regarding the theory covered in this post see @murphy2022probabilistic (chapters 4 and 10) and @bishop2006pattern (chapter 8). The section on `rstan` draws heavily on a great [blog post](https://www.r-bloggers.com/2020/02/bayesian-logistic-regression-with-stan/) and also the [Stan User's Guide](https://mc-stan.org/docs/2_21/stan-users-guide/index.html#overview). 

<div id="refs"></div>





