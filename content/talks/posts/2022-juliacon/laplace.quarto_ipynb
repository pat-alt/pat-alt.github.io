{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: Effortless Bayesian Deep Learning through Laplace Redux\n",
        "subtitle: JuliaCon 2022\n",
        "author: Patrick Altmeyer\n",
        "format: \n",
        "  revealjs:\n",
        "    logo: /www/images/delft_logo.png\n",
        "    footer: |\n",
        "      Effortless Bayesian Deep Learning through Laplace Redux -- JuliaCon 2022 -- Patrick Altmeyer\n",
        "    self-contained: true\n",
        "    smaller: true\n",
        "    scrollable: true\n",
        "    preview-links: auto\n",
        "    slide-number: true\n",
        "    transition: slide\n",
        "    background-transition: fade\n",
        "    fig-align: center\n",
        "bibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib\n",
        "execute:\n",
        "  eval: false\n",
        "  echo: true\n",
        "---\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "using Pkg; Pkg.activate(\"dev\")\n",
        "using Plots, PlotThemes\n",
        "theme(:wong)\n",
        "include(\"dev/utils.jl\")\n",
        "www_path = \"dev/resources/juliacon22/www\"\n",
        "```\n",
        "\n",
        "\n",
        "## Overview\n",
        "\n",
        ":::{.incremental}\n",
        "- The Case for Bayesian Deep Learning\n",
        "- Laplace Redux in Julia 📦\n",
        "    - From Bayesian Logistic Regression ...\n",
        "    - ... to Bayesian Neural Networks.\n",
        "- Goals and Ambitions 🎯\n",
        ":::\n",
        "\n",
        "# The Case for Bayesian Deep Learning\n",
        "\n",
        "## Bayesian Model Averaging\n",
        "\n",
        "> Don't put all your 🥚 in one 🧺.\n",
        "\n",
        "\n",
        ":::{.incremental}\n",
        "- In Deep Learning we typically maximise highly non-convex functions full of local optima and saddle points.\n",
        "- There may be many $\\hat\\theta_1, ..., \\hat\\theta_m$ that are slightly different, but yield similar performance.\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "> [...] parameters correspond to a diverse variety of compelling explanations for the data. \n",
        "> [@wilson2020case]\n",
        "\n",
        ". . .\n",
        "\n",
        "$\\theta$ is a random variable. Shouldn't we treat it that way?\n",
        "\n",
        "$$\n",
        "p(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n",
        "$$ {#eq-bma}\n",
        "\n",
        "> Intractable!\n",
        "\n",
        ". . .\n",
        "\n",
        "In practice we typically rely on a **plugin** approximation [@murphy2022probabilistic].\n",
        "\n",
        "$$\n",
        "p(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta \\approx p(y|x,\\hat\\theta)\n",
        "$$ {#eq-plugin}\n",
        "\n",
        "> Yes, \"plugin\" is literal ... can we do better?\n",
        "\n",
        "## Enter: Bayesian Deep Learning 🔮\n",
        "\n",
        "> Yes, we can! \n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "\n",
        "#### Popular approaches include ...\n",
        "\n",
        ":::{.fragment .semi-fade-out fragment-index=4}\n",
        "\n",
        ":::{.fragment .fade-left fragment-index=0}\n",
        "MCMC (see [`Turing`](https://turing.ml/dev/tutorials/03-bayesian-neural-network/))\n",
        ":::\n",
        "\n",
        ":::{.fragment .fade-left fragment-index=1}\n",
        "Variational Inference [@blundell2015weight]\n",
        ":::\n",
        "\n",
        ":::{.fragment .fade-left fragment-index=2}\n",
        "Monte Carlo Dropout [@gal2016dropout]\n",
        ":::\n",
        "\n",
        ":::{.fragment .fade-left fragment-index=3}\n",
        "Deep Ensembles [@lakshminarayanan2016simple]\n",
        ":::\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.fragment .fade-left fragment-index=5}\n",
        "Laplace Redux (@immer2020improving,@daxberger2021laplace)\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "![Pierre-Simon Laplace as chancellor of the Senate under the First French Empire. Source: [Wikipedia](https://en.wikipedia.org/wiki/Pierre-Simon_Laplace)](/www/images/laplace_dude.jpeg){#fig-laplace-dude width=\"25%\"}\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "![Simulation of changing posteriour predictive distribution. Image by author.](/www/images/intro.gif){#fig-intro}\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Laplace Approximation\n",
        "\n",
        "> We first need to estimate the weight posterior $p(\\theta|\\mathcal{D})$ ...\n",
        "\n",
        ". . .\n",
        "\n",
        "**Idea** 💡: Taylor approximation at the mode.\n",
        "\n",
        ":::{.incremental}\n",
        "- Going through the maths we find that this yields a Gaussian posteriour centered around the MAP estimate $\\hat\\theta$ (see pp. 148/149 in @murphy2022probabilistic).\n",
        "- Covariance corresponds to inverse Hessian at the mode (in practice we may have to rely on approximations).\n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "![Unnormalized log-posterior and corresponding Laplace Approximation. Source: @murphy2022probabilistic.](/www/images/laplace_posterior.png)\n",
        "\n",
        "> Now we can rely on MC or Probit Approximation to compute posterior predictive (classification).\n",
        "\n",
        "# Laplace Redux in Julia \n",
        "\n",
        "## [`LaplaceRedux.jl`](https://github.com/juliatrustworthyai/LaplaceRedux.jl) - a small package 📦\n",
        "\n",
        "[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://juliatrustworthyai.github.io/LaplaceRedux.jl/dev) [![Build Status](https://github.com/juliatrustworthyai/LaplaceRedux.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/juliatrustworthyai/LaplaceRedux.jl/actions/workflows/CI.yml?query=branch%3Amain) [![Coverage](https://codecov.io/gh/juliatrustworthyai/LaplaceRedux.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/juliatrustworthyai/LaplaceRedux.jl)\n",
        "\n",
        "> What started out as my first coding project Julia ... \n",
        "\n",
        ":::{.incremental}\n",
        "- Big fan of **learning by coding** so after reading the first chapters of @murphy2022probabilistic I decided to code up Bayesian Logisitic Regression from scratch. \n",
        "- I also wanted to learn Julia at the time, so tried to hit two birds with one stone. \n",
        "- **Outcome**: 1. This [blog post](https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f). 2. I have since been hooked on Julia. \n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "> ... has turned into a small package 📦  with great potential.\n",
        "\n",
        ":::{.incremental}\n",
        "- When coming across the NeurIPS 2021 paper on Laplace Redux for deep learning [@daxberger2021laplace], I figured I could step it up a notch.\n",
        "- **Outcome**: `LaplaceRedux.jl` and another [blog post](https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b).\n",
        ":::\n",
        "\n",
        ". . . \n",
        "\n",
        "So let's add the package ...\n",
        "\n",
        "```julia\n",
        "using Pkg\n",
        "Pkg.add(\"https://github.com/juliatrustworthyai/LaplaceRedux.jl\")\n",
        "```\n",
        "\n",
        "... and use it. \n",
        "\n",
        "\n",
        "```{julia}\n",
        "using LaplaceRedux\n",
        "```\n",
        "\n",
        "\n",
        "## From Bayesian Logistic Regression ...\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='50%'}\n",
        "#### From maths ...\n",
        "\n",
        ". . .\n",
        "\n",
        "We assume a Gaussian prior for our weights ...\n",
        "$$\n",
        "p(\\theta) \\sim \\mathcal{N} \\left( \\theta | \\mathbf{0}, \\lambda^{-1} \\mathbf{I} \\right)=\\mathcal{N} \\left( \\theta | \\mathbf{0}, \\mathbf{H}_0^{-1} \\right)\n",
        "$$ {#eq-prior}\n",
        "\n",
        ". . .\n",
        "\n",
        "... which corresponds to logit binary crossentropy loss with weight decay:\n",
        "\n",
        "$$\n",
        "\\ell(\\theta)= - \\sum_{n}^N [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\\\ \\frac{1}{2} (\\theta-\\theta_0)^T\\mathbf{H}_0(\\theta-\\theta_0)\n",
        "$$ {#eq-loss}\n",
        "\n",
        ". . .\n",
        "\n",
        "For Logistic Regression we have the Hessian in closed form (p. 338 in @murphy2022probabilistic):\n",
        "\n",
        "$$\n",
        "\\nabla_{\\theta}\\nabla_{\\theta}^\\mathsf{T}\\ell(\\theta) = \\frac{1}{N} \\sum_{n}^N(\\mu_n(1-\\mu_n)\\mathbf{x}_n)\\mathbf{x}_n^\\mathsf{T} + \\mathbf{H}_0\n",
        "$$ {#eq-hessian}\n",
        "\n",
        ":::\n",
        ":::{.column width='50%'}\n",
        "#### ... to code\n",
        ". . . \n",
        "\n",
        "```julia\n",
        "# Hessian:\n",
        "function ∇∇𝓁(θ,θ_0,H_0,X,y)\n",
        "    N = length(y)\n",
        "    μ = sigmoid(θ,X)\n",
        "    H = ∑(μ[n] * (1-μ[n]) * X[n,:] * X[n,:]' for n=1:N)\n",
        "    return H + H_0\n",
        "end\n",
        "```\n",
        "\n",
        "> Gotta love Julia ❤️💜💚\n",
        "\n",
        ". . .\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "# Import libraries.\n",
        "using Plots, PlotThemes, Statistics, LaplaceRedux\n",
        "theme(:wong)\n",
        "# Number of points to generate.\n",
        "xs, y = LaplaceRedux.Data.toy_data_linear(100)\n",
        "X = hcat(xs...); # bring into tabular format\n",
        "data = zip(xs,y);\n",
        "```\n",
        "\n",
        "\n",
        "Logistic Regression can be done in `Flux` ...\n",
        "\n",
        "\n",
        "```{julia}\n",
        "using Flux\n",
        "# Initializing weights as zeros only for illustrative purposes:\n",
        "nn = Chain(Dense(zeros(1,2),zeros(1))) \n",
        "```\n",
        "\n",
        "\n",
        ". . .\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "λ = 0.5\n",
        "sqnorm(x) = sum(abs2, x)\n",
        "weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\n",
        "loss(x, y; λ=λ) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization(λ)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "using Flux.Optimise: update!, ADAM\n",
        "opt = ADAM()\n",
        "epochs = 50\n",
        "\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "end\n",
        "```\n",
        "\n",
        "\n",
        "... but now we autograd! Leveraged in `LaplaceRedux`.\n",
        "\n",
        "\n",
        "```{julia}\n",
        "la = Laplace(nn, λ=λ)\n",
        "fit!(la, data)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "p_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin);\n",
        "p_laplace = plot_contour(X',y,la;title=\"Laplace\")\n",
        "# Plot the posterior distribution with a contour plot.\n",
        "plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\n",
        "savefig(plt, joinpath(www_path,\"posterior_predictive_logit.png\"))\n",
        "```\n",
        "\n",
        "\n",
        "![Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Image by author.](/www/images/posterior_predictive_logit.png){#fig-pred-logit width=\"70%\"}\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "Λ = [1e10, 0.5, 1e-10]\n",
        "plts = []\n",
        "nns = []\n",
        "for λ ∈ Λ\n",
        "  nn = Chain(Dense(zeros(1,2),zeros(1)))\n",
        "  # train\n",
        "  epochs = 50\n",
        "  for epoch = 1:epochs\n",
        "    for d in data\n",
        "      gs = gradient(Flux.params(nn)) do\n",
        "        l = loss(d...;λ=λ)\n",
        "      end\n",
        "      update!(opt, Flux.params(nn), gs)\n",
        "    end\n",
        "  end\n",
        "  # laplace\n",
        "  la = Laplace(nn, λ=λ)\n",
        "  fit!(la, data)  \n",
        "  # plot \n",
        "  plt = plot_contour(X',y,la;title=\"σ₀²=$(1/λ)\")\n",
        "  plts = vcat(plts..., plt)\n",
        "  nns = vcat(nns..., nn)\n",
        "end\n",
        "plt = plot(plts..., layout=(1,3), size=(1200,300))\n",
        "savefig(plt, joinpath(www_path,\"posterior_predictive_prior_logit.png\"))\n",
        "```\n",
        "\n",
        "\n",
        "## ... to Bayesian Neural Networks\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='30%'}\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "# Number of points to generate:\n",
        "xs, y = LaplaceRedux.Data.toy_data_non_linear(200)\n",
        "X = hcat(xs...); # bring into tabular format\n",
        "data = zip(xs,y)\n",
        "```\n",
        "\n",
        "\n",
        "#### Code\n",
        "\n",
        ". . .\n",
        "\n",
        "An actual MLP ...\n",
        "\n",
        "\n",
        "```{julia}\n",
        "# Build MLP:\n",
        "n_hidden = 32\n",
        "D = size(X)[1]\n",
        "nn = Chain(\n",
        "    Dense(\n",
        "      randn(n_hidden,D)./10,\n",
        "      zeros(n_hidden), σ\n",
        "    ),\n",
        "    Dense(\n",
        "      randn(1,n_hidden)./10,\n",
        "      zeros(1)\n",
        "    )\n",
        ")  \n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "λ = 0.01\n",
        "sqnorm(x) = sum(abs2, x)\n",
        "weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\n",
        "loss(x, y; λ=λ) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization(λ)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "# Training:\n",
        "epochs = 200\n",
        "for epoch = 1:epochs\n",
        "  for d in data\n",
        "    gs = gradient(Flux.params(nn)) do\n",
        "      l = loss(d...)\n",
        "    end\n",
        "    update!(opt, Flux.params(nn), gs)\n",
        "  end\n",
        "end\n",
        "```\n",
        "\n",
        "\n",
        ". . .\n",
        "\n",
        "... same API call:\n",
        "\n",
        "\n",
        "```{julia}\n",
        "la = Laplace(\n",
        "  nn, λ=λ, \n",
        "  subset_of_weights=:last_layer\n",
        ")\n",
        "fit!(la, data)\n",
        "```\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "p_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin)\n",
        "p_laplace = plot_contour(X',y,la;title=\"Laplace\")\n",
        "# Plot the posterior distribution with a contour plot.\n",
        "plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\n",
        "savefig(plt, joinpath(www_path,\"posterior_predictive_mlp.png\"))\n",
        "```\n",
        "\n",
        "\n",
        ". . .\n",
        "\n",
        ":::\n",
        ":::{.column width='70%'}\n",
        "#### Results\n",
        "\n",
        ". . .\n",
        "\n",
        "![Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Image by author.](/www/images/posterior_predictive_mlp.png){#fig-pred-mlp width=\"100%\"}\n",
        ":::\n",
        "::::\n",
        "\n",
        "## A quick note on the prior \n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "Λ = [1e10, 0.01, 1e-10]\n",
        "plts = []\n",
        "nns = []\n",
        "for λ ∈ Λ\n",
        "  nn = Chain(\n",
        "      Dense(randn(n_hidden,D)./10,zeros(n_hidden), σ),\n",
        "      Dense(randn(1,n_hidden)./10,zeros(1))\n",
        "  )  \n",
        "  # train\n",
        "  epochs = 200\n",
        "  for epoch = 1:epochs\n",
        "    for d in data\n",
        "      gs = gradient(Flux.params(nn)) do\n",
        "        l = loss(d...;λ=λ)\n",
        "      end\n",
        "      update!(opt, Flux.params(nn), gs)\n",
        "    end\n",
        "  end\n",
        "  # laplace\n",
        "  la = Laplace(nn, λ=λ)\n",
        "  fit!(la, data)  \n",
        "  # plot \n",
        "  plt = plot_contour(X',y,la;title=\"σ₀²=$(1/λ)\")\n",
        "  plts = vcat(plts..., plt)\n",
        "  nns = vcat(nns..., nn)\n",
        "end\n",
        "plt = plot(plts..., layout=(1,3), size=(1200,300))\n",
        "savefig(plt, joinpath(www_path,\"posterior_predictive_prior.png\"))\n",
        "```\n",
        "\n",
        "\n",
        "\n",
        "Low prior uncertainty $\\rightarrow$ posterior dominated by prior. High prior uncertainty $\\rightarrow$ posterior approaches MLE.\n",
        "\n",
        "#### Logistic Regression\n",
        "\n",
        "![Prior uncertainty increases from left to right (Logsitic Regression). Image by author.](/www/images/posterior_predictive_prior_logit.png){#fig-pred-prior-logit width=\"75%\"}\n",
        "\n",
        "#### MLP\n",
        "\n",
        "![Prior uncertainty increases from left to right (MLP). Image by author.](/www/images/posterior_predictive_prior.png){#fig-pred-prior width=\"75%\"}\n",
        "\n",
        "\n",
        "## A crucial detail I skipped\n",
        "\n",
        "> We're really been using linearized neural networks ...\n",
        "\n",
        ". . .\n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### MC fails\n",
        "\n",
        ":::{.incremental}\n",
        "- Could do Monte Carlo for true BNN predictive, but this performs poorly when using approximations for the Hessian.\n",
        "- Instead we rely on **linear expansion** of predictive around mode [@immer2020improving].\n",
        "- **Intuition**: Hessian approximation involves linearization, then so should the predictive. \n",
        ":::\n",
        "\n",
        ". . .\n",
        "\n",
        "> Applying the GNN approximation [...] turns the underlying probabilistic model locally from a BNN into a GLM [...] Because we have effectively done inference in the GGN-linearized model, we should instead predict using these modified features.\n",
        "> --- @immer2020improving\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "![MC samples from the Laplace posterior [@lawrence2001variational].](/www/images/mc_fail.png){#fig-mc-fail}\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "# Goals and Ambitions 🎯\n",
        "\n",
        "## JuliaCon 2022 and beyond\n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "\n",
        ":::{.fragment .semi-fade-out fragment-index=4}\n",
        "#### To JuliaCon ...\n",
        "\n",
        ":::{.fragment .strike fragment-index=1}\n",
        "Learn about Laplace Redux by implementing it in Julia.\n",
        ":::\n",
        "\n",
        ":::{.fragment .strike fragment-index=2}\n",
        "Turn code into a small package. \n",
        ":::\n",
        "\n",
        ":::{.fragment .strike fragment-index=3}\n",
        "Submit to [JuliaCon 2022](https://juliacon.org/2022/) and share the idea.\n",
        ":::\n",
        ":::\n",
        "\n",
        "#### ... and beyond\n",
        "\n",
        ". . .\n",
        "\n",
        "Package is bare-bones at this point and needs a lot of work.\n",
        "\n",
        ":::{.incremental}\n",
        "- **Goal**: reach same level of maturity as Python [counterpart](https://aleximmer.github.io/Laplace/). (Beautiful work btw!)\n",
        "- **Problem**: limited capacity and fairly new to Julia.\n",
        "- **Solution**: find contributors 🤗.\n",
        ":::\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "![Photo by [Ivan Diaz](https://unsplash.com/@ivvndiaz?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText) on [Unsplash](https://unsplash.com/?utm_source=unsplash&utm_medium=referral&utm_content=creditCopyText)](/www/images/launch.jpeg){width=\"70%\"}\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## Specific Goals\n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "#### Easy\n",
        "\n",
        "- Still missing support for multi-class and regression. \n",
        "- Due diligence: peer review and unit testing.\n",
        "\n",
        "#### Harder\n",
        "\n",
        "- Hessian approximations still quadratically large: use factorizations.\n",
        "- Hyperparameter tuning: what about that prior?\n",
        "- Scaling things up: subnetwork inference.\n",
        "- Early stopping: do we really end up at the mode?\n",
        "- ...\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"50%\"}\n",
        "\n",
        "![Source: [Giphy](https://giphy.com/gifs/theoffice-nbc-the-office-tv-dUHdTk3tvry9NETa67)](/www/images/dwight.gif)\n",
        "\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "\n",
        "## More Resources 📚\n",
        "\n",
        "::::{.columns}\n",
        "\n",
        ":::{.column width=\"60%\"}\n",
        "> Read on ...\n",
        "\n",
        "- Blog post (1) -- Bayesian Logisitic Regression: [[TDS](https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f), [homepage](https://www.patalt.org/blog/posts/bayesian-logit/)].\n",
        "- Blog post (2) -- Bayesian Deep Learning: [[TDS](https://towardsdatascience.com/go-deep-but-also-go-bayesian-ab25efa6f7b), [homepage](https://www.patalt.org/blog/posts/effortsless-bayesian-dl/)].\n",
        "- Detailed slide pack generously shared by [Professor José Miguel Hernández-Lobato](https://jmhl.org/): [[pdf](https://github.com/juliatrustworthyai/LaplaceRedux.jl/blob/main/dev/resources/juliacon22/slides_lobato.pdf)]\n",
        "- Package [docs](https://www.patalt.org/CounterfactualExplanations.jl/dev/).\n",
        "\n",
        "> ... or even better: get involved! 🤗\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width=\"40%\"}\n",
        "\n",
        "<img src=\"/www/images/profile.png\" height=\"auto\" width=\"250\" style=\"border-radius:50%; display: block; margin-left: auto; margin-right: auto;\">\n",
        "\n",
        "<div style=\"text-align: center;\">\n",
        "  <p style=\"display: inline; vertical-align: middle\"> \n",
        "    <a href=\"https://www.linkedin.com/in/patrick-altmeyer-a2a25494/\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n",
        "      <font style=\"\">\n",
        "        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/d0fc399dee4218d1e0e0399b8947acab.png\" alt=\"LinkedIn (Personal)\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n",
        "      </font>\n",
        "    </a>\n",
        "    <a href=\"https://twitter.com/paltmey\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n",
        "      <font style=\"\">\n",
        "        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/3949237f892004c237021ac9e3182b1d.png\" alt=\"Twitter\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n",
        "      </font>\n",
        "    </a>\n",
        "    <a href=\"https://github.com/pat-alt\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n",
        "      <font style=\"\">\n",
        "        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/47f4eb2d0082a8a3611d614b75a09db8.png\" alt=\"Github\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n",
        "      </font>\n",
        "    </a>\n",
        "    <a href=\"https://medium.com/@patrick.altmeyer\" style=\"display: inline-block; color: rgb(207, 142, 255) !important;\">\n",
        "      <font style=\"\">\n",
        "        <img width=\"60\" height=\"60\" src=\"https://s1g.s3.amazonaws.com/175f49662614345cb7dbb95fce3f88af.png\" alt=\"Medium\" style=\"border: none; max-width: 100%; height: 60px !important;\">\n",
        "      </font>\n",
        "    </a>\n",
        "  </p>\n",
        "</div>\n",
        "\n",
        "<img src=\"/www/images/qr.png\" height=\"auto\" width=\"100\" style=\"display: block; margin-left: auto; margin-right: auto;\">\n",
        ":::\n",
        "\n",
        "::::\n",
        "\n",
        "## References "
      ],
      "id": "3c7253d2"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}