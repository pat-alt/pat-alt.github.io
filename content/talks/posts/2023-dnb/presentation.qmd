---
title: Faithful Model Explanations
subtitle: Supervision in the Age of AI
author: 
  - name: "**Patrick Altmeyer**"
    url: https://www.paltmeyer.com/
  - name: Venkatesh Chandrasekar 
institute: Delft University of Technology
date: today
format: 
  tudelft-revealjs:
    theme: custom.scss
    self-contained: true
    smaller: false
    scrollable: true
    preview-links: auto
    slide-number: true
    transition: slide
    background-transition: fade
    fig-align: center
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          MathJax = {
            options: {
              menuOptions: {
                settings: {
                  assistiveMml: false
                }
              }
            }
          };
          </script>
revealjs-plugins:
  - pointer
crossref: 
  prp-title: RQ
  prp-prefix: RQ
bibliography: ../../../../bib.bib
---

## Quick Introduction {.smaller}

::::{.columns}

:::{.column width="60%"}

- I recently entered the 3rd year of my PhD in Trustworthy Artificial Intelligence at Delft University of Technology.
- Part of the [AI for FinTech Research Lab](https://se.ewi.tudelft.nl/ai4fintech/index.html)---5yr collaboration between TU Delft and ING.
- Previously, educational background in Economics and Finance and two years in Monetary Policy at the [Bank of England](https://www.bankofengland.co.uk/).
- Focus on applying Trustworthy AI to real-world problems, particularly in the financial sector.
- [Blogger](https://www.paltmeyer.com/blog/), Julia [developer](https://github.com/pat-alt) and founder of [Taija](https://github.com/JuliaTrustworthyAI).

:::

:::{.column width="40%"}

<img src="/www/images/qr.png" height="auto" width="250" style="display: block; margin-left: auto; margin-right: auto;">

<div style="text-align: center;">
  <p style="display: inline; vertical-align: middle"> 
    <a href="https://www.linkedin.com/in/patrick-altmeyer-a2a25494/" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/d0fc399dee4218d1e0e0399b8947acab.png" alt="LinkedIn (Personal)" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://twitter.com/paltmey" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/3949237f892004c237021ac9e3182b1d.png" alt="Twitter" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://github.com/pat-alt" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/47f4eb2d0082a8a3611d614b75a09db8.png" alt="Github" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://medium.com/@patrick.altmeyer" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/175f49662614345cb7dbb95fce3f88af.png" alt="Medium" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
  </p>
</div>
:::

::::

## Research {.smaller}

::::{.columns}::::
:::{.column width='50%'}
##### Trustworthy AI

- Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals (under review).
- Endogenous Macrodynamics in Algorithmic Recourse [@altmeyer2023endogenous].
- Explaining Black-Box Models Through Counterfactuals [@altmeyer2023explaining]. 
:::
:::{.column width='50%'}
##### Finance and Economics

- Yield curve sensitivity to investor positioning around economic shocks [@altmeyer2023yield] (BoE Staff Working Paper).
- Deep vector autoregression for macroeconomic data [@agusti2021deep] (NeurIPS MLECON 2021, DNB DS workshop and published through [BIS](https://www.bis.org/ifc/publ/ifcb59_39.pdf))
- Option pricing in the Heston stochastic volatility model: an empirical evaluation [@altmeyer2018option]
:::
::::

# Delft FinTech Lab

## In A Nutshell

::::{.columns}::::
:::{.column width='50%'}
### Background

- Finance at the forefront of digitalization.
- Over 50 TU Delft researchers in FinTech
- With dozens of societal partners
:::
:::{.column width='50%'}
### Objective

- Strengthen societal and industrial impact
- Increase collaboration and visibility
:::
::::

## Core Pillars {.smaller}

> "[...] unified front of our expertise in this area formed to help the financial industry solve the growingly complex challenges."
>
> ---[Delft FinTech Lab](https://www.tudelft.nl/fintech)

1. [Trustworthy Financial Systems](https://www.tudelft.nl/fintech/expertise#c974723) led by Stefan Buijsman.
2. [Quantitative Modelling](https://www.tudelft.nl/fintech/expertise#c974723) led by Antonis Papapantoleon.
3. [Financial Data Intelligence](https://www.tudelft.nl/fintech/expertise#c974723) led by Asterios Katsifodimos.
4. [Blockchain](https://www.tudelft.nl/fintech/expertise#c974723) led by JÃ©rÃ©mie Decouchant.

::: {.notes}
1. Reliable and trustworthy financial systems are explainable and transparent.
2. Model development and validation, derivative pricing, and investment analysis. 
3. Artificial intelligence, data and software analytics.
4. Development of decentralized systems.
:::

. . .

> What about topics specific to supervision?

## Natural language processing 

1. Mine large volumes of text.
2. Retrieve relevant information from large collections of text.
3. Long document understanding and track/identify/predict anomaly patterns.
4. Explainable AI for NLP [@arous2021marta].
5. Predictive Uncertainty Quantification for LLMs (see [blog](https://www.paltmeyer.com/blog/posts/conformal-llm/) post). 

## Privacy Enhancing Technologies (PET)

1. Synthetic data generation by ML, algorithmic, probabilistic and statistical techniques [@porsius2023private;@van2021one].
2. Statistics/econometrics under privacy constraints ex. differential privacy. 
3. Multiparty computations and homomorphic encryptions.

## Human-AI Collaboration

> Augmenting human experts with AI.

1. Hybrid human-AI workflows: amplify human intelligence, reduce human costs, and improve precaution.
2. Collaborative human-AI knowledge synthesis.
3. Explainable AI [@anand2022explainable; @leonhardt2023extractive], and human-in-the-loop continual learning [@yang2018leveraging].

## Automated Compliance

1. Ensure decisions made by banks' models are compliant by being explainable and robust: 
  - Counterfactual reasoning and contestability [@altmeyer2023endogenous; @altmeyer2023explaining]
2. Ethics assessment, AI governance 
3. Predictive uncertainty: How robust/uncertain predictions are (conformal predictions) in a model-agnostic manner.

# Background

## Counterfactual Explanations

> Born out of the [need for explanations](https://www.turing.ac.uk/research/impact-stories/a-right-to-explanation) ...

**Counterfactual Explanation** (CE) explain how inputs into a model need to change for it to produce different outputs [@wachter2017counterfactual].

. . .

Provided the changes are realistic and actionable, they can be used for **Algorithmic Recourse** (AR) to help individuals who face adverse outcomes.

## Example: Consumer Credit

From 'loan denied' to 'loan supplied': [`CounterfactualExplanations.jl`](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) ðŸ“¦.

::::{.columns}::::
:::{.column width='55%'}
![Gradient-based counterfactual search.](www/ce_intro.gif){#fig-intro}
:::
:::{.column width='45%'}
![Counterfactuals for Give Me Some Credit dataset [@kaggle2011give].](www/gmsc.svg){#fig-credit}
:::
::::

## Example: Insurance Premium^[For simplicity, we'll stay in the classification setting. Work on counterfactual regression like @spooner2021counterfactual exists but it is scarce.] {transition="none" .smaller}

:::{.incremental}

- **Input** $\mathbf{X}$: A dataset of individuals containing demographic and financial information.
- **Additional Input** $\mathbf{Z}$: Individuals can opt-in to provide their personal Apple Health data to improve their chance of receiving a lower premium.
- **Binary output** $\mathbf{Y}$: based on the data, the individual is either eligible ($y=1$) or not eligible ($y=0$) for a lower premium.
- To model $p(y=1|X)$ the insurance provider can rely on an interpretable linear classifier. 
- To model $p(y=1|X,Z)$ the insurance provider turns to a more accurate but less interpretable black-box model.
:::

## Example: Insurance Premium {transition="none"}

In the EU, individuals have the right "[...] to *obtain* an explanation of the decision reached after such assessment and to challenge the decision." ([Recital 71 of the General Data Protection Regulation (GDPR)](https://gdpr-info.eu/recitals/no-71/))

. . .

> In our example, who do you think is most likely to ask for an explanation?

# Een heel kleen beetje maths ...

## Gradient-based Counterfactual Search

The starting point for most counterfactual generators is as follows,

$$
\begin{aligned}
\mathbf{Z}^\prime =& \arg \min_{\mathbf{Z}^\prime \in \mathcal{Z}^L} \{  {\text{yloss}(M_{\theta}(f(\mathbf{Z}^\prime)),\mathbf{y}^+)} \\ &+ \lambda {\text{cost}(f(\mathbf{Z}^\prime)) }  \} 
\end{aligned} 
$$ {#eq-cf}

where $\mathbf{Z}^\prime$ is a counterfactual, $M_{\theta}$ is the black-box model and $\mathbf{y}^+$ is the desired output.

## But wait a second ...

@eq-cf looks a lot like an adversarial attack [@goodfellow2014explaining], doesn't it? 

![Adversarial attack on an Image Classifier.](www/panda.png){#fig-adv-example}

In both settings, we take gradients with respect to features $\nabla_{\mathbf{Z}^\prime}\text{yloss}(M_{\theta}(f(\mathbf{Z}^\prime)),\mathbf{y}^+)$ in order to trigger changes in the model's output.

## Gradient Descend Visualized

![Gradient-based counterfactual search.](www/ce_intro.gif){#fig-intro}

# Our Research

## Open Questions

:::{.incremental}

1. What makes a counterfactual **plausible**?
2. Why do we need plausibility?
3. Is plausibility all we need?
4. What makes models more **explainable**?

:::

## Plausibility

There's no consensus on the exact definition of plausibility but we think about it as follows:

::: {#def-plausible}

## Plausible Counterfactuals

Let $\mathcal{X}|\mathbf{y}^+= p(\mathbf{x}|\mathbf{y}^+)$ denote the true conditional distribution of samples in the target class $\mathbf{y}^+$. Then for $\mathbf{x}^{\prime}$ to be considered a plausible counterfactual, we need: $\mathbf{x}^{\prime} \sim \mathcal{X}|\mathbf{y}^+$.

:::

## Counter Example

::::{.columns}::::
:::{.column width='50%'}
- The counterfactual in @fig-implausible is valid: it has crossed the decision boundary. 
- But is it consistent with the data in the target class (blue)?
:::
:::{.column width='50%'}
![A valid but implausible counterfactual. Source: @altmeyer2023explaining](www/implausible.png){#fig-implausible}
:::
::::

## Why Plausibility?

- Actionability: If a counterfactual is implausible, it is unlikely to be actionable.
- Fairness: If a counterfactual is implausible, it is unlikely to be fair.
- Robustness: If a counterfactual is implausible, it is unlikely to be robust.

**But**: Higher plausibility seems to require larger changes and hence increase costs to individuals. 

## Recourse Dynamics

Moving just across the decision boundary may minimize costs to individuals but it may also generate external costs for other stakeholders [@altmeyer2023endogenous].

![](www/dynamics_poc.png){align="center" width="100%"}

## A Balancing Act

:::{.incremental}
- Minimizing **private** costs generates **external** costs for other stakeholders.
- To avoid this, counterfactuals need to be **plausible**, i.e. comply with the data-generating process.
- In practice, costs to various stakeholders need to be carefully **balanced**.
:::

. . . 

> Is plausibility really all we need?

## Pick your Poison?

All of these counterfactuals are valid explanations for the model's prediction. Which one would you pick?

![Turning a 9 into a 7: Counterfactual Examplanations for an Image Classifier.](www/poison.png){#fig-cf-example}

## What do Models Learn?

These images are sampled from the posterior distribution learned by the model. Looks different, no?

![Conditional Generated Images from the Image Classifier](www/learn.png){#fig-learn}

## Faithful Counterfactuals {.smaller}

::::{.columns}::::
:::{.column width='60%'}
We propose a way to generate counterfactuals that are as plausible as the underlying model permits (under review).

::: {#def-faithful}

## Faithful Counterfactuals

Let $\mathcal{X}_{\theta}|\mathbf{y}^+ = p_{\theta}(\mathbf{x}|\mathbf{y}^+)$ denote the conditional distribution of $\mathbf{x}$ in the target class $\mathbf{y}^+$, where $\theta$ denotes the parameters of model $M_{\theta}$. Then for $\mathbf{x}^{\prime}$ to be considered a faithful counterfactual, we need: $\mathbf{x}^{\prime} \sim \mathcal{X}_{\theta}|\mathbf{y}^+$.

:::

:::
:::{.column width='40%'}
![](www/mnist_eccco_new.png)
:::
::::

![Gradient fields and counterfactual paths for different generators.](www/poc_gradient_fields.png){#fig-poc-gradient-fields width="65%"}

## Improving Models

Now that we have a tool to faithfully explain models we may ask: **how** do models learn plausible explanations? Initial evidence:

1. Incorporating predictive uncertainty (e.g. ensembling). 
2. Addressing robustness (e.g. adversarial training in @schut2021generating).
3. Better model architectures.
4. Hybrid modelling (i.e. combining generative and discriminative models).

## Example: Architecture

![Counterfactuals for LeNet-5 convolutional neural network [@lecun1998gradient].](www/mnist_all_lenet_eccco.png){#fig-mnist-lenets width="100%"}

## Example: JEM Ensemble

![Counterfactuals for an ensemble of Joint Energy Models (JEM) [@grathwohl2020your].](www/mnist_all_jem_ens_eccco.png){#fig-mnist-jem width="100%"}

# Open-Source work in `Julia`

## ðŸ¶ Taija {.smaller}

> Research informs development, development informs research. 

[![Trustworthy Artificial Intelligence in Julia.](www/logo.png){width="50%"}](https://github.com/JuliaTrustworthyAI)

[Taija](https://github.com/JuliaTrustworthyAI) is a collection of open-source packages for Trustworthy AI in Julia. Our goal is to help researchers and practitioners assess the trustworthiness of predictive models.

Our work has been presented online for JuliaCon 2022, at MIT in Boston for JuliaCon 2023 and hopefully beyond. 

## Counterfactual Explanations

All the work presented today is powered by [`CounterfactualExplanations.jl`](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) ðŸ“¦.

There is also a corresponding paper, [*Explaining Black-Box Models through Counterfactuals*](https://proceedings.juliacon.org/papers/10.21105/jcon.00130), which has been published in JuliaCon Proceedings. 

If you decide to use this package in your work, please consider citing the paper:

[![DOI](https://proceedings.juliacon.org/papers/10.21105/jcon.00130/status.svg)](https://doi.org/10.21105/jcon.00130) [![DOI](https://zenodo.org/badge/440782065.svg)](https://zenodo.org/badge/latestdoi/440782065)

## Conformal Prediction

Conformal Prediction is a model-agnostic, distribution-free approach to Predictive Uncertainty Quantification: [`ConformalPrediction.jl`](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl) ðŸ“¦.

::::{.columns}::::
:::{.column width='45%'}
![Conformal Prediction intervals for regression.](www/cp_anim.gif){#fig-reg}
:::
:::{.column width='55%'}
![Conformal Prediction sets for an Image Classifier.](www/cp_example.svg){#fig-cp-example}
:::
::::

## Laplace Redux

::::{.columns}::::
:::{.column width='50%'}
Effortless Bayesian Deep Learning through Laplace Approximation [@immer2020improving, @daxberger2021laplace]: [`LaplaceRedux.jl`](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl) ðŸ“¦.
:::
:::{.column width='50%'}
![Predictive interval for neural network with Laplace Approximation.](www/laplace.png){#fig-laplace width="80%"}
:::
::::

## Joint Energy Models

Joint Energy Models (JEMs) are hybrid models trained to learn the conditional output **and** input distribution [@grathwohl2020your]: [`JointEnergyModels.jl`](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl) ðŸ“¦.

![A JEM trained on Circles data.](www/jem.svg){#fig-jem }

## Questions? {.smaller}

::::{.columns}::::
:::{.column width='50%'}
Includes joint work with Cynthia C. S. Liem, Arie van Deursen, Mojtaba Farmanbar, Aleksander Buszydlik, Karol Dobiczek, Giovan Angela and many other students at TU Delft.

Slides power by [Quarto](https://quarto.org/).

:::
:::{.column width='50%'}
<img src="/www/images/qr.png" height="auto" width="250" style="display: block; margin-left: auto; margin-right: auto;">

<div style="text-align: center;">
  <p style="display: inline; vertical-align: middle"> 
    <a href="https://www.linkedin.com/in/patrick-altmeyer-a2a25494/" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/d0fc399dee4218d1e0e0399b8947acab.png" alt="LinkedIn (Personal)" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://twitter.com/paltmey" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/3949237f892004c237021ac9e3182b1d.png" alt="Twitter" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://github.com/pat-alt" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/47f4eb2d0082a8a3611d614b75a09db8.png" alt="Github" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
    <a href="https://medium.com/@patrick.altmeyer" style="display: inline-block; color: rgb(207, 142, 255) !important;">
      <font style="">
        <img width="60" height="60" src="https://s1g.s3.amazonaws.com/175f49662614345cb7dbb95fce3f88af.png" alt="Medium" style="border: none; max-width: 100%; height: 60px !important;">
      </font>
    </a>
  </p>
</div>
:::
::::



## References


