---
title: Stop Making Unscientific AGI Performance Claims
subtitle: Forty-first International Conference on Machine Learning (ICML)
author: 
  - name: Patrick Altmeyer
    url: https://www.patalt.org/
  - name: Andrew M. Demetriou
  - name: Antony Bartlett
  - name: Cynthia C. S. Liem
institute: Delft University of Technology
date: today
format: 
  tudelft-revealjs:
    theme: custom.scss
    width: 1244.45
    self-contained: true
    smaller: false
    scrollable: true
    preview-links: auto
    slide-number: true
    transition: slide
    background-transition: fade
    fig-align: center
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          MathJax = {
            options: {
              menuOptions: {
                settings: {
                  assistiveMml: false
                }
              }
            }
          };
          </script>
revealjs-plugins:
  - pointer
crossref: 
  prp-title: RQ
  prp-prefix: RQ
bibliography: biblio.bib
classoption: "notheorems"
---

# Introduction {.nostretch} 

Economist by training, previously Bank of England, currently 3rd year PhD in Trustworthy AI \@ TU Delft.

![](/www/images/qr.png){width="25%" fig-align="center"}

## Motivation {auto-animate=true}

- $A_1$: $enc($„It is essential to bring inflation back to target to avoid drifting into deflation territory.“$)$
- $A_2$: $enc($„It is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.“$)$

## Motivation {auto-animate=true .smaller}

::::{.columns}::::
:::{.column width='50%'}
- $A_1$: $enc($„It is essential to bring inflation back to target to avoid drifting into deflation territory.“$)$
- $A_2$: $enc($„It is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.“$)$

> "They're exactly the same."
> 
> --- Linear probe $\widehat{cpi}=f(A)$
:::
:::{.column width='50%'}
![](www/spider.jpeg)
:::
::::

## Position

> Current LLMs embed knowledge. They don‘t „understand“ anything. They are useful tools, but tools nonetheless.

:::{.incremental}

- Meaningful patterns in embeddings are like doves in the sky.
- Humans are prone to seek patterns and anthropomorphize. 
- Observed ‘sparks’ of Artificial General Intelligence are spurious.
- The academic community should exercise extra caution.
- Publishing incentives need to be adjusted.

:::

## Outline

:::{.incremental}

- **Experiments**: We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers.
  - All of them successfully distill knowledge and yet none of them develop true understanding.
- **Social sciences review**: Humans are prone to seek patterns and anthropomorphize.
- **Conclusion and outlook**: More caution at the individual level, and different incentives at the institutional level.

:::

# "There! It's sentient!" {.nostretch} 

![](www/leo.png){fig-align="center"}

## The Holy Grail {.smaller}

Achievement of Artificial General Intelligence (AGI) has become a grand challenge, and in some cases, an explicit business goal.

::::{.columns}::::
:::{.column width='50%'}
### Definition

The definition of AGI itself is not as clear-cut or consistent:

- (loosely) a phenomenon contrasting with ‘narrow AI’ systems, that were trained for specific tasks [@goertzel2014artificial].
:::
:::{.column width='50%'}
### Practice

Researchers have sought to show that AI models generalize to different (and possibly unseen) tasks or show performance
considered ‘surprising’ to humans.

- For example, Google DeepMind claimed their AlphaGeometry model [@trinh2024geometry] reached a ‘milestone’ towards AGI.
:::
::::

## A Perfect Storm

Recent developments in the field have created a ‘perfect storm’ for inflated claims:

:::{.incremental}

-  Early sharing of preprints and code.
-  Volume of publishable work has exploded.
-  Social media influencers start playing a role in article discovery and citeability [@weissburg2024tweets].
-  Complexity is increasing because it is incentivized [@values_in_ML].

:::

## "Not Mere Stochastic Parrots"

- We consider a recently viral work [@gurnee2023languagev1], in which claims about the learning of world models by LLMs were made.
  - Linear probes (ridge regression) were successfully used to predict geographical locations from LLM embeddings.
- Claims on [X](https://twitter.com/tegmark/status/1709572469978231063) that this indicates that LLMs are not mere ‘stochastic parrots’ [@bender2021dangers].
- Reactions on X seemed to largely exhibit excitement and surprise at the authors’ findings.

# "The human mind is a pattern-seeking device"

![](www/tarot.png){.absolute bottom=-300 right=50}

## Are Neural Networks Born with World Models? {.smaller}

::::{.columns}::::
:::{.column width='60%'}
- Llama-2 model tested in @gurnee2023languagev2 has ingested huge amounts of publicly available data [@touvron2023llama].
  - Geographical locations are literally in the training data: e.g. Wikipedia [article](https://en.wikipedia.org/wiki/London) for "London". 
  - Where would this information be encoded if not in the embedding space $\mathcal{A}$? Is it surprising that $A_{\text{LDN}}=enc(\text{"London"}) \not\!\perp\!\!\!\perp (\text{lat}_{\text{LDN}},\text{long}_{\text{LDN}})$?
- @fig-map shows the predicted coordinates of a linear probe on the final-layer activations of an untrained neural network.
:::
:::{.column width='40%'}
![Predicted coordinate values (out-of-sample) from a linear probe on final-layer activations of an untrained
neural network.](www/map.png){#fig-map}

- Model has seen noisy coordinates plus $d$ random features.
- Single hidden layer with $h < d$ hidden units.
:::
::::

## PCA as a Yield Curve Interpreter {.smaller .nostretch}

What are principal components if not model embeddings?

![Top chart: The first two principal components of US Treasury yields over time at daily frequency. Bottom chart: Observed average level and 10yr-3mo spread of the yield curve. Vertical stalks roughly indicate the onset (|GFC) and the beginning of the aftermath (GFC|) of the Global Financial Crisis.](www/pca_yield.png){#fig-pca width="80%"}

## Autoencoders as Economic Growth Predictors {.smaller .nostretch auto-animate=true}

::::{.columns}::::
:::{.column width='50%'}
- We train a neural network with a bottleneck layer to predict GDP growth from the yield curve [@fig-autoenc].
  - **Input**: UST yields at different maturities. 
  - Hidden layer, bottleneck layer, hidden layer. 
  - **Output**: GDP growth.
- Can we use this for more than just forecasting?
:::
:::{.column width='50%'}
![Simple autoencoder architecture.](www/autoenc.png){#fig-autoenc}
:::
::::

## Autoencoders as Economic Growth Predictors {.smaller .nostretch auto-animate=true}

- Yes, this can be used for feature extraction and forecasting:
  - Bottle-neck layer embeddings predict spread and level of the yield curve.

![The left chart shows the actual GDP growth and fitted values from the autoencoder model. The right chart shows the observed average level and spread of the yield curve (solid) along with the predicted values (in-sample) from the linear probe based on the latent embeddings (dashed)](www/dl.png){#fig-dl width="80%"}

## Embedding FOMC comms {.smaller .nostretch}

- BERT-based model trained on FOMC minutes, speeches and press conferences to classify statements as hawkish or dovish (or neutral) [@shah2023trillion].
- We linearly probe all layers to predict unseen economic indicators (CPI, PPI, UST yields).
- Predictive power increases with layer depth and probes outperform simple AR($p$) models.

![Out-of-sample root mean squared error (RMSE) for the linear probe plotted against FOMC-RoBERTa’s n-th
layer for different indicators. ](www/mse_pca_128.png){#fig-mse width="80%"}  

## Sparks of Economic Understanding? {.smaller .nostretch}

**Premise**: If probe results were indicative of some intrinsic ‘understanding’ of the economy, then the probe should not be sensitive to random sentences unrelated to economics. 

#### Parrot Test

1. Select the best-performing probe for each economic indicator.
2. Predict inflation levels for real (related) and perturbed (unrelated) sentences.

![Probe predictions for sentences about inflation of prices (IP), deflation of prices (DP), inflation of birds (IB)
and deflation of birds (DB). The vertical axis shows predicted inflation levels subtracted by the average predicted value
of the probe for random noise.](www/attack_all_measures.png){#fig-attack width="80%"}

As evidenced by @fig-attack, the probe is easily fooled. 

# "We're fascinated with robots because they are reflections of ourselves."

![](www/wall_e.PNG){.absolute bottom=-300 right=50}

## Spurious Relationships

**Definiton**: Varies somewhat [@haig2003spurious] but distinctly implies that the observation of correlations does not imply causation.

- Humans struggle to tell the difference between random and non-random sequences [@falk1997making].
- Lack of expectation that randomness that hints towards a causal relationship will still appear at random. 
- Even experts perceive correlations of inflated magnitude [@nickerson1998confirmation] and causal relationships where none exist [@zgraggen2018investigating].

## Antropomorphism

**Definition**: Human tendency to attribute human-like characteristics to non-human agents and/or objects.

1. Experience as humans is an always-readily-available template to interpret the world [@epley2007seeing].
2. Motivation to avoid loneliness may lead us to anthropomorphize inanimate objects [@epley2007seeing,@waytz2010social].
3. Motivation to be competent may lead us anthropomorphize opaque technologies like LLMs [@epley2007seeing,@waytz2010social]

## Confirmation Bias

**Definition**: Favoring interpretations of evidence that support existing beliefs or hypotheses [@nickerson1998confirmation].

- Hypotheses in present-day AI research are often implicit, often framed simply as a system being more accurate or efficient, compared to other systems.
  - Failing to articulate a sufficiently strong null hypothesis leading to a ‘weak’ experiment [@claesen2022severity].
- Individuals may place greater emphasis on evidence in support of their hypothesis, and lesser emphasis on evidence that opposes it [@nickerson1998confirmation].

## Conclusion and Outlook

- We call for the community to create explicit room for organized skepticism
  - Welcome negative results
  - Encouraging replication studies.
  - Move from authorship to contribution-based credit (see e.g. [Liem and Demetriou, 2023](https://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=10173886) and [Smith, 1997](https://www.bmj.com/content/315/7110/696.short)).
- Return to the Mertonian norms (communism, universalism, disinterestedness, organized skepticism) [@merton1942science].

# Questions? {.nostretch} 

With thanks to my co-authors Andrew M. Demetriou, Antony Bartlett, and Cynthia C. S. Liem and to the audience for their attention.

![](/www/images/qr.png){width="25%" fig-align="center"}

## References {.scrollable .smaller}

::: {#refs}
:::

## Image sources

- Leonardo DiCaprio: Meme template by user on <a href="https://www.reddit.com/r/MemeTemplatesOfficial/comments/g46e21/i_made_a_png_version_of_leonardo_dicaprio/#lightbox">Reddit</a>
- Tarot cards: Photo by <a href="https://unsplash.com/@vivalunastudios?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Viva Luna Studios</a> on <a href="https://unsplash.com/photos/the-love-of-jesus-book-8FobV-Ub0eM?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>
- Wall-E: Photo by <a href="https://unsplash.com/@ray30?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">ray rui</a> on <a href="https://unsplash.com/photos/yellow-and-black-wall-e-toy-on-brown-wooden-table-SyzQ5aByJnE?utm_content=creditCopyText&utm_medium=referral&utm_source=unsplash">Unsplash</a>

## Quote sources

- "There! It's sentient"---that engineer at Google (probably!)
- "The human mind is a pattern-seeking device"---Daniel Kahneman
- "We're fascinated with robots because they are reflections of ourselves."---Ken Goldberg

