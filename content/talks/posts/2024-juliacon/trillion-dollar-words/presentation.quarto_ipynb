{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: ðŸ’° Trillion Dollar Words in Julia\n",
        "subtitle: JuliaCon 2024\n",
        "author: Patrick Altmeyer\n",
        "format:\n",
        "  julia-revealjs: \n",
        "    scrollable: true\n",
        "    title-slide-attributes:\n",
        "      data-background-image: ../www/qr.png\n",
        "    self-contained: true\n",
        "date: 2024-07-11\n",
        "date-format: full\n",
        "bibliography: biblio.bib\n",
        "execute:\n",
        "  freeze: auto\n",
        "  eval: true\n",
        "  echo: true\n",
        "  output: false\n",
        "---\n",
        "\n",
        "```{julia}\n",
        "#| echo: false\n",
        "projectdir = splitpath(pwd()) |>\n",
        "    ss -> joinpath(ss[1:findall([s == \"pat-alt.github.io\" for s in ss])[1]]...) \n",
        "cd(projectdir)\n",
        "\n",
        "include(\"$projectdir/content/talks/posts/2024-juliacon/trillion-dollar-words/setup.jl\")\n",
        "```\n",
        "\n",
        "\n",
        "## [TrillionDollarWords.jl](https://github.com/pat-alt/TrillionDollarWords.jl)\n",
        "\n",
        "[![Stable](https://img.shields.io/badge/docs-stable-blue.svg)](https://pat-alt.github.io/TrillionDollarWords.jl/stable/)\n",
        "[![Dev](https://img.shields.io/badge/docs-dev-blue.svg)](https://pat-alt.github.io/TrillionDollarWords.jl/dev/)\n",
        "[![Build Status](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml/badge.svg?branch=main)](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml?query=branch%3Amain)\n",
        "[![Coverage](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl/branch/main/graph/badge.svg)](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl)\n",
        "[![Code Style: Blue](https://img.shields.io/badge/code%20style-blue-4495d1.svg)](https://github.com/invenia/BlueStyle)\n",
        "\n",
        "A light-weight package providing Julia users easy access to the Trillion Dollar Words dataset and model [@shah2023trillion].\n",
        "\n",
        "::: {.callout-note}\n",
        "\n",
        "## Disclaimer  \n",
        "\n",
        "Please note that I am not the author of the Trillion Dollar Words paper nor am I affiliated with the authors. The package was developed as a by-product of our research and is not officially endorsed by the authors of the paper. \n",
        "\n",
        ":::\n",
        "\n",
        "## Context\n",
        "\n",
        "[ICML 2024](https://icml.cc/Conferences/2024) paper *Stop Making Unscientific AGI Performance Claims* [@altmeyer2024position] ([preprint](https://arxiv.org/abs/2402.03962), [blog post](https://www.patalt.org/blog/posts/eccco/), [code](https://github.com/pat-alt/spurious_sentience)):\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='75%'}\n",
        "- Even simple models can distill meaningful information that predicts external data.\n",
        "- Humans are prone to seek patterns and anthropomorphize.\n",
        ":::\n",
        ":::{.column width='25%'}\n",
        "![](www/qr.png)\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "\n",
        "## Basic Functionality\n",
        "\n",
        "The package provides the following functionality:\n",
        "\n",
        "- Load pre-processed data.\n",
        "- Load the model proposed in the paper. \n",
        "- Basic model inference: compute forward passes and layer-wise activations.\n",
        "- Download pre-computed activations for probing the model.\n",
        "\n",
        "## Loading the Data {.smaller}\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='50%'}\n",
        "\n",
        "#### Sentences\n",
        "\n",
        "40,000 time-stamped sentences from \n",
        "\n",
        "- meeting minutes\n",
        "- press conferences\n",
        "- speeches \n",
        "\n",
        "by members of the Federal Open Market Committee (FOMC):\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| output: true\n",
        "\n",
        "using TrillionDollarWords\n",
        "load_all_sentences() |>\n",
        "  x -> names(x)\n",
        "```\n",
        "\n",
        "\n",
        ":::\n",
        ":::{.column width='50%'}\n",
        "\n",
        "#### All Data\n",
        "\n",
        "Merged data includes economic indicators\n",
        "\n",
        "- Consumer Price Index (CPI)\n",
        "- Producer Price Index (PPI)\n",
        "- US Treasury (UST) yields\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| output: true\n",
        "\n",
        "load_all_data() |>\n",
        "  x -> names(x)\n",
        "```\n",
        "\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Loading the Model\n",
        "\n",
        "- Can be loaded with or without the classifier head.\n",
        "- Uses [Transformers.jl](https://github.com/chengchingwen/Transformers.jl) to retrieve the model from [HuggingFace](https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves).\n",
        "-  Any keyword arguments accepted by `Transformers.HuggingFace.HGFConfig` can also be passed.\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| eval: false\n",
        "\n",
        "load_model(; load_head=false, output_hidden_states=true)\n",
        "```\n",
        "\n",
        "\n",
        "## Basic Model Inference\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='50%'}\n",
        "\n",
        "#### From Scratch\n",
        "\n",
        "Layer-wise activations can be computed as follows:\n",
        "\n",
        "\n",
        "```{julia}\n",
        "#| eval: false\n",
        "\n",
        "df = load_all_sentences()\n",
        "mod = load_model(\n",
        "  load_head=false, \n",
        "  output_hidden_states=true\n",
        ")\n",
        "n = 5\n",
        "queries = df[1:n, :]\n",
        "layerwise_activations(\n",
        "  mod, queries\n",
        ") \n",
        "```\n",
        "\n",
        ":::\n",
        "\n",
        ":::{.column width='50%'}\n",
        "\n",
        "#### From Artifacts\n",
        "\n",
        "We have archived activations for each layer and sentence as [artifacts](https://github.com/pat-alt/TrillionDollarWords.jl/releases/tag/activations_2024-01-17):\n",
        "\n",
        "``` julia\n",
        "using LazyArtifacts\n",
        "\n",
        "artifact\"activations_layer_24\"\n",
        "```\n",
        "\n",
        "> OK, but why would I need all this? ðŸ¤”\n",
        "\n",
        ":::\n",
        "::::\n",
        "\n",
        "\n",
        "# \"There! It's sentient!\" {.nostretch} \n",
        "\n",
        "![](www/leo.png){fig-align=\"center\"}\n",
        "\n",
        "\n",
        "\n",
        "## Motivation {auto-animate=true}\n",
        "\n",
        "- $A_1$: $enc($â€žIt is essential to bring inflation back to target to avoid drifting into deflation territory.â€œ$)$\n",
        "- $A_2$: $enc($â€žIt is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.â€œ$)$\n",
        "\n",
        "## Motivation {auto-animate=true .smaller}\n",
        "\n",
        "::::{.columns}::::\n",
        ":::{.column width='50%'}\n",
        "- $A_1$: $enc($â€žIt is essential to bring inflation back to target to avoid drifting into deflation territory.â€œ$)$\n",
        "- $A_2$: $enc($â€žIt is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.â€œ$)$\n",
        "\n",
        "> \"They're exactly the same.\"\n",
        "> \n",
        "> --- Linear probe $\\widehat{cpi}=f(A)$\n",
        ":::\n",
        ":::{.column width='50%'}\n",
        "![](www/spider.jpeg)\n",
        ":::\n",
        "::::\n",
        "\n",
        "## Embedding FOMC comms {.nostretch}\n",
        "\n",
        "- We linearly probe all layers to predict unseen economic indicators (CPI, PPI, UST yields).\n",
        "- Predictive power increases with layer depth and probes outperform simple AR($p$) models.\n",
        "\n",
        "![Out-of-sample root mean squared error (RMSE) for the linear probe plotted against FOMC-RoBERTaâ€™s n-th\n",
        "layer for different indicators. ](www/mse_pca_128.png){#fig-mse width=\"80%\"}  \n",
        "\n",
        "## Sparks of Economic Understanding? {.nostretch}\n",
        "\n",
        "If probe results were indicative of some intrinsic â€˜understandingâ€™, probe should not be sensitive to unrelated sentences.\n",
        "\n",
        "![Probe predictions for sentences about inflation of prices (IP), deflation of prices (DP), inflation of birds (IB)\n",
        "and deflation of birds (DB). The vertical axis shows predicted inflation levels subtracted by the average predicted value\n",
        "of the probe for random noise.](www/attack_all_measures.png){#fig-attack width=\"80%\"}\n",
        "\n",
        "## Intended Purpose and Goals\n",
        "\n",
        "Good starting point for the following ideas:\n",
        "\n",
        "- Fine-tune additional models on the classification task or other tasks of interest.\n",
        "- Further model probing, e.g. using other market indicators not discussed in the original paper.\n",
        "- Improve and extend the label annotations. \n",
        "\n",
        "Any contributions are very much welcome.\n",
        "\n",
        "# Questions? {.nostretch} \n",
        "\n",
        "With thanks to my co-authors Andrew M. Demetriou, Antony Bartlett, and Cynthia C. S. Liem and to the audience for their attention.\n",
        "\n",
        "![](/www/images/qr.png){width=\"25%\" fig-align=\"center\"}\n",
        "\n",
        "## References {.scrollable .smaller}\n",
        "\n",
        "::: {#refs}\n",
        ":::\n",
        "\n",
        "## Image sources\n",
        "\n",
        "- Leonardo DiCaprio: Meme template by user on <a href=\"https://www.reddit.com/r/MemeTemplatesOfficial/comments/g46e21/i_made_a_png_version_of_leonardo_dicaprio/#lightbox\">Reddit</a>\n",
        "\n",
        "## Quote sources\n",
        "\n",
        "- \"There! It's sentient\"---that engineer at Google (probably!)\n"
      ],
      "id": "f327d8d0"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}