---
title: Trustworthy AI in Julia
subtitle: The Alan Turing Institute
author: 
  - name: Patrick Altmeyer
    url: https://www.paltmeyer.com/
institute: Delft University of Technology
date: 2024-05-08
format: 
  julia-revealjs:
    theme: custom.scss
    self-contained: true
    smaller: false
    scrollable: true
    preview-links: auto
    slide-number: true
    transition: slide
    background-transition: fade
    fig-align: center
revealjs-plugins:
  - pointer
crossref: 
  prp-title: RQ
  prp-prefix: RQ
---

# Introduction {.nostretch} 

Economist by training, previously Bank of England, currently 3rd year PhD in Trustworthy AI \@ TU Delft.

![](/www/images/qr.png){width="25%" fig-align="center"}

## Motivation {.smaller}

Why Trustworthy AI and why in Julia?

::: {.incremental}

- Opaque AI technologies have entered the public domain with far-reaching stakes.
- These technologies are here to stay, so at best, we can make them more trustworthy. 
- Julia has an edge:
  - **Transparency**: most packages are written in pure Julia.
  - **Intuitiveness**: great Lisp-like support for symbolic computing.
  - **Community**: welcoming, supportive and diverse (sort of!). 
  - **Autodiff**: [top-notch support](https://discourse.julialang.org/t/automatic-differentiation-julia-implementation-advantages/24975), which helps with common XAI approaches.

:::

## Outline

:::{.incremental}

- **Taija**: A brief overview of the [Taija](https://github.com/JuliaTrustworthyAI) ecosystem.
  - Overview, Projects, Research. 
- **Deep Dive**: A closer look at some of our core packages.
  - Counterfactual Explanations, Conformal Prediction, Laplace Redux, Joint Energy Models. 
- **The Journey**: Julia throught my PhD 
  - From "I'll try this out" to "I'll never go back".

:::

# Taija

Taija is the organization that hosts software geared towards Trustworthy Artificial Intelligence in Julia.

![](www/logo.png){.absolute bottom=-300 right=50 width="50%"}

## Overview {.smaller}

#### Core Packages

- Model Explainability ([CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl))
- Predictive Uncertainty Quantification ([ConformalPrediction.jl](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl))
- Effortless Bayesian Deep Learning ([LaplaceRedux.jl](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl))

#### Meta Packages

- Plotting ([TaijaPlotting.jl](https://github.com/JuliaTrustworthyAI/TaijaPlotting.jl))
- Datasets for testing and benchmarking ([TaijaData.jl](https://github.com/JuliaTrustworthyAI/TaijaData.jl))
- Parallelization ([TaijaParallel.jl](https://github.com/JuliaTrustworthyAI/TaijaParallel.jl))
- Interoperability with other programming languages ([TaijaInteroperability.jl](https://github.com/JuliaTrustworthyAI/TaijaInteroperability.jl))

The [TaijaBase.jl](https://github.com/JuliaTrustworthyAI/TaijaBase.jl) package provides common symbols, types and functions that are used across all or multiple Taija packages.

## Milestones {auto-animate=true}

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2023

- Presented `ConformalPrediction.jl` at JuliaCon. 
- TU Delft students work on `CounterfactualExplanations.jl` and `LaPlaceRedux.jl`.
- `CounterfactualExplanations.jl` published in JuliaCon proceedings.

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2024

- Multiple presentations at JuliaCon this summer.
- GSoC/JSoC projects on Causal Counterfactuals and Conformal Bayes.
- TU Delft students working on `TaijaInteractive.jl`.

#### 2023

- Presented `ConformalPrediction.jl` at JuliaCon. 
- TU Delft students work on `CounterfactualExplanations.jl` and `LaPlaceRedux.jl`.
- `CounterfactualExplanations.jl` published in JuliaCon proceedings.

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Research {.smaller}

Taija has been used in the following publications:

- *Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition* [@hengst2024conformal] [upcoming](https://arxiv.org/abs/2403.18973) in ACLâ€™s NAACL Findings 2024^[Experiments were run in parallel using Python's MAPIE and ConformalPrediction.jl, in order to cross-check results. Reported results were produced using MAPIE.].
- *Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals* [@altmeyer2024faithful] [published](https://ojs.aaai.org/index.php/AAAI/article/view/28956) in Proceedings of the AAAI Conference on Artificial Intelligence 2024.
- *Explaining Black-Box Models through Counterfactuals* [@altmeyer2023explaining] [published](https://proceedings.juliacon.org/papers/10.21105/jcon.00130) in JuliaCon Proceedings.
- *Endogenous Macrodynamics in Algorithmic Recourse* [@altmeyer2023endogenous] [published](https://ieeexplore.ieee.org/abstract/document/10136130) in Proceedings of the 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

# Counterfactual Explanations

`CounterfactualExplanations.jl`: A [package](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) for Counterfactual Explanations and Algorithmic Recourse in Julia.

![](www/ce_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background

$$
\begin{aligned}
\min_{\mathbf{Z}^\prime \in \mathcal{Z}^L} \{  {\text{yloss}(M_{\theta}(f(\mathbf{Z}^\prime)),\mathbf{y}^+)} + \lambda {\text{cost}(f(\mathbf{Z}^\prime)) }  \} 
\end{aligned} 
$$ 

::::{.columns}::::
:::{.column width='50%'}
**Counterfactual Explanations** (CE) explain how inputs into a model need to change for it to produce different outputs. 

ðŸ“œ @altmeyer2023explaining \@ JuliaCon 2022.
:::
:::{.column width='50%'}
![Gradient-based counterfactual search.](www/simple_ce.png){#fig-ce-intro width="60%"}
:::
::::

## Examples

- A motivating [example](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Example:-Give-Me-Some-Credit).
- A simple usage [example](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Usage-example).
- Whistle-stop tour of different generators [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/tutorials/whistle_stop/).
- Composing a custom generator using simple macros [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Example:-MNIST).
- Scaling things up with parallelization [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/tutorials/parallelization/).
- Extension for differentiable tree-based models [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/extensions/neurotree/).

## Pick your Poison

All of these counterfactuals are valid explanations for the model's prediction. 

> Which one would you pick?

![Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using *Wachter* [@wachter2017counterfactual], *Schut* [@schut2021generating] and *REVISE* [@joshi2019realistic].](www/mnist_motivation.png){#fig-cf-example width="80%"}

## ECCCos from the Black-Box {.nostretch .smaller} 

::::{.columns}::::
:::{.column width='30%'}

ðŸ“œ @altmeyer2023faithful \@ AAAI 2024

::: {.callout-tip appearance="minimal"}

## Key Idea

Use the hybrid objective of joint energy models (JEM) and a model-agnostic penalty for predictive uncertainty: Energy-Constrained ($\mathcal{E}_{\theta}$) Conformal ($\Omega$) Counterfactuals (*ECCCo*). 

:::
:::
:::{.column width='70%'}

*ECCCo* objective^[We leverage ideas from @grathwohl2020your and @stutz2022learning. See the paper and appendix for a derivation of the objective from first principles.]:

$$
\begin{aligned}
& \min_{\mathbf{Z}^\prime \in \mathcal{Z}^L} \{  {L_{\text{clf}}(f(\mathbf{Z}^\prime);M_{\theta},\mathbf{y}^+)}+ \lambda_1 {\text{cost}(f(\mathbf{Z}^\prime)) } \\
&+ \lambda_2 \mathcal{E}_{\theta}(f(\mathbf{Z}^\prime)|\mathbf{y}^+) + \lambda_3 \Omega(C_{\theta}(f(\mathbf{Z}^\prime);\alpha)) \} 
\end{aligned} 
$$

![Gradient fields and counterfactual paths for different generators.](www/poc_gradient_fields.png){#fig-poc-gradient-fields width="75%" fig-align="center"}

:::
::::

## Faithful Counterfactuals

::::{.columns}::::
:::{.column width='30%'}
![Turning a 9 into a 7. *ECCCo* applied to MLP (a), Ensemble (b), JEM (c), JEM Ensemble (d).](www/mnist_eccco.png){#fig-mnist-eccco width="75%"}
:::
:::{.column width='70%'}
*ECCCo* generates counterfactuals that

- faithfully represent model quality (@fig-mnist-eccco).
- achieve state-of-the-art plausibility (@fig-mnist-benchmark).

![Results for different generators (from 3 to 5).](www/mnist_benchmark.png){#fig-mnist-benchmark width="100%"}
:::
::::

## Ongoing Work

- JSoC 2024: *From Counterfactuals to Interventions - Recourse through Minimal Causal Interventions* with Jorge Luiz Franco and co-mentor [Moritz Schauer](https://github.com/mschauer) ([CausalInference.jl](https://github.com/mschauer/CausalInference.jl))
  - We typically make the implicit causal assumption that all features are independent.
  - If we have (imperfect) causal knowledge, we can use it to guide the search for counterfactuals.
- *What Makes Models Explainable?*: using *ECCCo* to benchmark models.
- Four master's theses all using `CounterfactualExplanations.jl` (LLM explainability, imbalanced data, recourse in practice and counterfactuals for model testing).

# Conformal Prediction

`ConformalPrediction.jl`: A [package](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl) for Predictive Uncertainty Quantification through Conformal Prediction for Machine Learning models trained in MLJ.

![](www/cp_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background

Intuitively, CP works under the premise of turning heuristic notions of uncertainty into rigorous uncertainty estimates through repeated sampling or the use of dedicated calibration data.

![Conformal Prediction in action: prediction intervals at varying coverage rates. As coverage grows, so does the width of the prediction interval.](https://raw.githubusercontent.com/pat-alt/pat-alt.github.io/main/blog/posts/conformal-regression/www/medium.gif)

## Examples

- A simple usage [example](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/#Usage-Example).
- Conformalizing an image classifier [here](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/how_to_guides/mnist/).
- How to build a conformal chatbot [here](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/how_to_guides/llm/).

![`ConformalPrediction.jl` meets `SymbolicRegression.jl`.](www/cp.gif)

## Ongoing Work

- GSoC 2024: *Add the support to Conformal Bayes to Taija* with Pasquale Caterino and co-mentor Mojtaba Farmanbar
- Support for conformal training [@stutz2022learning].
- Code refactoring to allow adding support for parallelization using `TaijaParallel.jl`.
- JuliaCon Proceedings paper (in planning).

## Interactive Tour

... if time permits at the end.

> First time here? Take a quick interactive [tour](https://juliahub.com/ui/Notebooks/juliahub/Tutorials/ConformalPrediction.jl) to see what this package can do right on [JuliaHub](https://juliahub.com/ui/Notebooks/juliahub/Tutorials/ConformalPrediction.jl){target="_blank"} (To run the notebook, hit login and then edit). 

This [`Pluto.jl`](https://github.com/fonsp/Pluto.jl){target="_blank"} ðŸŽˆ notebook won the 2nd Price in the [JuliaCon 2023 Notebook Competition](https://info.juliahub.com/pluto-notebook-winner-23).

# Laplace Redux

`LaplaceRedux.jl`: A [package](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl) for Effortless Bayesian Deep Learning through Laplace Approximation for Flux.jl neural networks.

![](www/lr_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background {.smaller}

We want BMA for neural networks underspecified by the data [@wilson2020case], 

$$
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
$$ {#eq-bma}

where $p(y|x,\theta)$ is the likelihood, $p(\theta|\mathcal{D})$ is the posterior and $\mathcal{D}$ is the training data.

::::{.columns}::::
:::{.column width='60%'}
- **Problem** ðŸ˜¢: Intractable posterior $p(\theta|\mathcal{D})$.
- **Convention** ðŸ¤”: Use MAP estimate $p(y|x,\mathcal{D}) \approx p(y|x,\hat\theta)$ with $\hat\theta=\arg\max_{\theta}p(\theta|\mathcal{D})$. 
- **Idea** ðŸ’¡: Taylor approximation at the mode amounts to multivariate Gaussian centered around MAP.
:::
:::{.column width='40%'}
![Unnormalized log-posterior and corresponding Laplace Approximation. Source: @murphy2022probabilistic.](/www/images/laplace_posterior.png)
:::
::::

## Model Assumptions {.smaller}

**Example (binary classification)**: We assume Bernoulli likelihood and a Gaussian prior for our weights ...

$$
p(y_n|\mathbf{x}_n;\mathbf{w})\sim\text{Ber}(y_n|\sigma(\mathbf{w}^T\mathbf{x}_n))
$$ {#eq-prior}

$$
p(\theta) \sim \mathcal{N} \left( \theta | \mathbf{0}, \lambda^{-1} \mathbf{I} \right)=\mathcal{N} \left( \theta | \mathbf{0}, \mathbf{H}_0^{-1} \right)
$$ {#eq-prior}

. . .

... which yields the following negative log-likelihood (or energy) function [@murphy2022probabilistic]:

$$
\psi(\theta)= - \sum_{n}^N [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \\ \frac{1}{2} (\theta-\theta_0)^T\mathbf{H}_0(\theta-\theta_0)
$$ {#eq-loss}

## Taylor Expansion {.smaller}

We have the following normalized posterior: 

$$
p(\theta|\mathcal{D}) = Z^{-1} p(\mathcal{D}|\theta)p(\theta) = Z^{-1} \exp(-\psi(\theta)) 
$$ {#eq-normalized}

with $Z = \int p(\mathcal{D}|\theta) p(\theta)$.

. . .

Then, second-order Taylor expansion of the energy function around the mode $\theta_0$ gives us the Laplace approximation [@murphy2022probabilistic]:

$$
\psi(\theta) \approxeq \psi(\hat\theta) + \frac{1}{2} (\theta-\hat\theta)^T \mathbf{H}(\theta-\hat\theta)
$$ {#eq-taylor}

. . .

Thus, we have

$$
p(\theta|\mathcal{D}) \approxeq \mathcal{N}(\theta;\hat\theta,-\mathbf{H}^{-1})
$$ {#eq-posterior}

## Hessian Approximation

- Challenge lies in computing the Hessian $\mathbf{H}$.
- We can use the Empirical Fisher or Generalized Gauss-Newton approximation [@murphy2022probabilistic].
  - Still quadratic in the number of parameters.
- For scalable solutions, we can use the diagonal approximation or the K-FAC approximation or work with a subnetwork.

## Examples

- A basic usage [example](https://juliatrustworthyai.github.io/LaplaceRedux.jl/stable/#Basic-Usage).
- A binary classification [example](https://juliatrustworthyai.github.io/LaplaceRedux.jl/stable/#Binary-Classification) with prior tuning through empirical Bayes.
- Counterfactual explanations with Laplace Redux [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/extensions/laplace_redux/).

![](www/laplace.svg)

# Joint Energy Models

`JointEnergyModels.jl`: A [package](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl?tab=readme-ov-file) for Joint Energy Models and Energy-Based Models in Julia.

![](www/jem.svg){.absolute bottom=-400 right=50 width="50%"}

## Background 

**Joint Energy Models** (JEM) are a class of energy-based models that learn a joint distribution over inputs and outputs [@grathwohl2020your].

![Training JEMs.](www/jem.png)

## Package

::::{.columns}::::
:::{.column width='50%'}
- Package used in @altmeyer2024faithful, but less mature than the other packages.
- Simple usage example can be found [here](https://juliatrustworthyai.github.io/JointEnergyModels.jl/stable/).

> Contributions welcome!
:::
:::{.column width='50%'}
![Samples generated through SGLD for JEM ensemble.](www/mnist_generated_jem.png)
:::
::::

# The Journey

## Timeline

- Background in R and Python (mostly R), a little bit of MATLAB during my master's
- First looked at Julia (`v0.6`) in 2017 during my master's but quickly abandoned it.
- During my master's in Data Science (2021) I started to get frustrated with the speed of R and Python.
  - Did some work in C++ and Rcpp, but it was too cumbersome.
- Started my PhD in Trustworthy AI in September 2021 and decided to give Julia another go.
- Haven't looked back since!

## The Good

- Julia is uniquely expressive and intuitive, making it easy to prototype and test ideas (especially in research).
- The package manager will have you writing and shipping your own packages in no time.
- Students at TU Delft have been able to pick up Julia quickly and contribute to Taija projects.
- Multiple dispatch is a game-changer for writing clean and efficient code.
- The release of Quarto in 2021 has made it easy to transition from R (Studio) to Julia.

## The Bad

- Expect to implement things from scratch (not always a bad thing!).
- Some important packages are still in development and lack contributors (e.g. `Transformers.jl`).
- I'm still sometimes puzzled by implicit imports (and I know my students are too!).
- AI research is still dominated by Python/R, so developing in Julia does not always feel impactful.

# Questions? {.nostretch} 

![](/www/images/qr.png){width="25%" fig-align="center"}

## References {.scrollable .smaller}

::: {#refs}
:::
