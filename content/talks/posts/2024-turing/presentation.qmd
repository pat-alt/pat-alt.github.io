---
title: Trustworthy AI in Julia
subtitle: The Alan Turing Institute
author: 
  - name: Patrick Altmeyer
    url: https://www.paltmeyer.com/
institute: Delft University of Technology
date: 2024-05-08
format: 
  julia-revealjs:
    theme: custom.scss
    self-contained: true
    smaller: false
    scrollable: true
    preview-links: auto
    slide-number: true
    transition: slide
    background-transition: fade
    fig-align: center
    html-math-method: mathjax
    include-in-header:
      - text: |
          <script>
          MathJax = {
            options: {
              menuOptions: {
                settings: {
                  assistiveMml: false
                }
              }
            }
          };
          </script>
revealjs-plugins:
  - pointer
crossref: 
  prp-title: RQ
  prp-prefix: RQ
---

# Introduction {.nostretch} 

Economist by training, previously Bank of England, currently 3rd year PhD in Trustworthy AI \@ TU Delft.

![](/www/images/qr.png){width="25%" fig-align="center"}

## Motivation {.smaller}

Why Trustworthy AI and why in Julia?

::: {.incremental}

- Opaque AI technologies have entered the public domain with far-reaching stakes.
- These technologies are here to stay, so at best, we can make them more trustworthy. 
- Julia has an edge:
  - **Transparency**: most packages are written in pure Julia.
  - **Intuitiveness**: great Lisp-like support for symbolic computing.
  - **Community**: welcoming, supportive and diverse (sort of!). 
  - **Autodiff**: [top-notch support](https://discourse.julialang.org/t/automatic-differentiation-julia-implementation-advantages/24975), which helps with common XAI approaches.

:::

## Outline

:::{.incremental}

- **Taija**: A brief overview of the [Taija]() ecosystem.
  - Overview, Projects, Research. 
- **Deep Dive**: A closer look at some of our core packages.
  - Counterfactual Explanations, Conformal Prediction, Laplace Redux, Joint Energy Models. 
- **The Journey**: Julia throught my PhD 
  - From "I'll try this out" to "I'll never go back".

:::

# Taija

Taija is the organization that hosts software geared towards Trustworthy Artificial Intelligence in Julia.

![](www/logo.png){.absolute bottom=-300 right=50 width="50%"}

## Overview {.smaller}

#### Core Packages

- Model Explainability ([CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl))
- Predictive Uncertainty Quantification ([ConformalPrediction.jl](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl))
- Effortless Bayesian Deep Learning ([LaplaceRedux.jl](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl))

#### Meta Packages

- Plotting ([TaijaPlotting.jl](https://github.com/JuliaTrustworthyAI/TaijaPlotting.jl))
- Datasets for testing and benchmarking ([TaijaData.jl](https://github.com/JuliaTrustworthyAI/TaijaData.jl))
- Parallelization ([TaijaParallel.jl](https://github.com/JuliaTrustworthyAI/TaijaParallel.jl))
- Interoperability with other programming languages ([TaijaInteroperability.jl](https://github.com/JuliaTrustworthyAI/TaijaInteroperability.jl))

The [TaijaBase.jl](https://github.com/JuliaTrustworthyAI/TaijaBase.jl) package provides common symbols, types and functions that are used across all or multiple Taija packages.

## Milestones {auto-animate=true}

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2023

- Presented `ConformalPrediction.jl` at JuliaCon. 
- TU Delft students work on `CounterfactualExplanations.jl` and `LaPlaceRedux.jl`.
- `CounterfactualExplanations.jl` published in JuliaCon proceedings.

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Milestones {auto-animate=true}

#### 2024

- Multiple presentations at JuliaCon this summer.
- GSoC/JSoC projects on Causal Counterfactuals and Conformal Bayes.
- TU Delft students working on `TaijaInteractive.jl`.

#### 2023

- Presented `ConformalPrediction.jl` at JuliaCon. 
- TU Delft students work on `CounterfactualExplanations.jl` and `LaPlaceRedux.jl`.
- `CounterfactualExplanations.jl` published in JuliaCon proceedings.

#### 2022

- Presented `CounterfactualExplanations.jl` and `LaplaceRedux.jl` at JuliaCon.

#### 2021

- First small-scale project in Julia on Bayesian regression.

## Research {.smaller}

Taija has been used in the following publications:

- *Conformal Intent Classification and Clarification for Fast and Accurate Intent Recognition* [@hengst2024conformal] [upcoming](https://arxiv.org/abs/2403.18973) in ACLâ€™s NAACL Findings 2024^[Experiments were run in parallel using Python's MAPIE and ConformalPrediction.jl, in order to cross-check results. Reported results were produced using MAPIE.].
- *Faithful Model Explanations through Energy-Constrained Conformal Counterfactuals* [@altmeyer2024faithful] [published](https://ojs.aaai.org/index.php/AAAI/article/view/28956) in Proceedings of the AAAI Conference on Artificial Intelligence 2024.
- *Explaining Black-Box Models through Counterfactuals* [@altmeyer2023explaining] [published](https://proceedings.juliacon.org/papers/10.21105/jcon.00130) in JuliaCon Proceedings.
- *Endogenous Macrodynamics in Algorithmic Recourse* [@altmeyer2023endogenous] [published](https://ieeexplore.ieee.org/abstract/document/10136130) in Proceedings of the 2023 IEEE Conference on Secure and Trustworthy Machine Learning (SaTML).

# Counterfactual Explanations

`CounterfactualExplanations.jl`: A [package](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl) for Counterfactual Explanations and Algorithmic Recourse in Julia.

![](www/ce_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background

$$
\begin{aligned}
\min_{\mathbf{Z}^\prime \in \mathcal{Z}^L} \{  {\text{yloss}(M_{\theta}(f(\mathbf{Z}^\prime)),\mathbf{y}^+)} + \lambda {\text{cost}(f(\mathbf{Z}^\prime)) }  \} 
\end{aligned} 
$$ 

::::{.columns}::::
:::{.column width='50%'}
**Counterfactual Explanations** (CE) explain how inputs into a model need to change for it to produce different outputs. 

ðŸ“œ @altmeyer2023explaining \@ JuliaCon 2022.
:::
:::{.column width='50%'}
![Gradient-based counterfactual search.](www/simple_ce.png){#fig-ce-intro width="60%"}
:::
::::

## Examples

- A motivating [example](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Example:-Give-Me-Some-Credit).
- A simple usage [example](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Usage-example).
- Whistle-stop tour of different generators [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/tutorials/whistle_stop/).
- Composing a custom generator using simple macros [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/#Example:-MNIST).
- Scaling things up with parallelization [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/tutorials/parallelization/).
- Extension for differentiable tree-based models [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/extensions/neurotree/).

## Pick your Poison

All of these counterfactuals are valid explanations for the model's prediction. 

> Which one would you pick?

![Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using *Wachter* [@wachter2017counterfactual], *Schut* [@schut2021generating] and *REVISE* [@joshi2019realistic].](www/mnist_motivation.png){#fig-cf-example width="80%"}

## ECCCos from the Black-Box {.nostretch .smaller} 

::::{.columns}::::
:::{.column width='30%'}

ðŸ“œ @altmeyer2023faithful \@ AAAI 2024

::: {.callout-tip appearance="minimal"}

## Key Idea

Use the hybrid objective of joint energy models (JEM) and a model-agnostic penalty for predictive uncertainty: Energy-Constrained ($\mathcal{E}_{\theta}$) Conformal ($\Omega$) Counterfactuals (*ECCCo*). 

:::
:::
:::{.column width='70%'}

*ECCCo* objective^[We leverage ideas from @grathwohl2020your and @stutz2022learning. See the paper and appendix for a derivation of the objective from first principles.]:

$$
\begin{aligned}
& \min_{\mathbf{Z}^\prime \in \mathcal{Z}^L} \{  {L_{\text{clf}}(f(\mathbf{Z}^\prime);M_{\theta},\mathbf{y}^+)}+ \lambda_1 {\text{cost}(f(\mathbf{Z}^\prime)) } \\
&+ \lambda_2 \mathcal{E}_{\theta}(f(\mathbf{Z}^\prime)|\mathbf{y}^+) + \lambda_3 \Omega(C_{\theta}(f(\mathbf{Z}^\prime);\alpha)) \} 
\end{aligned} 
$$

![Gradient fields and counterfactual paths for different generators.](www/poc_gradient_fields.png){#fig-poc-gradient-fields width="75%" fig-align="center"}

:::
::::

## Faithful Counterfactuals

::::{.columns}::::
:::{.column width='30%'}
![Turning a 9 into a 7. *ECCCo* applied to MLP (a), Ensemble (b), JEM (c), JEM Ensemble (d).](www/mnist_eccco.png){#fig-mnist-eccco width="75%"}
:::
:::{.column width='70%'}
*ECCCo* generates counterfactuals that

- faithfully represent model quality (@fig-mnist-eccco).
- achieve state-of-the-art plausibility (@fig-mnist-benchmark).

![Results for different generators (from 3 to 5).](www/mnist_benchmark.png){#fig-mnist-benchmark width="100%"}
:::
::::

# Conformal Prediction

`ConformalPrediction.jl`: A [package](https://github.com/JuliaTrustworthyAI/ConformalPrediction.jl) for Predictive Uncertainty Quantification through Conformal Prediction for Machine Learning models trained in MLJ.

![](www/cp_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background

Intuitively, CP works under the premise of turning heuristic notions of uncertainty into rigorous uncertainty estimates through repeated sampling or the use of dedicated calibration data.

![Conformal Prediction in action: prediction intervals at varying coverage rates. As coverage grows, so does the width of the prediction interval.](https://raw.githubusercontent.com/pat-alt/pat-alt.github.io/main/blog/posts/conformal-regression/www/medium.gif)

## Examples

- A simple usage [example](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/#Usage-Example).
- Conformalizing an image classifier [here](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/how_to_guides/mnist/).
- How to build a conformal chatbot [here](https://juliatrustworthyai.github.io/ConformalPrediction.jl/stable/how_to_guides/llm/).

![`ConformalPrediction.jl` meets `SymbolicRegression.jl`.](www/cp.gif)

# Laplace Redux

`LaplaceRedux.jl`: A [package](https://github.com/JuliaTrustworthyAI/LaplaceRedux.jl) for Effortless Bayesian Deep Learning through Laplace Approximation for Flux.jl neural networks.

![](www/lr_logo.png){.absolute bottom=-300 right=50 width="50%"}

## Background {.smaller}

We want BMA for neural networks, 

$$
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
$$ {#eq-bma}

where $p(y|x,\theta)$ is the likelihood, $p(\theta|\mathcal{D})$ is the posterior and $\mathcal{D}$ is the training data.

::::{.columns}::::
:::{.column width='50%'}
- **Problem** ðŸ˜¢: Intractable posterior $p(\theta|\mathcal{D})$.
- **Idea** ðŸ’¡: Taylor approximation at the mode.
:::
:::{.column width='50%'}
![Unnormalized log-posterior and corresponding Laplace Approximation. Source: @murphy2022probabilistic.](/www/images/laplace_posterior.png)
:::
::::

## Examples

- A basic usage [example](https://juliatrustworthyai.github.io/LaplaceRedux.jl/stable/#Basic-Usage).
- A binary classification [example](https://juliatrustworthyai.github.io/LaplaceRedux.jl/stable/#Binary-Classification) with prior tuning through empirical Bayes.
- Counterfactual explanations with Laplace Redux [here](https://juliatrustworthyai.github.io/CounterfactualExplanations.jl/stable/extensions/laplace_redux/).

![](www/laplace.svg)

# Joint Energy Models

`JointEnergyModels.jl`: A [package](https://github.com/JuliaTrustworthyAI/JointEnergyModels.jl?tab=readme-ov-file) for Joint Energy Models and Energy-Based Models in Julia.

![](www/jem.svg){.absolute bottom=-400 right=50 width="50%"}

## Background and Examples

- **Joint Energy Models** (JEM) are a class of energy-based models that learn a joint distribution over inputs and outputs [@grathwohl2020your].
- Package used in @altmeyer2024faithful, but less mature than the other packages.
- Simple usage example can be found [here](https://juliatrustworthyai.github.io/JointEnergyModels.jl/stable/).

> Contributions welcome!

# The Journey

## The Good

- Julia is uniquely expressive and intuitive, making it easy to prototype and test ideas (especially in research).
- The package manager will have you writing and shipping your own packages in no time.
- Students at TU Delft have been able to pick up Julia quickly and contribute to Taija projects.
- Multiple dispatch is a game-changer for writing clean and efficient code.
- The release of Quarto in 2021 has made it easy to transition from R (Studio) to Julia.

## The Bad

- Expect to implement things from scratch (not always a bad thing!).
- Some important packages are still in development and lack contributors (e.g. `Transformers.jl`).
- I'm still sometimes puzzled by implicit imports (and I know my students are too!).
- AI research is still dominated by Python/R, so developing in Julia does not always feel impactful.

# Questions? {.nostretch} 

![](/www/images/qr.png){width="25%" fig-align="center"}

## References {.scrollable .smaller}

::: {#refs}
:::
