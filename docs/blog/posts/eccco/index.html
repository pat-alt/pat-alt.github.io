<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick Altmeyer">
<meta name="dcterms.date" content="2024-02-08">
<meta name="description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">

<title>Patrick Altmeyer - ECCCos from the Black Box</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../..//www/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BEEZ30787D"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BEEZ30787D', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Patrick Altmeyer - ECCCos from the Black Box">
<meta property="og:description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">
<meta property="og:image" content="https://www.paltmeyer.com/blog/blog/posts/eccco/www/intro.gif">
<meta property="og:site-name" content="Patrick Altmeyer">
<meta name="twitter:title" content="Patrick Altmeyer - ECCCos from the Black Box">
<meta name="twitter:description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">
<meta name="twitter:image" content="https://www.paltmeyer.com/blog/blog/posts/eccco/www/intro.gif">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../www/icon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Patrick Altmeyer</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/publications/index.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/talks/index.html" rel="" target="">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/software.html" rel="" target="">
 <span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/paltmey" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#pick-your-poison" id="toc-pick-your-poison" class="nav-link active" data-scroll-target="#pick-your-poison">Pick your Poison</a></li>
  <li><a href="#faithful-first-plausible-second" id="toc-faithful-first-plausible-second" class="nav-link" data-scroll-target="#faithful-first-plausible-second">Faithful First, Plausible Second</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">ECCCos from the Black Box</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</p>
  <div class="quarto-categories">
    <div class="quarto-category">counterfactuals</div>
    <div class="quarto-category">explainable AI</div>
    <div class="quarto-category">algorithmic recourse</div>
    <div class="quarto-category">Julia</div>
  </div>
  </div>

<div>
  <div class="description">
    <em>ECCCo</em> is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a> </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.tudelft.nl/en/">
            Delft University of Technology
            </a>
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 8, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<div class="intro-gif">
<figure class="figure">
<img src="www/intro.gif" class="figure-img">
<figcaption class="figure-caption">
Photo by <a href="https://unsplash.com/@jake_oates2000?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Jake Oates</a> on <a href="https://unsplash.com/photos/steel-wool-photography-MovsEr-Bgts?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>
</figcaption>
</figure>
</div>
<p>Counterfactual explanations offer an intuitive and straightforward way to explain opaque machine learning (ML) models. They work under the premise of perturbing inputs to achieve a desired change in the predicted output. There are typically many ways to achieve this, in other words, many different counterfactuals may yield the same desired outcome. A key challenge for researchers has therefore been to, firstly, define certain desirable characteristics of counterfactual explanations and, secondly, come up with efficient ways to achieve them.</p>
<p>One of the most important and studied characteristics of counterfactual explanations is <em>plausibility</em>. Broadly speaking, counterfactuals are considered plausible if they are indistinguishable from actual observed data in the target domain. Plausibility is positively associated with actionability, robustness <span class="citation" data-cites="artelt2021evaluating">(<a href="#ref-artelt2021evaluating" role="doc-biblioref">Artelt et al. 2021</a>)</span> and causal validity <span class="citation" data-cites="mahajan2019preserving">(<a href="#ref-mahajan2019preserving" role="doc-biblioref">Mahajan, Tan, and Sharma 2020</a>)</span>. To achieve plausibility, many existing approaches rely on surrogate models. This is straightforward but it also convolutes things further: it essentially reallocates the task of learning plausibile explanations for the data from the model itself to the surrogate.</p>
<p>In our AAAI 2024 paper, <a href="https://arxiv.org/abs/2312.10648">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</a> (<em>ECCCo</em>), we argue that in our pursuit of plausible explanations, we should not forget that ultimately we are in the business of explaining models. We propose that we should not only look for explanations that please us but rather focus on generating counterfactuals that faithfully explain model behavior. It turns out that we can achieve both faithfulness and plausibility relying solely on the model itself by leveraging recent advances in energy-based modelling and conformal prediction. We support this claim through extensive empirical studies and believe that <em>ECCCo</em> opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is a companion post to our recent AAAI 2024 paper co-authored with <a href="https://nl.linkedin.com/in/mfarmanbar">Mojtaba Farmanbar</a>, <a href="https://avandeursen.com/about/">Arie van Deursen</a> and <a href="https://www.cynthialiem.com/">Cynthia C. S. Liem</a>. The paper is a more formal and detailed treatment of the topic and is available <a href="https://arxiv.org/abs/2312.10648">here</a>.</p>
</div>
</div>
<section id="pick-your-poison" class="level2">
<h2 class="anchored" data-anchor-id="pick-your-poison">Pick your Poison</h2>
<p>There are two major debates in the field of Explainable AI (XAI). The first one centers around the role of explainability in AI: do we even need explanations and if so, why? Some have argued that we need not care about explaining models as long as they produce reliable outcomes <span class="citation" data-cites="robbins2019misdirected">(<a href="#ref-robbins2019misdirected" role="doc-biblioref">Robbins 2019</a>)</span>. Humans have not shied away from this in other domains either: doctors, for example, have prescribed aspirin for decades without understanding why it is a reliable drug for pain relief <span class="citation" data-cites="london2019artificial">(<a href="#ref-london2019artificial" role="doc-biblioref">London 2019</a>)</span>. While this reasoning may rightfully apply in some cases, experiments conducted by my colleagues at TU Delft have shown that humans do not make better decisions if aided by a reliable and yet opaque model <span class="citation" data-cites="he2023how">(<a href="#ref-he2023how" role="doc-biblioref">He, Buijsman, and Gadiraju 2023</a>)</span>. In these studies, subjects tended to ignore the AI model indicating that they did not trust its decisions. Beyond this, explainability comes with numerous advantages such as accountability, control, contestability and the potential for uncovering causal relationships.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intermezzo: Why Bother?
</div>
</div>
<div class="callout-body-container callout-body">
<blockquote class="blockquote">
<p>“If we can blindly rely on the decisions made by an AI, why should we even bother to come up with explanations?”</p>
</blockquote>
<p>I must confess I had never even seriously considered this as an option until attending <a href="https://www.tudelft.nl/staff/s.n.r.buijsman/">Stefan Buijsman</a>’s recent talk at a Delft Design for Values <a href="https://www.tudelft.nl/evenementen/2024/delft-ai/workshop-navigating-the-interplay-of-explainability-and-privacy-in-ai">workshop</a>. It is an interesting critique of recent efforts towards XAI, especially considering that the field has so far struggled to produce satisfactory and robust results with meaningful real-world impact. Much like <span class="citation" data-cites="he2023how">He, Buijsman, and Gadiraju (<a href="#ref-he2023how" role="doc-biblioref">2023</a>)</span> find that blind reliance on reliable AI models does not seem to work in practice, numerous other studies have shown that explanations for AI models either fail to help users or even mislead them <span class="citation" data-cites="mittelstadt2019explaining">Lakkaraju and Bastani (<a href="#ref-lakkaraju2020how" role="doc-biblioref">2020</a>)</span>.</p>
<p>So, have all efforts toward explainable AI been in vain? Should we simply stop explaining black-box models altogether as proposed by <span class="citation" data-cites="rudin2019stop">Rudin (<a href="#ref-rudin2019stop" role="doc-biblioref">2019</a>)</span>? Having worked in this field for a little over 2 years now, I have personally grown more and more skeptical of certain trends. In particular, I think that the community has focused too much on finding explanations that please us independent of the model itself. This is a bit like applying a band-aid to a wound without first cleaning it. At best, plausible explanations for untrustworthy models provide a false sense of security. At worst, they can be used to manipulate and deceive. The AAAI paper presented in this post is very much born out of skepticism about this approach.</p>
<p>Nonetheless, I do not think all hope is lost for XAI. I strongly believe that there is a need for algorithmic recourse as long as we continue to deploy black-box models for automated decision-making. While I am fully supportive of the idea that we should always strive to use models that are as interpretable as possible, I also think that there are cases where this is not feasible or it is simply too convenient to use a black-box model instead. The aspirin example mentioned above is a striking example of this. But it is easy enough to stretch that example further to illustrate why explainability is important. What if the use of aspirin was prohibited for a small minority of people and there was a reliable, opaque model to decide who belonged to that group? If you were part of that group, would you not want to know why? Why should you have to endure headaches for the rest of your life while others do not?</p>
<p>In summary, I think that—like it or not—we do need to bother.</p>
</div>
</div>
<p>The second major debate is about what constitutes a good explanation, because, crucially, explanations are not unique: was your headache cured by the aspirin you took before going to sleep or sleep itself? Or a combination of both? This multiplicity of explanations arises almost naturally in the context of counterfactual explanations. Unless the combination of input features for which the model predicts the target class or value is unique, there is always more than one possible explanation. As an illustrative example, consider the counterfactuals presented in <a href="#fig-cf-example">Figure&nbsp;1</a>. All of these are valid explanations for turning a ‘nine’ into ‘seven’ according to the underlying classifier (a simple multi-layer perceptron). They are all valid in the sense the model predicts the target label with high probability in each case. The troubling part is that even though all of the generated counterfactuals provide valid explanations for why the model predicts ‘seven’ instead of ‘nine’, they all look very different.</p>
<p>So, which explanations do we trust most? Which one would you choose to present to an audience to explain how the classifier decides which digit to predict? Arguably, the counterfactual on the far right looks most like a ‘seven’, so I am willing to bet that most people would simply choose that one. It is valid after all and it looks plausible, while the other two counterfactuals might just lead to awkward questions from particularly some of the more interrogative audience members. In any case, I mentioned earlier that more plausible explanations tend to also be more actionable and robust, so this seems fair game. The counterfactual produced by <em>REVISE</em> <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span> is the poison we will pick—dump the rest and move on<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. Plausibility is all we need!</p>
<p>I am exaggerating but I do think that as a community of researchers studying counterfactual explanations, we have become so absorbed by the pursuit of a few desiderata that we have forgotten that ultimately we are in the business of explaining models. Our primary mandate is to design tools that help us understand why models predict certain outcomes. How useful is a plausible, actionable, sparse, causally valid explanation in gaining that understanding, if there exist a whole bunch of other valid explanations that do not meet these desiderata? Have we significantly improved our understanding of the underlying classifier in <a href="#fig-cf-example">Figure&nbsp;1</a> and therefore established a higher degree of trust in the model, simply because we have found a plausible counterfactual?</p>
<p>In my mind, we most certainly have not. I would argue that the existence of a valid and plausible explanation merely serves to reassure us that the model is not entirely ignorant about meaningful representations in the data. But as long as entirely implausible counterfactuals are also valid according to the model, selectively relying only on the subset of plausible counterfactuals may lead to a wrong sense of trust in untrustworthy models. That is why in our paper we argue that explanations should be faithful first, and plausible second.</p>
<div id="fig-cf-example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/mnist_motivation.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using <em>Wachter</em> <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>, <em>Schut</em> <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span> and <em>REVISE</em> <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span>.</figcaption>
</figure>
</div>
</section>
<section id="faithful-first-plausible-second" class="level2">
<h2 class="anchored" data-anchor-id="faithful-first-plausible-second">Faithful First, Plausible Second</h2>
</section>
<section id="references" class="level2">



<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-artelt2021evaluating" class="csl-entry" role="listitem">
Artelt, André, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, and Barbara Hammer. 2021. <span>“Evaluating Robustness of Counterfactual Explanations.”</span> In <em>2021 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 01–09. IEEE.
</div>
<div id="ref-he2023how" class="csl-entry" role="listitem">
He, Gaole, Stefan Buijsman, and Ujwal Gadiraju. 2023. <span>“How Stated Accuracy of an AI System and Analogies to Explain Accuracy Affect Human Reliance on the System.”</span> <em>Proc. ACM Hum.-Comput. Interact.</em> 7 (CSCW2). <a href="https://doi.org/10.1145/3610067">https://doi.org/10.1145/3610067</a>.
</div>
<div id="ref-joshi2019realistic" class="csl-entry" role="listitem">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>“Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.”</span> <a href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.
</div>
<div id="ref-lakkaraju2020how" class="csl-entry" role="listitem">
Lakkaraju, Himabindu, and Osbert Bastani. 2020. <span>“" <span>How Do I Fool You</span>?" <span>Manipulating User Trust</span> via <span>Misleading Black Box Explanations</span>.”</span> In <em>Proceedings of the <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 79–85.
</div>
<div id="ref-london2019artificial" class="csl-entry" role="listitem">
London, Alex John. 2019. <span>“Artificial Intelligence and Black-Box Medical Decisions: Accuracy Versus Explainability.”</span> <em>Hastings Center Report</em> 49 (1): 15–21.
</div>
<div id="ref-mahajan2019preserving" class="csl-entry" role="listitem">
Mahajan, Divyat, Chenhao Tan, and Amit Sharma. 2020. <span>“Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.”</span> <a href="https://arxiv.org/abs/1912.03277">https://arxiv.org/abs/1912.03277</a>.
</div>
<div id="ref-mittelstadt2019explaining" class="csl-entry" role="listitem">
Mittelstadt, Brent, Chris Russell, and Sandra Wachter. 2019. <span>“Explaining Explanations in <span>AI</span>.”</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 279–88. <a href="https://doi.org/10.1145/3287560.3287574">https://doi.org/10.1145/3287560.3287574</a>.
</div>
<div id="ref-robbins2019misdirected" class="csl-entry" role="listitem">
Robbins, Scott. 2019. <span>“A Misdirected Principle with a Catch: Explicability for AI.”</span> <em>Minds and Machines</em> 29 (4): 495–514.
</div>
<div id="ref-rudin2019stop" class="csl-entry" role="listitem">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div>
<div id="ref-schut2021generating" class="csl-entry" role="listitem">
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. <span>“Generating <span>Interpretable Counterfactual Explanations By Implicit Minimisation</span> of <span>Epistemic</span> and <span>Aleatoric Uncertainties</span>.”</span> In <em>International <span>Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, 1756–64. <span>PMLR</span>.
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>.”</span> <em>Harv. JL &amp; Tech.</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Considering how much I have cited <span class="citation" data-cites="joshi2019realistic">Joshi et al. (<a href="#ref-joshi2019realistic" role="doc-biblioref">2019</a>)</span> in the past I think it should go without saying that I very much like this paper, despite taking a critical stance on it here.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{altmeyer2024,
  author = {Altmeyer, Patrick},
  title = {ECCCos from the {Black} {Box}},
  date = {2024-02-08},
  url = {https://www.paltmeyer.com/blog//blog/posts/eccco},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-altmeyer2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Altmeyer, Patrick. 2024. <span>“ECCCos from the Black Box.”</span>
February 8, 2024. <a href="https://www.paltmeyer.com/blog//blog/posts/eccco">https://www.paltmeyer.com/blog//blog/posts/eccco</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="pat-alt/pat-alt.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">title:</span><span class="co"> ECCCos from the Black Box</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="an">subtitle:</span><span class="co"> Faithful Model Explanations through Energy-Based Conformal Counterfactuals</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="an">date:</span><span class="co"> '2024-02-08'</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="an">categories:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  - counterfactuals</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  - explainable AI</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">  - algorithmic recourse</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">  - Julia</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="an">description:</span><span class="co"> &gt;-</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">  *ECCCo* is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="an">image:</span><span class="co"> www/intro.gif</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="an">jupyter:</span><span class="co"> julia-1.10</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb1-15"><a href="#cb1-15"></a><span class="co">---</span></span>
<span id="cb1-16"><a href="#cb1-16"></a></span>
<span id="cb1-19"><a href="#cb1-19"></a><span class="in">```{julia}</span></span>
<span id="cb1-20"><a href="#cb1-20"></a><span class="co">#| echo: false</span></span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>include(<span class="ot">"</span><span class="wa">$(</span><span class="st">pwd())/blog/posts/eccco/src/setup.jl</span><span class="ot">"</span>)</span>
<span id="cb1-23"><a href="#cb1-23"></a><span class="in">```</span></span>
<span id="cb1-24"><a href="#cb1-24"></a></span>
<span id="cb1-25"><a href="#cb1-25"></a><span class="kw">&lt;div</span> <span class="er">class</span><span class="ot">=</span><span class="st">"intro-gif"</span><span class="kw">&gt;</span></span>
<span id="cb1-26"><a href="#cb1-26"></a>  <span class="kw">&lt;figure&gt;</span></span>
<span id="cb1-27"><a href="#cb1-27"></a>    <span class="kw">&lt;img</span> <span class="er">src</span><span class="ot">=</span><span class="st">"www/intro.gif"</span><span class="kw">&gt;</span></span>
<span id="cb1-28"><a href="#cb1-28"></a>    <span class="kw">&lt;figcaption&gt;</span>Photo by <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"https://unsplash.com/@jake_oates2000?utm_content=creditCopyText</span><span class="er">&amp;</span><span class="st">utm_medium=referral</span><span class="er">&amp;</span><span class="st">utm_source=unsplash"</span><span class="kw">&gt;</span>Jake Oates<span class="kw">&lt;/a&gt;</span> on <span class="kw">&lt;a</span> <span class="er">href</span><span class="ot">=</span><span class="st">"https://unsplash.com/photos/steel-wool-photography-MovsEr-Bgts?utm_content=creditCopyText</span><span class="er">&amp;</span><span class="st">utm_medium=referral</span><span class="er">&amp;</span><span class="st">utm_source=unsplash"</span><span class="kw">&gt;</span>Unsplash<span class="kw">&lt;/a&gt;&lt;/figcaption&gt;</span></span>
<span id="cb1-29"><a href="#cb1-29"></a>  <span class="kw">&lt;/figure&gt;</span></span>
<span id="cb1-30"><a href="#cb1-30"></a><span class="kw">&lt;/div&gt;</span> </span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>Counterfactual explanations offer an intuitive and straightforward way to explain opaque machine learning (ML) models. They work under the premise of perturbing inputs to achieve a desired change in the predicted output. There are typically many ways to achieve this, in other words, many different counterfactuals may yield the same desired outcome. A key challenge for researchers has therefore been to, firstly, define certain desirable characteristics of counterfactual explanations and, secondly, come up with efficient ways to achieve them.</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a>One of the most important and studied characteristics of counterfactual explanations is *plausibility*. Broadly speaking, counterfactuals are considered plausible if they are indistinguishable from actual observed data in the target domain. Plausibility is positively associated with actionability, robustness <span class="co">[</span><span class="ot">@artelt2021evaluating</span><span class="co">]</span> and causal validity <span class="co">[</span><span class="ot">@mahajan2019preserving</span><span class="co">]</span>. To achieve plausibility, many existing approaches rely on surrogate models. This is straightforward but it also convolutes things further: it essentially reallocates the task of learning plausibile explanations for the data from the model itself to the surrogate.</span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>In our AAAI 2024 paper, <span class="co">[</span><span class="ot">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</span><span class="co">](https://arxiv.org/abs/2312.10648)</span> (*ECCCo*), we argue that in our pursuit of plausible explanations, we should not forget that ultimately we are in the business of explaining models. We propose that we should not only look for explanations that please us but rather focus on generating counterfactuals that faithfully explain model behavior. It turns out that we can achieve both faithfulness and plausibility relying solely on the model itself by leveraging recent advances in energy-based modelling and conformal prediction.</span>
<span id="cb1-37"><a href="#cb1-37"></a>We support this claim through extensive empirical studies and believe that *ECCCo* opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</span>
<span id="cb1-38"><a href="#cb1-38"></a></span>
<span id="cb1-39"><a href="#cb1-39"></a>::: {.callout-note}</span>
<span id="cb1-40"><a href="#cb1-40"></a></span>
<span id="cb1-41"><a href="#cb1-41"></a>This is a companion post to our recent AAAI 2024 paper co-authored with <span class="co">[</span><span class="ot">Mojtaba Farmanbar</span><span class="co">](https://nl.linkedin.com/in/mfarmanbar)</span>, <span class="co">[</span><span class="ot">Arie van Deursen</span><span class="co">](https://avandeursen.com/about/)</span> and <span class="co">[</span><span class="ot">Cynthia C. S. Liem</span><span class="co">](https://www.cynthialiem.com/)</span>. The paper is a more formal and detailed treatment of the topic and is available <span class="co">[</span><span class="ot">here</span><span class="co">](https://arxiv.org/abs/2312.10648)</span>.</span>
<span id="cb1-42"><a href="#cb1-42"></a></span>
<span id="cb1-43"><a href="#cb1-43"></a>:::</span>
<span id="cb1-44"><a href="#cb1-44"></a></span>
<span id="cb1-45"><a href="#cb1-45"></a><span class="fu">## Pick your Poison</span></span>
<span id="cb1-46"><a href="#cb1-46"></a></span>
<span id="cb1-47"><a href="#cb1-47"></a>There are two major debates in the field of Explainable AI (XAI). The first one centers around the role of explainability in AI: do we even need explanations and if so, why? Some have argued that we need not care about explaining models as long as they produce reliable outcomes <span class="co">[</span><span class="ot">@robbins2019misdirected</span><span class="co">]</span>. Humans have not shied away from this in other domains either: doctors, for example, have prescribed aspirin for decades without understanding why it is a reliable drug for pain relief <span class="co">[</span><span class="ot">@london2019artificial</span><span class="co">]</span>. While this reasoning may rightfully apply in some cases, experiments conducted by my colleagues at TU Delft have shown that humans do not make better decisions if aided by a reliable and yet opaque model <span class="co">[</span><span class="ot">@he2023how</span><span class="co">]</span>. In these studies, subjects tended to ignore the AI model indicating that they did not trust its decisions. Beyond this, explainability comes with numerous advantages such as accountability, control, contestability and the potential for uncovering causal relationships. </span>
<span id="cb1-48"><a href="#cb1-48"></a></span>
<span id="cb1-49"><a href="#cb1-49"></a>::: {.callout-note}</span>
<span id="cb1-50"><a href="#cb1-50"></a></span>
<span id="cb1-51"><a href="#cb1-51"></a><span class="fu">## Intermezzo: Why Bother?</span></span>
<span id="cb1-52"><a href="#cb1-52"></a></span>
<span id="cb1-53"><a href="#cb1-53"></a><span class="at">&gt; "If we can blindly rely on the decisions made by an AI, why should we even bother to come up with explanations?" </span></span>
<span id="cb1-54"><a href="#cb1-54"></a></span>
<span id="cb1-55"><a href="#cb1-55"></a>I must confess I had never even seriously considered this as an option until attending <span class="co">[</span><span class="ot">Stefan Buijsman</span><span class="co">](https://www.tudelft.nl/staff/s.n.r.buijsman/)</span>'s recent talk at a Delft Design for Values <span class="co">[</span><span class="ot">workshop</span><span class="co">](https://www.tudelft.nl/evenementen/2024/delft-ai/workshop-navigating-the-interplay-of-explainability-and-privacy-in-ai)</span>. It is an interesting critique of recent efforts towards XAI, especially considering that the field has so far struggled to produce satisfactory and robust results with meaningful real-world impact. Much like @he2023how find that blind reliance on reliable AI models does not seem to work in practice, numerous other studies have shown that explanations for AI models either fail to help users or even mislead them <span class="co">[</span><span class="ot">@mittelstadt2019explaining,@alufaisan2021does,@lakkaraju2020how</span><span class="co">]</span>.</span>
<span id="cb1-56"><a href="#cb1-56"></a></span>
<span id="cb1-57"><a href="#cb1-57"></a>So, have all efforts toward explainable AI been in vain? Should we simply stop explaining black-box models altogether as proposed by @rudin2019stop? Having worked in this field for a little over 2 years now, I have personally grown more and more skeptical of certain trends. In particular, I think that the community has focused too much on finding explanations that please us independent of the model itself. This is a bit like applying a band-aid to a wound without first cleaning it. At best, plausible explanations for untrustworthy models provide a false sense of security. At worst, they can be used to manipulate and deceive. The AAAI paper presented in this post is very much born out of skepticism about this approach.</span>
<span id="cb1-58"><a href="#cb1-58"></a></span>
<span id="cb1-59"><a href="#cb1-59"></a>Nonetheless, I do not think all hope is lost for XAI. I strongly believe that there is a need for algorithmic recourse as long as we continue to deploy black-box models for automated decision-making. While I am fully supportive of the idea that we should always strive to use models that are as interpretable as possible, I also think that there are cases where this is not feasible or it is simply too convenient to use a black-box model instead. The aspirin example mentioned above is a striking example of this. But it is easy enough to stretch that example further to illustrate why explainability is important. What if the use of aspirin was prohibited for a small minority of people and there was a reliable, opaque model to decide who belonged to that group? If you were part of that group, would you not want to know why? Why should you have to endure headaches for the rest of your life while others do not?</span>
<span id="cb1-60"><a href="#cb1-60"></a></span>
<span id="cb1-61"><a href="#cb1-61"></a>In summary, I think that---like it or not---we do need to bother. </span>
<span id="cb1-62"><a href="#cb1-62"></a></span>
<span id="cb1-63"><a href="#cb1-63"></a>:::</span>
<span id="cb1-64"><a href="#cb1-64"></a></span>
<span id="cb1-65"><a href="#cb1-65"></a>The second major debate is about what constitutes a good explanation, because, crucially, explanations are not unique: was your headache cured by the aspirin you took before going to sleep or sleep itself? Or a combination of both? This multiplicity of explanations arises almost naturally in the context of counterfactual explanations. Unless the combination of input features for which the model predicts the target class or value is unique, there is always more than one possible explanation. As an illustrative example, consider the counterfactuals presented in @fig-cf-example. All of these are valid explanations for turning a 'nine' into 'seven' according to the underlying classifier (a simple multi-layer perceptron). They are all valid in the sense the model predicts the target label with high probability in each case. The troubling part is that even though all of the generated counterfactuals provide valid explanations for why the model predicts 'seven' instead of 'nine', they all look very different. </span>
<span id="cb1-66"><a href="#cb1-66"></a></span>
<span id="cb1-67"><a href="#cb1-67"></a>So, which explanations do we trust most? Which one would you choose to present to an audience to explain how the classifier decides which digit to predict? Arguably, the counterfactual on the far right looks most like a 'seven', so I am willing to bet that most people would simply choose that one. It is valid after all and it looks plausible, while the other two counterfactuals might just lead to awkward questions from particularly some of the more interrogative audience members. In any case, I mentioned earlier that more plausible explanations tend to also be more actionable and robust, so this seems fair game. The counterfactual produced by *REVISE* <span class="co">[</span><span class="ot">@joshi2019realistic</span><span class="co">]</span> is the poison we will pick---dump the rest and move on^<span class="co">[</span><span class="ot">Considering how much I have cited @joshi2019realistic in the past I think it should go without saying that I very much like this paper, despite taking a critical stance on it here.</span><span class="co">]</span>. Plausibility is all we need!</span>
<span id="cb1-68"><a href="#cb1-68"></a></span>
<span id="cb1-69"><a href="#cb1-69"></a>I am exaggerating but I do think that as a community of researchers studying counterfactual explanations, we have become so absorbed by the pursuit of a few desiderata that we have forgotten that ultimately we are in the business of explaining models. Our primary mandate is to design tools that help us understand why models predict certain outcomes. How useful is a plausible, actionable, sparse, causally valid explanation in gaining that understanding, if there exist a whole bunch of other valid explanations that do not meet these desiderata? Have we significantly improved our understanding of the underlying classifier in @fig-cf-example and therefore established a higher degree of trust in the model, simply because we have found a plausible counterfactual? </span>
<span id="cb1-70"><a href="#cb1-70"></a></span>
<span id="cb1-71"><a href="#cb1-71"></a>In my mind, we most certainly have not. I would argue that the existence of a valid and plausible explanation merely serves to reassure us that the model is not entirely ignorant about meaningful representations in the data. But as long as entirely implausible counterfactuals are also valid according to the model, selectively relying only on the subset of plausible counterfactuals may lead to a wrong sense of trust in untrustworthy models. That is why in our paper we argue that explanations should be faithful first, and plausible second.</span>
<span id="cb1-72"><a href="#cb1-72"></a></span>
<span id="cb1-73"><a href="#cb1-73"></a>!<span class="co">[</span><span class="ot">Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using *Wachter* [@wachter2017counterfactual], *Schut* [@schut2021generating] and *REVISE* [@joshi2019realistic].</span><span class="co">](www/mnist_motivation.png)</span>{#fig-cf-example width="80%"}</span>
<span id="cb1-74"><a href="#cb1-74"></a></span>
<span id="cb1-75"><a href="#cb1-75"></a><span class="fu">## Faithful First, Plausible Second</span></span>
<span id="cb1-76"><a href="#cb1-76"></a></span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a></span>
<span id="cb1-79"><a href="#cb1-79"></a></span>
<span id="cb1-80"><a href="#cb1-80"></a><span class="fu">## References</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© 2023, Patrick Altmeyer</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.paltmeyer.com/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/paltmey">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/\@patrick.altmeyer">
      <i class="bi bi-medium" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>