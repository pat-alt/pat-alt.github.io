<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.450">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick Altmeyer">
<meta name="dcterms.date" content="2024-02-08">
<meta name="description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">

<title>Patrick Altmeyer - ECCCos from the Black Box</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../..//www/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BEEZ30787D"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BEEZ30787D', { 'anonymize_ip': true});
</script>


<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Patrick Altmeyer - ECCCos from the Black Box">
<meta property="og:description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">
<meta property="og:image" content="https://www.paltmeyer.com/blog/blog/posts/eccco/www/density_model.png">
<meta property="og:site-name" content="Patrick Altmeyer">
<meta property="og:image:height" content="1499">
<meta property="og:image:width" content="1499">
<meta name="twitter:title" content="Patrick Altmeyer - ECCCos from the Black Box">
<meta name="twitter:description" content="ECCCo is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.">
<meta name="twitter:image" content="https://www.paltmeyer.com/blog/blog/posts/eccco/www/density_model.png">
<meta name="twitter:image-height" content="1499">
<meta name="twitter:image-width" content="1499">
<meta name="twitter:card" content="summary_large_image">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../www/icon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Patrick Altmeyer</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html" rel="" target="">
 <span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/publications/index.html" rel="" target="">
 <span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/talks/index.html" rel="" target="">
 <span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html" rel="" target="">
 <span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/software.html" rel="" target="">
 <span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/about/index.html" rel="" target="">
 <span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog/index.xml" rel="" target=""><i class="bi bi-rss" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/paltmey" rel="" target=""><i class="bi bi-twitter" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#sec-poison" id="toc-sec-poison" class="nav-link active" data-scroll-target="#sec-poison">Pick your Poison</a></li>
  <li><a href="#faithful-first-plausible-second" id="toc-faithful-first-plausible-second" class="nav-link" data-scroll-target="#faithful-first-plausible-second">Faithful First, Plausible Second</a>
  <ul class="collapse">
  <li><a href="#faithful-counterfactuals" id="toc-faithful-counterfactuals" class="nav-link" data-scroll-target="#faithful-counterfactuals">Faithful Counterfactuals</a></li>
  <li><a href="#plausible-counterfactuals" id="toc-plausible-counterfactuals" class="nav-link" data-scroll-target="#plausible-counterfactuals">Plausible Counterfactuals</a></li>
  </ul></li>
  <li><a href="#our-approach-eccco" id="toc-our-approach-eccco" class="nav-link" data-scroll-target="#our-approach-eccco">Our approach: <em>ECCCo</em></a></li>
  <li><a href="#results" id="toc-results" class="nav-link" data-scroll-target="#results">Results</a></li>
  <li><a href="#conclusion" id="toc-conclusion" class="nav-link" data-scroll-target="#conclusion">Conclusion</a></li>
  <li><a href="#software" id="toc-software" class="nav-link" data-scroll-target="#software">Software</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><div><i class="bi bi-github"></i></div><div class="action-links"><p><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action">Report an issue</a></p></div></div></nav>
    </div>
<!-- main -->
<main class="content page-columns page-full" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">ECCCos from the Black Box</h1><button type="button" class="btn code-tools-button" id="quarto-code-tools-source"><i class="bi"></i> Code</button></div></div>
<p class="subtitle lead">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</p>
  <div class="quarto-categories">
    <div class="quarto-category">counterfactuals</div>
    <div class="quarto-category">explainable AI</div>
    <div class="quarto-category">algorithmic recourse</div>
    <div class="quarto-category">Julia</div>
  </div>
  </div>

<div>
  <div class="description">
    <em>ECCCo</em> is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a> </p>
  </div>
    <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.tudelft.nl/en/">
            Delft University of Technology
            </a>
          </p>
      </div>
    </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 8, 2024</p>
    </div>
  </div>
  
    
  </div>
  

</header>

<p>Counterfactual explanations offer an intuitive and straightforward way to explain opaque machine learning (ML) models. They work under the premise of perturbing inputs to achieve a desired change in the predicted output. There are typically many ways to achieve this, in other words, many different counterfactuals may yield the same desired outcome. A key challenge for researchers has therefore been to, firstly, define certain desirable characteristics of counterfactual explanations and, secondly, come up with efficient ways to achieve them.</p>
<p>One of the most important and studied characteristics of counterfactual explanations is ‘plausibility’: explanations should look realistic to humans. Plausibility is positively associated with actionability, robustness <span class="citation" data-cites="artelt2021evaluating">(<a href="#ref-artelt2021evaluating" role="doc-biblioref">Artelt et al. 2021</a>)</span> and causal validity <span class="citation" data-cites="mahajan2019preserving">(<a href="#ref-mahajan2019preserving" role="doc-biblioref">Mahajan, Tan, and Sharma 2020</a>)</span>. To achieve plausibility, many existing approaches rely on surrogate models. This is straightforward but it also convolutes things further: it essentially reallocates the task of learning plausible explanations for the data from the model itself to the surrogate.</p>
<p>In our AAAI 2024 paper, <a href="https://arxiv.org/abs/2312.10648">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</a> (<em>ECCCo</em>), we propose that we should not only look for explanations that please us but rather focus on generating counterfactuals that faithfully explain model behavior. It turns out that we can achieve both faithfulness and plausibility by relying solely on the model itself, leveraging recent advances in energy-based modelling and conformal prediction. We support this claim through extensive empirical studies and believe that <em>ECCCo</em> opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Note
</div>
</div>
<div class="callout-body-container callout-body">
<p>This is a companion post to our recent AAAI 2024 paper co-authored with <a href="https://nl.linkedin.com/in/mfarmanbar">Mojtaba Farmanbar</a>, <a href="https://avandeursen.com/about/">Arie van Deursen</a> and <a href="https://www.cynthialiem.com/">Cynthia C. S. Liem</a><a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a>. The paper is a more formal and detailed treatment of the topic and is available <a href="https://arxiv.org/abs/2312.10648">here</a>. This post is intentionally free of technical details, maths or code. It is meant to provide a high-level overview of the paper.</p>
</div>
</div>
<section id="sec-poison" class="level2">
<h2 class="anchored" data-anchor-id="sec-poison">Pick your Poison</h2>
<p>There are two main philosophical debates in the field of Explainable AI (XAI). The first one centers around the role of explainability in AI: do we even need explanations and if so, why? Some have argued that we need not care about explaining models as long as they produce reliable outcomes <span class="citation" data-cites="robbins2019misdirected">(<a href="#ref-robbins2019misdirected" role="doc-biblioref">Robbins 2019</a>)</span>. Humans have not shied away from this in other domains either: doctors, for example, have prescribed aspirin for decades without understanding why it is a reliable drug for pain relief <span class="citation" data-cites="london2019artificial">(<a href="#ref-london2019artificial" role="doc-biblioref">London 2019</a>)</span>. While this reasoning may rightfully apply in some cases, experiments conducted by my colleagues at TU Delft have shown that humans do not make better decisions if aided by a reliable and yet opaque model <span class="citation" data-cites="he2023how">(<a href="#ref-he2023how" role="doc-biblioref">He, Buijsman, and Gadiraju 2023</a>)</span>. In these studies, subjects tended to ignore the AI model indicating that they did not trust its decisions. Beyond this, explainability comes with numerous advantages such as accountability, control, contestability and the potential for uncovering causal relationships.</p>
<div class="callout callout-style-default callout-tip callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Intermezzo: Why Bother?
</div>
</div>
<div class="callout-body-container callout-body">
<p>If we can blindly rely on the decisions made by an AI, why should we even bother to come up with explanations? I must confess I had never even seriously considered this as an option until attending <a href="https://www.tudelft.nl/staff/s.n.r.buijsman/">Stefan Buijsman</a>’s recent talk at a Delft Design for Values <a href="https://www.tudelft.nl/evenementen/2024/delft-ai/workshop-navigating-the-interplay-of-explainability-and-privacy-in-ai">workshop</a>, which inspired the previous paragraph. As a philosopher and AI ethicist at TU Delft, some of Stefan’s most recent research investigates causal human-in-the-loop explanations for AI models <span class="citation" data-cites="biswas2022chime">(<a href="#ref-biswas2022chime" role="doc-biblioref">Biswas et al. 2022</a>)</span>. I do not think that he adheres to the view that we should simply trust AI models blindly. In fact, he and his colleagues have shown that most humans do not tend to simply trust AI models even if they have been assured about their reliability <span class="citation" data-cites="he2023how">(<a href="#ref-he2023how" role="doc-biblioref">He, Buijsman, and Gadiraju 2023</a>)</span>.</p>
<p>Still, the question of why we even bother is an interesting challenge, especially considering that the field of XAI has so far struggled to produce satisfactory and robust results with meaningful real-world impact. Numerous studies related to <span class="citation" data-cites="he2023how">He, Buijsman, and Gadiraju (<a href="#ref-he2023how" role="doc-biblioref">2023</a>)</span> have shown that explanations for AI models either fail to help users or even mislead them <span class="citation" data-cites="mittelstadt2019explaining alufaisan2021does lakkaraju2020how">(<a href="#ref-mittelstadt2019explaining" role="doc-biblioref">Mittelstadt, Russell, and Wachter 2019</a>; <a href="#ref-alufaisan2021does" role="doc-biblioref">Alufaisan et al. 2021</a>; <a href="#ref-lakkaraju2020how" role="doc-biblioref">Lakkaraju and Bastani 2020</a>)</span>. It seems that neither blind trust nor explanations are a silver bullet.</p>
<p>So, have our efforts toward explainable AI been in vain? Should we simply stop explaining black-box models altogether as proposed by <span class="citation" data-cites="rudin2019stop">Rudin (<a href="#ref-rudin2019stop" role="doc-biblioref">2019</a>)</span>? Having worked in this field for a little over 2 years now, I have personally grown more and more skeptical of certain trends I have observed. In particular, I think that the community has focused too much on finding explanations that please us independent of the model itself (ideally model-agnostic, really!). This is a bit like applying a band-aid to a wound without first cleaning it. At best, plausible explanations for untrustworthy models provide a false sense of security. At worst, they can be used to manipulate and deceive. The AAAI paper presented in this post is very much born out of skepticism about this trend.</p>
<p>Nonetheless, I do not think all hope is lost for XAI. I strongly believe that there is a need for algorithmic recourse as long as we continue to deploy black-box models for automated decision-making. While I am fully supportive of the idea that we should always strive to use models that are as interpretable as possible <span class="citation" data-cites="rudin2019stop">(<a href="#ref-rudin2019stop" role="doc-biblioref">Rudin 2019</a>)</span>, I also think that there are cases where this is not feasible or it is simply too convenient to use a black-box model instead. The aspirin example mentioned above is a striking example of this. But it is easy enough to stretch that example further to illustrate why explainability is important. What if the use of aspirin was prohibited for a small minority of people and there was a reliable, opaque model to decide who belonged to that group? If you were part of that group, would you not want to know why? Why should you have to endure headaches for the rest of your life while others do not?</p>
<p>In summary, I think that—like it or not—we do need to bother.</p>
</div>
</div>
<p>The second major debate is about what constitutes a good explanation, because, crucially, explanations are not unique: was your headache cured by the aspirin you took before going to sleep or sleep itself? Or a combination of both? This multiplicity of explanations arises almost naturally in the context of counterfactual explanations. Unless the combination of input features for which the model predicts the target class or value is unique, there is always more than one possible explanation. As an illustrative example, consider the counterfactuals presented in <a href="#fig-cf-example">Figure&nbsp;1</a>. All of these are valid explanations for turning a ‘nine’ into ‘seven’ according to the underlying classifier (a simple multi-layer perceptron). They are all valid in the sense the model predicts the target label with high probability in each case. The troubling part is that even though all of the generated counterfactuals provide valid explanations for why the model predicts ‘seven’ instead of ‘nine’, they all look very different.</p>
<div id="fig-cf-example" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/mnist_motivation.png" class="img-fluid figure-img" style="width:80.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;1: Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using <em>Wachter</em> <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span>, <em>Schut</em> <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span> and <em>REVISE</em> <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span>.</figcaption>
</figure>
</div>
<p>So, which explanations do we trust most? Which one would you choose to present to an audience to explain how the classifier decides which digit to predict? Arguably, the counterfactual on the far right looks most like a ‘seven’, so I am willing to bet that most people would simply choose that one. It is valid after all and it looks plausible, while the other two counterfactuals might just lead to awkward questions from more interrogative audience members. In any case, I mentioned earlier that more plausible explanations tend to also be more actionable and robust, so this seems fair game. The counterfactual produced by <em>REVISE</em> <span class="citation" data-cites="joshi2019realistic">(<a href="#ref-joshi2019realistic" role="doc-biblioref">Joshi et al. 2019</a>)</span> is the poison we will pick—dump the rest and move on. Plausibility is all we need!<a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a></p>
<p>I am exaggerating but I do think that as a community of researchers studying counterfactual explanations, we have become so absorbed by the pursuit of a few desiderata that we have forgotten that ultimately we are in the business of explaining models. Our primary mandate is to design tools that help us understand why models predict certain outcomes. How useful is a plausible, actionable, sparse, causally valid explanation in gaining that understanding, if there exist a whole bunch of other valid explanations that do not meet these desiderata? Have we significantly improved our understanding of the underlying classifier in <a href="#fig-cf-example">Figure&nbsp;1</a> and therefore established a higher degree of trust in the model, simply because we have found a plausible counterfactual?</p>
<p>In my mind, we most certainly have not. I would argue that the existence of a valid and plausible explanation merely serves to reassure us that the model is not entirely ignorant about meaningful representations in the data. But as long as entirely implausible counterfactuals are also valid according to the model, selectively relying only on the subset of plausible counterfactuals may lead to a wrong sense of trust in untrustworthy models. That is why in our paper we argue that explanations should be faithful first, and plausible second.</p>
</section>
<section id="faithful-first-plausible-second" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="faithful-first-plausible-second">Faithful First, Plausible Second</h2>
<p>To navigate the interplay between faithfulness and plausibility, we propose a way to generate counterfactuals that are consistent with what the model has learned about the data. In doing so, we can also achieve plausibility but only in case the model has learned something meaningful.</p>
<section id="faithful-counterfactuals" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="faithful-counterfactuals">Faithful Counterfactuals</h3>
<p>When inquiring about what is “consistent with what the model has learned about the data”, we are essentially asking about the model’s posterior conditional distribution of the input data given the target output. It turns out that we can approximate that distribution using ideas relevant to energy-based modelling. In particular, we can use something called Stochastic Gradient Langevin Dynamics (SGLD) to sample from the model’s posterior conditional distribution <span class="citation" data-cites="welling2011bayesian">(<a href="#ref-welling2011bayesian" role="doc-biblioref">Welling and Teh 2011</a>)</span>.</p>
<p>Without going into too much detail here, the idea is to use the model’s energy function to guide the sampling process. The energy function is a scalar function that assigns a value to each possible configuration of the input data. The lower the energy, the higher the likelihood corresponding to the configuration. This is a powerful tool: <span class="citation" data-cites="grathwohl2020your">Grathwohl et al. (<a href="#ref-grathwohl2020your" role="doc-biblioref">2020</a>)</span>, for example, use SGLD in this fashion to train hybrid models—joint-energy models (JEM)—that are trained to both classify and generate data.</p>
<p><a href="#fig-faithful">Figure&nbsp;2</a> illustrates this concept. It shows samples (yellow stars) drawn from the posterior of a simple JEM trained on linearly separable data. The contour shows the kernel density estimate (KDE) for the learned conditional distribution. Although it seems that the posterior is too sharp in this case, the learned conditional distribution is overall consistent with the data (at least the mode is).</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-faithful" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/density_model.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;2: Kernel Density Estimate (KDE) for the learned conditional distribution. Yellow stars indicate samples generated through Stochastic Gradient Langevin Dynamics for a joint energy model (JEM).</figcaption>
</figure>
</div>
</div></div><p>Also shown in <a href="#fig-faithful">Figure&nbsp;2</a> is a single counterfactual path from the orange to the blue class. I have relied on the baseline approach proposed in <span class="citation" data-cites="wachter2017counterfactual">Wachter, Mittelstadt, and Russell (<a href="#ref-wachter2017counterfactual" role="doc-biblioref">2017</a>)</span> here using only a small penalty for the distance between the counterfactual and the original input. A truly faithful counterfactual, as we define it in our paper, would be one that we could expect to sample from the learned conditional distribution (with high probability)<a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>. Based on this notion, we would not characterize the counterfactual in <a href="#fig-faithful">Figure&nbsp;2</a> as faithful, but it also is not too far off.</p>
<p>It is easy to see how other desiderata may conflict with faithfulness. If I had penalized the distance between the counterfactual and the original input more, for example, then the counterfactual would have been less costly but also less faithful. This is the sort of trade-off between different desiderata that we always need to navigate carefully in the context of counterfactual explanations. As we will see next, the same also applies to plausibility but in a different way.</p>
</section>
<section id="plausible-counterfactuals" class="level3 page-columns page-full">
<h3 class="anchored" data-anchor-id="plausible-counterfactuals">Plausible Counterfactuals</h3>
<p>If you have followed the discussion so far, then you have already understood the trickiest concept in our paper. Plausibility can be defined much like we have done for faithfulness, but it is a bit more straightforward. In our paper, we broadly define plausible counterfactals as those that are indistinguishable from the observed data in the target domain. We already touched on this above when discussing the counterfactual images in <a href="#fig-cf-example">Figure&nbsp;1</a>.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-plausible" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/density_true.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;3: KDE for the conditional distribution based on observed data. Counterfactual path as in <a href="#fig-faithful">Figure&nbsp;2</a>.</figcaption>
</figure>
</div>
</div></div><p><a href="#fig-plausible">Figure&nbsp;3</a> illustrates the same concept for the same JEM as in <a href="#fig-faithful">Figure&nbsp;2</a>. The KDE in <a href="#fig-plausible">Figure&nbsp;3</a> shows the conditional distribution based on the observed data. The counterfactual path is the same as in <a href="#fig-faithful">Figure&nbsp;2</a>. The counterfactual is plausible in this case since it is not easily distinguishable from the observed data in the target domain.</p>
<p>Looking at both <a href="#fig-faithful">Figure&nbsp;2</a> and <a href="#fig-plausible">Figure&nbsp;3</a>, it becomes evident why the interplay between faithfulness and plausibility need not necessary be a trade-off. In this case, the counterfactual is neither terribly unfaithful nor implausible. This is because the learned conditional distribution is broadly consistent with the observed distribution of the data.</p>
</section>
</section>
<section id="our-approach-eccco" class="level2">
<h2 class="anchored" data-anchor-id="our-approach-eccco">Our approach: <em>ECCCo</em></h2>
<p>Now that we have covered the two major concepts in our paper, we can move on to our proposed approach for generating faithful counterfactuals: <em>ECCCo</em>. As the title of the paper suggests, <em>ECCCo</em> is an acronym for <em>E</em>nergy-<em>C</em>onstrained <em>C</em>onformal <em>Co</em>unterfactuals. We leverage ideas from energy-based modelling and conformal prediction, in particular from <span class="citation" data-cites="grathwohl2020your">Grathwohl et al. (<a href="#ref-grathwohl2020your" role="doc-biblioref">2020</a>)</span> and <span class="citation" data-cites="stutz2022learning">Stutz et al. (<a href="#ref-stutz2022learning" role="doc-biblioref">2022</a>)</span>, respectively. Our proposed counterfactual generation process involves little to no overhead and is broadly applicable to any model that can be trained using stochastic gradient descent. Technical details can be found in the paper. For now, let us focus on the high-level idea.</p>
<p><a href="#fig-poc">Figure&nbsp;4</a> compares the counterfactual path generated by <em>Wachter</em> <span class="citation" data-cites="wachter2017counterfactual">(<a href="#ref-wachter2017counterfactual" role="doc-biblioref">Wachter, Mittelstadt, and Russell 2017</a>)</span> to those generated by <em>ECCCo</em>, where we use ablation to remove the energy constraint—<em>ECCCo (no EBM)</em>—and the conformal prediction component—<em>ECCCo (no CP)</em>. In this case, the counterfactual generated by <em>Wachter</em> is neither faithful nor plausible. It does, however, minimize the distance between the counterfactual and the original input.</p>
<p>The counterfactual generated by <em>ECCCo (no EBM)</em> is deeper inside the blue class and has avoided points near the decision boundary on its path to its final destination. This is because <em>ECCCo (no EBM)</em> involves a penalty term for predictive uncertainty, which is high near the decision boundary. Intuitively, we would expect that avoiding regions of high predictive uncertainty in our counterfactual search should help with plausibility <span class="citation" data-cites="schut2021generating">(<a href="#ref-schut2021generating" role="doc-biblioref">Schut et al. 2021</a>)</span>. In this particular case, the final counterfactual is neither more faithful nor more plausible than the one generated by <em>Wachter</em>, but in our experiments we have generally found that penalizing predictive uncertainty alone can help to generate more faithful and plausible counterfactuals.</p>
<p>The counterfactual generated by <em>ECCCo (no CP)</em> is more faithful than the one generated by <em>Wachter</em> and <em>ECCCo (no EBM)</em>. This is because the energy constraint induces counterfactuals that are more consistent with the learned conditional distribution (as in <a href="#fig-faithful">Figure&nbsp;2</a>). Since the model has learned something meaningful about the data, the counterfactual is also more plausible than the one generated by <em>Wachter</em> and <em>ECCCo (no EBM)</em> in this case.</p>
<p>The counterfactual path generated by <em>ECCCo</em> combines benefits from both the energy constraint and the conformal prediction component. It avoids regions of high predictive uncertainty and ends up at a point that is consistent with the learned conditional distribution.</p>
<div id="fig-poc" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/poc_gradient_fields.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;4: Gradient fields and counterfactual paths for different generators.</figcaption>
</figure>
</div>
</section>
<section id="results" class="level2 page-columns page-full">
<h2 class="anchored" data-anchor-id="results">Results</h2>
<p>In the paper, we present results from extensive empirical studies involving eight datasets from different domains and a variety of models. We compare <em>ECCCo</em> to state-of-the-art counterfactual generators and show that it consistently outperforms these in terms of faithfulness and often achieves the highest degrees of plausibility. Here we will highlight some visual results from the MNIST dataset.</p>
<p><a href="#fig-mnist-benchmark">Figure&nbsp;5</a> shows counterfactuals generated using different counterfactual generators on the MNIST dataset. In this example, the goal is to generate a counterfactual in class ‘five’ for the factual ‘three’. The <em>ECCCo+</em> generator is a variant of <em>ECCCo</em> that performs gradient search in the space spanned by the first few principal components. This reduces computational costs and often helps with plausibility, sometimes at a small cost of faithfulness. The counterfactuals generated by <em>ECCCo</em> and <em>ECCCo+</em> are visibly more plausible than those generated by the other generators. In the paper, we quantify this using custom metrics for plausibility and faithfulness that we propose.</p>
<div id="fig-mnist-benchmark" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/mnist_benchmark.png" class="img-fluid figure-img" style="width:100.0%"></p>
<figcaption class="figure-caption">Figure&nbsp;5: Results for different generators (from 3 to 5).</figcaption>
</figure>
</div>
<p>We also find that the counterfactuals generated by <em>ECCCo</em> are more faithful in this case. The underlying model is a LeNet-5 convolutional neural network <span class="citation" data-cites="lecun1998gradient">(<a href="#ref-lecun1998gradient" role="doc-biblioref">LeCun et al. 1998</a>)</span>. Even today, convolutional neural networks are still among the most popular neural network architectures for image classification. Contrary to the simple multi-layer perceptron (MLP) used in <a href="#fig-cf-example">Figure&nbsp;1</a>, the LeNet-5 model is a bit more complex and it is not surprising that it has distilled more meaningful representations in the data.</p>
<p>More generally, we find that <em>ECCCo</em> is particularly effective at producing plausible counterfactuals for models that we would expect to have learned more meaningful representations of the data. This is consistent with our claim that <em>ECCCo</em> generates faithful counterfactuals. <a href="#fig-mnist-benchmark">Figure&nbsp;5</a> shows the results for applying <em>ECCCo</em> to the same factual ‘nine’ as in <a href="#fig-cf-example">Figure&nbsp;1</a> for different models from left to right and top to bottom: (a) an MLP, (b) a deep ensemble of MLPs, (c) a JEM, and, (d) a deep ensemble of JEMs. The plausibility of the generated counterfactual gradually improves from left to right and top to bottom as we get more rigorous about model complexity and training: deep ensembling can help to capture predictive uncertainty and joint-energy modelling is explicitly concerned with learning meaningful representations in the data.</p>

<div class="no-row-height column-margin column-container"><div class="">
<div id="fig-mnist-eccco" class="quarto-figure quarto-figure-center anchored">
<figure class="figure">
<p><img src="www/mnist_eccco.png" class="img-fluid figure-img"></p>
<figcaption class="figure-caption">Figure&nbsp;6: Turning a 9 into a 7. <em>ECCCo</em> applied to MLP (a), Ensemble (b), JEM (c), JEM Ensemble (d).</figcaption>
</figure>
</div>
</div></div><p>We would argue that in general, this is a desirable property of a counterfactual explainer, because it helps to distinguish trustworthy from unreliable models. The generated counterfactual for the MLP in (a) in <a href="#fig-mnist-eccco">Figure&nbsp;6</a> is grainy and altogether not very plausible. But this is precisely because the MLP is not very trustworthy: it is sensitive to input perturbations that are not meaningful. We think that explanations should reflect these kinds of shortcomings of models instead of hiding them.</p>
</section>
<section id="conclusion" class="level2">
<h2 class="anchored" data-anchor-id="conclusion">Conclusion</h2>
<p>This post has provided a brief and accessible overview of our AAAI 2024 <a href="https://arxiv.org/abs/2312.10648">paper</a> that introduces <em>ECCCo</em>: a new way to generate faithful model explanations through energy-constrained conformal counterfactuals. The post has covered some of the main points from the paper:</p>
<ul>
<li>We have argued that explanations should be faithful first, and plausible second.</li>
<li>We show that <em>ECCCo</em> consistently outperforms state-of-the-art counterfactual generators in terms of faithfulness and often achieves the highest degrees of plausibility.</li>
<li>We believe that <em>ECCCo</em> opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</li>
</ul>
</section>
<section id="software" class="level2">
<h2 class="anchored" data-anchor-id="software">Software</h2>
<p>The code for the experiments in the paper is available on GitHub: <a href="https://github.com/pat-alt/ECCCo.jl">https://github.com/pat-alt/ECCCo.jl</a>. The repo contains job scripts for running the experiments on a SLURM cluster, as well as the source code for the <em>ECCCo</em> package. The package is written in Julia and built on top of <a href="https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl">CounterfactualExplanations.jl</a>, which will eventually absorb the functionality of <em>ECCCo</em>.</p>
</section>
<section id="references" class="level2">



<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" role="list">
<div id="ref-alufaisan2021does" class="csl-entry" role="listitem">
Alufaisan, Yasmeen, Laura R Marusich, Jonathan Z Bakdash, Yan Zhou, and Murat Kantarcioglu. 2021. <span>“Does Explainable Artificial Intelligence Improve Human Decision-Making?”</span> In <em>Proceedings of the AAAI Conference on Artificial Intelligence</em>, 35:6618–26. 8.
</div>
<div id="ref-artelt2021evaluating" class="csl-entry" role="listitem">
Artelt, André, Valerie Vaquet, Riza Velioglu, Fabian Hinder, Johannes Brinkrolf, Malte Schilling, and Barbara Hammer. 2021. <span>“Evaluating Robustness of Counterfactual Explanations.”</span> In <em>2021 IEEE Symposium Series on Computational Intelligence (SSCI)</em>, 01–09. IEEE.
</div>
<div id="ref-biswas2022chime" class="csl-entry" role="listitem">
Biswas, Shreyan, Lorenzo Corti, Stefan Buijsman, and Jie Yang. 2022. <span>“CHIME: Causal Human-in-the-Loop Model Explanations.”</span> In <em>Proceedings of the AAAI Conference on Human Computation and Crowdsourcing</em>, 10:27–39. 1.
</div>
<div id="ref-grathwohl2020your" class="csl-entry" role="listitem">
Grathwohl, Will, Kuan-Chieh Wang, Joern-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, and Kevin Swersky. 2020. <span>“Your Classifier Is Secretly an Energy Based Model and You Should Treat It Like One.”</span> In <em>International Conference on Learning Representations</em>.
</div>
<div id="ref-he2023how" class="csl-entry" role="listitem">
He, Gaole, Stefan Buijsman, and Ujwal Gadiraju. 2023. <span>“How Stated Accuracy of an AI System and Analogies to Explain Accuracy Affect Human Reliance on the System.”</span> <em>Proc. ACM Hum.-Comput. Interact.</em> 7 (CSCW2). <a href="https://doi.org/10.1145/3610067">https://doi.org/10.1145/3610067</a>.
</div>
<div id="ref-joshi2019realistic" class="csl-entry" role="listitem">
Joshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. <span>“Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.”</span> <a href="https://arxiv.org/abs/1907.09615">https://arxiv.org/abs/1907.09615</a>.
</div>
<div id="ref-lakkaraju2020how" class="csl-entry" role="listitem">
Lakkaraju, Himabindu, and Osbert Bastani. 2020. <span>“" <span>How Do I Fool You</span>?" <span>Manipulating User Trust</span> via <span>Misleading Black Box Explanations</span>.”</span> In <em>Proceedings of the <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 79–85.
</div>
<div id="ref-lecun1998gradient" class="csl-entry" role="listitem">
LeCun, Yann, Léon Bottou, Yoshua Bengio, and Patrick Haffner. 1998. <span>“Gradient-Based Learning Applied to Document Recognition.”</span> <em>Proceedings of the IEEE</em> 86 (11): 2278–2324.
</div>
<div id="ref-london2019artificial" class="csl-entry" role="listitem">
London, Alex John. 2019. <span>“Artificial Intelligence and Black-Box Medical Decisions: Accuracy Versus Explainability.”</span> <em>Hastings Center Report</em> 49 (1): 15–21.
</div>
<div id="ref-mahajan2019preserving" class="csl-entry" role="listitem">
Mahajan, Divyat, Chenhao Tan, and Amit Sharma. 2020. <span>“Preserving Causal Constraints in Counterfactual Explanations for Machine Learning Classifiers.”</span> <a href="https://arxiv.org/abs/1912.03277">https://arxiv.org/abs/1912.03277</a>.
</div>
<div id="ref-mittelstadt2019explaining" class="csl-entry" role="listitem">
Mittelstadt, Brent, Chris Russell, and Sandra Wachter. 2019. <span>“Explaining Explanations in <span>AI</span>.”</span> In <em>Proceedings of the Conference on Fairness, Accountability, and Transparency</em>, 279–88. <a href="https://doi.org/10.1145/3287560.3287574">https://doi.org/10.1145/3287560.3287574</a>.
</div>
<div id="ref-robbins2019misdirected" class="csl-entry" role="listitem">
Robbins, Scott. 2019. <span>“A Misdirected Principle with a Catch: Explicability for AI.”</span> <em>Minds and Machines</em> 29 (4): 495–514.
</div>
<div id="ref-rudin2019stop" class="csl-entry" role="listitem">
Rudin, Cynthia. 2019. <span>“Stop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.”</span> <em>Nature Machine Intelligence</em> 1 (5): 206–15. <a href="https://doi.org/10.1038/s42256-019-0048-x">https://doi.org/10.1038/s42256-019-0048-x</a>.
</div>
<div id="ref-schut2021generating" class="csl-entry" role="listitem">
Schut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. <span>“Generating <span>Interpretable Counterfactual Explanations By Implicit Minimisation</span> of <span>Epistemic</span> and <span>Aleatoric Uncertainties</span>.”</span> In <em>International <span>Conference</span> on <span>Artificial Intelligence</span> and <span>Statistics</span></em>, 1756–64. <span>PMLR</span>.
</div>
<div id="ref-stutz2022learning" class="csl-entry" role="listitem">
Stutz, David, Krishnamurthy, Dvijotham, Ali Taylan Cemgil, and Arnaud Doucet. 2022. <span>“Learning Optimal Conformal Classifiers.”</span> <a href="https://arxiv.org/abs/2110.09192">https://arxiv.org/abs/2110.09192</a>.
</div>
<div id="ref-wachter2017counterfactual" class="csl-entry" role="listitem">
Wachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. <span>“Counterfactual Explanations Without Opening the Black Box: <span>Automated</span> Decisions and the <span>GDPR</span>.”</span> <em>Harv. JL &amp; Tech.</em> 31: 841. <a href="https://doi.org/10.2139/ssrn.3063289">https://doi.org/10.2139/ssrn.3063289</a>.
</div>
<div id="ref-welling2011bayesian" class="csl-entry" role="listitem">
Welling, Max, and Yee W Teh. 2011. <span>“Bayesian Learning via Stochastic Gradient Langevin Dynamics.”</span> In <em>Proceedings of the 28th International Conference on Machine Learning (ICML-11)</em>, 681–88. Citeseer.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>Besides my co-authors, I also want to thank the anonymous reviewers at both NeurIPS and AAAI for their valuable feedback as well as Nico Potyka and Francesco Leofante for interesting discussions on the topic.<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>Considering how much I have cited <span class="citation" data-cites="joshi2019realistic">Joshi et al. (<a href="#ref-joshi2019realistic" role="doc-biblioref">2019</a>)</span> in the past, I think it should go without saying that I very much like this paper, despite taking a critical stance on it here.<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>I have had an interesting chat with <a href="https://profiles.cardiff.ac.uk/staff/potykan">Nico Potyka</a> and <a href="https://fraleo.github.io/">Francesco Leofante</a>, recently, where they rightly pointed out that this definition of faithfulness needs to be refined. In particular, one might wonder what constitutes a ‘high probability’ in this context. I think this is a very valid point and I am looking forward to discussing this further with them.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{altmeyer2024,
  author = {Altmeyer, Patrick},
  title = {ECCCos from the {Black} {Box}},
  date = {2024-02-08},
  url = {https://www.paltmeyer.com/blog//blog/posts/eccco},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-altmeyer2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Altmeyer, Patrick. 2024. <span>“ECCCos from the Black Box.”</span>
February 8, 2024. <a href="https://www.paltmeyer.com/blog//blog/posts/eccco">https://www.paltmeyer.com/blog//blog/posts/eccco</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="pat-alt/pat-alt.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb1" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb1-1"><a href="#cb1-1"></a><span class="co">---</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="an">title:</span><span class="co"> ECCCos from the Black Box</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="an">subtitle:</span><span class="co"> Faithful Model Explanations through Energy-Based Conformal Counterfactuals</span></span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="an">date:</span><span class="co"> '2024-02-08'</span></span>
<span id="cb1-5"><a href="#cb1-5"></a><span class="an">categories:</span></span>
<span id="cb1-6"><a href="#cb1-6"></a><span class="co">  - counterfactuals</span></span>
<span id="cb1-7"><a href="#cb1-7"></a><span class="co">  - explainable AI</span></span>
<span id="cb1-8"><a href="#cb1-8"></a><span class="co">  - algorithmic recourse</span></span>
<span id="cb1-9"><a href="#cb1-9"></a><span class="co">  - Julia</span></span>
<span id="cb1-10"><a href="#cb1-10"></a><span class="an">description:</span><span class="co"> &gt;-</span></span>
<span id="cb1-11"><a href="#cb1-11"></a><span class="co">  *ECCCo* is a new way to generate faithful model explanations through counterfactuals that are as plausible as the underlying model permits. Companion post to our AAAI 2024 paper.</span></span>
<span id="cb1-12"><a href="#cb1-12"></a><span class="an">image:</span><span class="co"> www/density_model.png</span></span>
<span id="cb1-13"><a href="#cb1-13"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb1-14"><a href="#cb1-14"></a><span class="co">---</span></span>
<span id="cb1-15"><a href="#cb1-15"></a></span>
<span id="cb1-16"><a href="#cb1-16"></a>Counterfactual explanations offer an intuitive and straightforward way to explain opaque machine learning (ML) models. They work under the premise of perturbing inputs to achieve a desired change in the predicted output. There are typically many ways to achieve this, in other words, many different counterfactuals may yield the same desired outcome. A key challenge for researchers has therefore been to, firstly, define certain desirable characteristics of counterfactual explanations and, secondly, come up with efficient ways to achieve them.</span>
<span id="cb1-17"><a href="#cb1-17"></a></span>
<span id="cb1-18"><a href="#cb1-18"></a>One of the most important and studied characteristics of counterfactual explanations is 'plausibility': explanations should look realistic to humans. Plausibility is positively associated with actionability, robustness <span class="co">[</span><span class="ot">@artelt2021evaluating</span><span class="co">]</span> and causal validity <span class="co">[</span><span class="ot">@mahajan2019preserving</span><span class="co">]</span>. To achieve plausibility, many existing approaches rely on surrogate models. This is straightforward but it also convolutes things further: it essentially reallocates the task of learning plausible explanations for the data from the model itself to the surrogate.</span>
<span id="cb1-19"><a href="#cb1-19"></a></span>
<span id="cb1-20"><a href="#cb1-20"></a>In our AAAI 2024 paper, <span class="co">[</span><span class="ot">Faithful Model Explanations through Energy-Based Conformal Counterfactuals</span><span class="co">](https://arxiv.org/abs/2312.10648)</span> (*ECCCo*), we propose that we should not only look for explanations that please us but rather focus on generating counterfactuals that faithfully explain model behavior. It turns out that we can achieve both faithfulness and plausibility by relying solely on the model itself, leveraging recent advances in energy-based modelling and conformal prediction. We support this claim through extensive empirical studies and believe that *ECCCo* opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</span>
<span id="cb1-21"><a href="#cb1-21"></a></span>
<span id="cb1-22"><a href="#cb1-22"></a>::: {.callout-note}</span>
<span id="cb1-23"><a href="#cb1-23"></a></span>
<span id="cb1-24"><a href="#cb1-24"></a>This is a companion post to our recent AAAI 2024 paper co-authored with <span class="co">[</span><span class="ot">Mojtaba Farmanbar</span><span class="co">](https://nl.linkedin.com/in/mfarmanbar)</span>, <span class="co">[</span><span class="ot">Arie van Deursen</span><span class="co">](https://avandeursen.com/about/)</span> and <span class="co">[</span><span class="ot">Cynthia C. S. Liem</span><span class="co">](https://www.cynthialiem.com/)</span>^<span class="co">[</span><span class="ot">Besides my co-authors, I also want to thank the anonymous reviewers at both NeurIPS and AAAI for their valuable feedback as well as Nico Potyka and Francesco Leofante for interesting discussions on the topic.</span><span class="co">]</span>. The paper is a more formal and detailed treatment of the topic and is available <span class="co">[</span><span class="ot">here</span><span class="co">](https://arxiv.org/abs/2312.10648)</span>. This post is intentionally free of technical details, maths or code. It is meant to provide a high-level overview of the paper.</span>
<span id="cb1-25"><a href="#cb1-25"></a></span>
<span id="cb1-26"><a href="#cb1-26"></a>:::</span>
<span id="cb1-27"><a href="#cb1-27"></a></span>
<span id="cb1-28"><a href="#cb1-28"></a><span class="fu">## Pick your Poison {#sec-poison}</span></span>
<span id="cb1-29"><a href="#cb1-29"></a></span>
<span id="cb1-30"><a href="#cb1-30"></a>There are two main philosophical debates in the field of Explainable AI (XAI). The first one centers around the role of explainability in AI: do we even need explanations and if so, why? Some have argued that we need not care about explaining models as long as they produce reliable outcomes <span class="co">[</span><span class="ot">@robbins2019misdirected</span><span class="co">]</span>. Humans have not shied away from this in other domains either: doctors, for example, have prescribed aspirin for decades without understanding why it is a reliable drug for pain relief <span class="co">[</span><span class="ot">@london2019artificial</span><span class="co">]</span>. While this reasoning may rightfully apply in some cases, experiments conducted by my colleagues at TU Delft have shown that humans do not make better decisions if aided by a reliable and yet opaque model <span class="co">[</span><span class="ot">@he2023how</span><span class="co">]</span>. In these studies, subjects tended to ignore the AI model indicating that they did not trust its decisions. Beyond this, explainability comes with numerous advantages such as accountability, control, contestability and the potential for uncovering causal relationships. </span>
<span id="cb1-31"><a href="#cb1-31"></a></span>
<span id="cb1-32"><a href="#cb1-32"></a>::: {.callout-tip}</span>
<span id="cb1-33"><a href="#cb1-33"></a></span>
<span id="cb1-34"><a href="#cb1-34"></a><span class="fu">## Intermezzo: Why Bother?</span></span>
<span id="cb1-35"><a href="#cb1-35"></a></span>
<span id="cb1-36"><a href="#cb1-36"></a>If we can blindly rely on the decisions made by an AI, why should we even bother to come up with explanations? I must confess I had never even seriously considered this as an option until attending <span class="co">[</span><span class="ot">Stefan Buijsman</span><span class="co">](https://www.tudelft.nl/staff/s.n.r.buijsman/)</span>'s recent talk at a Delft Design for Values <span class="co">[</span><span class="ot">workshop</span><span class="co">](https://www.tudelft.nl/evenementen/2024/delft-ai/workshop-navigating-the-interplay-of-explainability-and-privacy-in-ai)</span>, which inspired the previous paragraph. As a philosopher and AI ethicist at TU Delft, some of Stefan's most recent research investigates causal human-in-the-loop explanations for AI models <span class="co">[</span><span class="ot">@biswas2022chime</span><span class="co">]</span>. I do not think that he adheres to the view that we should simply trust AI models blindly. In fact, he and his colleagues have shown that most humans do not tend to simply trust AI models even if they have been assured about their reliability <span class="co">[</span><span class="ot">@he2023how</span><span class="co">]</span>.</span>
<span id="cb1-37"><a href="#cb1-37"></a></span>
<span id="cb1-38"><a href="#cb1-38"></a>Still, the question of why we even bother is an interesting challenge, especially considering that the field of XAI has so far struggled to produce satisfactory and robust results with meaningful real-world impact. Numerous studies related to @he2023how have shown that explanations for AI models either fail to help users or even mislead them <span class="co">[</span><span class="ot">@mittelstadt2019explaining;@alufaisan2021does;@lakkaraju2020how</span><span class="co">]</span>. It seems that neither blind trust nor explanations are a silver bullet.</span>
<span id="cb1-39"><a href="#cb1-39"></a></span>
<span id="cb1-40"><a href="#cb1-40"></a>So, have our efforts toward explainable AI been in vain? Should we simply stop explaining black-box models altogether as proposed by @rudin2019stop? Having worked in this field for a little over 2 years now, I have personally grown more and more skeptical of certain trends I have observed. In particular, I think that the community has focused too much on finding explanations that please us independent of the model itself (ideally model-agnostic, really!). This is a bit like applying a band-aid to a wound without first cleaning it. At best, plausible explanations for untrustworthy models provide a false sense of security. At worst, they can be used to manipulate and deceive. The AAAI paper presented in this post is very much born out of skepticism about this trend.</span>
<span id="cb1-41"><a href="#cb1-41"></a></span>
<span id="cb1-42"><a href="#cb1-42"></a>Nonetheless, I do not think all hope is lost for XAI. I strongly believe that there is a need for algorithmic recourse as long as we continue to deploy black-box models for automated decision-making. While I am fully supportive of the idea that we should always strive to use models that are as interpretable as possible <span class="co">[</span><span class="ot">@rudin2019stop</span><span class="co">]</span>, I also think that there are cases where this is not feasible or it is simply too convenient to use a black-box model instead. The aspirin example mentioned above is a striking example of this. But it is easy enough to stretch that example further to illustrate why explainability is important. What if the use of aspirin was prohibited for a small minority of people and there was a reliable, opaque model to decide who belonged to that group? If you were part of that group, would you not want to know why? Why should you have to endure headaches for the rest of your life while others do not?</span>
<span id="cb1-43"><a href="#cb1-43"></a></span>
<span id="cb1-44"><a href="#cb1-44"></a>In summary, I think that---like it or not---we do need to bother. </span>
<span id="cb1-45"><a href="#cb1-45"></a></span>
<span id="cb1-46"><a href="#cb1-46"></a>:::</span>
<span id="cb1-47"><a href="#cb1-47"></a></span>
<span id="cb1-48"><a href="#cb1-48"></a>The second major debate is about what constitutes a good explanation, because, crucially, explanations are not unique: was your headache cured by the aspirin you took before going to sleep or sleep itself? Or a combination of both? This multiplicity of explanations arises almost naturally in the context of counterfactual explanations. Unless the combination of input features for which the model predicts the target class or value is unique, there is always more than one possible explanation. As an illustrative example, consider the counterfactuals presented in @fig-cf-example. All of these are valid explanations for turning a 'nine' into 'seven' according to the underlying classifier (a simple multi-layer perceptron). They are all valid in the sense the model predicts the target label with high probability in each case. The troubling part is that even though all of the generated counterfactuals provide valid explanations for why the model predicts 'seven' instead of 'nine', they all look very different. </span>
<span id="cb1-49"><a href="#cb1-49"></a></span>
<span id="cb1-50"><a href="#cb1-50"></a>!<span class="co">[</span><span class="ot">Turning a 9 into a 7: Counterfactual explanations for an image classifier produced using *Wachter* [@wachter2017counterfactual], *Schut* [@schut2021generating] and *REVISE* [@joshi2019realistic].</span><span class="co">](www/mnist_motivation.png)</span>{#fig-cf-example width="80%"}</span>
<span id="cb1-51"><a href="#cb1-51"></a></span>
<span id="cb1-52"><a href="#cb1-52"></a>So, which explanations do we trust most? Which one would you choose to present to an audience to explain how the classifier decides which digit to predict? Arguably, the counterfactual on the far right looks most like a 'seven', so I am willing to bet that most people would simply choose that one. It is valid after all and it looks plausible, while the other two counterfactuals might just lead to awkward questions from more interrogative audience members. In any case, I mentioned earlier that more plausible explanations tend to also be more actionable and robust, so this seems fair game. The counterfactual produced by *REVISE* <span class="co">[</span><span class="ot">@joshi2019realistic</span><span class="co">]</span> is the poison we will pick---dump the rest and move on. Plausibility is all we need!^<span class="co">[</span><span class="ot">Considering how much I have cited @joshi2019realistic in the past, I think it should go without saying that I very much like this paper, despite taking a critical stance on it here.</span><span class="co">]</span></span>
<span id="cb1-53"><a href="#cb1-53"></a></span>
<span id="cb1-54"><a href="#cb1-54"></a>I am exaggerating but I do think that as a community of researchers studying counterfactual explanations, we have become so absorbed by the pursuit of a few desiderata that we have forgotten that ultimately we are in the business of explaining models. Our primary mandate is to design tools that help us understand why models predict certain outcomes. How useful is a plausible, actionable, sparse, causally valid explanation in gaining that understanding, if there exist a whole bunch of other valid explanations that do not meet these desiderata? Have we significantly improved our understanding of the underlying classifier in @fig-cf-example and therefore established a higher degree of trust in the model, simply because we have found a plausible counterfactual? </span>
<span id="cb1-55"><a href="#cb1-55"></a></span>
<span id="cb1-56"><a href="#cb1-56"></a>In my mind, we most certainly have not. I would argue that the existence of a valid and plausible explanation merely serves to reassure us that the model is not entirely ignorant about meaningful representations in the data. But as long as entirely implausible counterfactuals are also valid according to the model, selectively relying only on the subset of plausible counterfactuals may lead to a wrong sense of trust in untrustworthy models. That is why in our paper we argue that explanations should be faithful first, and plausible second.</span>
<span id="cb1-57"><a href="#cb1-57"></a></span>
<span id="cb1-58"><a href="#cb1-58"></a><span class="fu">## Faithful First, Plausible Second</span></span>
<span id="cb1-59"><a href="#cb1-59"></a></span>
<span id="cb1-60"><a href="#cb1-60"></a>To navigate the interplay between faithfulness and plausibility, we propose a way to generate counterfactuals that are consistent with what the model has learned about the data. In doing so, we can also achieve plausibility but only in case the model has learned something meaningful. </span>
<span id="cb1-61"><a href="#cb1-61"></a></span>
<span id="cb1-62"><a href="#cb1-62"></a><span class="fu">### Faithful Counterfactuals</span></span>
<span id="cb1-63"><a href="#cb1-63"></a></span>
<span id="cb1-64"><a href="#cb1-64"></a>When inquiring about what is "consistent with what the model has learned about the data", we are essentially asking about the model's posterior conditional distribution of the input data given the target output. It turns out that we can approximate that distribution using ideas relevant to energy-based modelling. In particular, we can use something called Stochastic Gradient Langevin Dynamics (SGLD) to sample from the model's posterior conditional distribution <span class="co">[</span><span class="ot">@welling2011bayesian</span><span class="co">]</span>. </span>
<span id="cb1-65"><a href="#cb1-65"></a></span>
<span id="cb1-66"><a href="#cb1-66"></a>Without going into too much detail here, the idea is to use the model's energy function to guide the sampling process. The energy function is a scalar function that assigns a value to each possible configuration of the input data. The lower the energy, the higher the likelihood corresponding to the configuration. This is a powerful tool: @grathwohl2020your, for example, use SGLD in this fashion to train hybrid models---joint-energy models (JEM)---that are trained to both classify and generate data. </span>
<span id="cb1-67"><a href="#cb1-67"></a></span>
<span id="cb1-68"><a href="#cb1-68"></a>@fig-faithful illustrates this concept. It shows samples (yellow stars) drawn from the posterior of a simple JEM trained on linearly separable data. The contour shows the kernel density estimate (KDE) for the learned conditional distribution. Although it seems that the posterior is too sharp in this case, the learned conditional distribution is overall consistent with the data (at least the mode is).</span>
<span id="cb1-69"><a href="#cb1-69"></a></span>
<span id="cb1-70"><a href="#cb1-70"></a>::: {.column-margin}</span>
<span id="cb1-71"><a href="#cb1-71"></a><span class="al">![Kernel Density Estimate (KDE) for the learned conditional distribution. Yellow stars indicate samples generated through Stochastic Gradient Langevin Dynamics for a joint energy model (JEM).](www/density_model.png)</span>{#fig-faithful}</span>
<span id="cb1-72"><a href="#cb1-72"></a>:::</span>
<span id="cb1-73"><a href="#cb1-73"></a></span>
<span id="cb1-74"><a href="#cb1-74"></a>Also shown in @fig-faithful is a single counterfactual path from the orange to the blue class. I have relied on the baseline approach proposed in @wachter2017counterfactual here using only a small penalty for the distance between the counterfactual and the original input. A truly faithful counterfactual, as we define it in our paper, would be one that we could expect to sample from the learned conditional distribution (with high probability)^<span class="co">[</span><span class="ot">I have had an interesting chat with [Nico Potyka](https://profiles.cardiff.ac.uk/staff/potykan) and [Francesco Leofante](https://fraleo.github.io/), recently, where they rightly pointed out that this definition of faithfulness needs to be refined. In particular, one might wonder what constitutes a 'high probability' in this context. I think this is a very valid point and I am looking forward to discussing this further with them.</span><span class="co">]</span>. Based on this notion, we would not characterize the counterfactual in @fig-faithful as faithful, but it also is not too far off. </span>
<span id="cb1-75"><a href="#cb1-75"></a></span>
<span id="cb1-76"><a href="#cb1-76"></a>It is easy to see how other desiderata may conflict with faithfulness. If I had penalized the distance between the counterfactual and the original input more, for example, then the counterfactual would have been less costly but also less faithful. This is the sort of trade-off between different desiderata that we always need to navigate carefully in the context of counterfactual explanations. As we will see next, the same also applies to plausibility but in a different way.</span>
<span id="cb1-77"><a href="#cb1-77"></a></span>
<span id="cb1-78"><a href="#cb1-78"></a><span class="fu">### Plausible Counterfactuals</span></span>
<span id="cb1-79"><a href="#cb1-79"></a></span>
<span id="cb1-80"><a href="#cb1-80"></a>If you have followed the discussion so far, then you have already understood the trickiest concept in our paper. Plausibility can be defined much like we have done for faithfulness, but it is a bit more straightforward. In our paper, we broadly define plausible counterfactals as those that are indistinguishable from the observed data in the target domain. We already touched on this above when discussing the counterfactual images in @fig-cf-example. </span>
<span id="cb1-81"><a href="#cb1-81"></a></span>
<span id="cb1-82"><a href="#cb1-82"></a>::: {.column-margin}</span>
<span id="cb1-83"><a href="#cb1-83"></a><span class="al">![KDE for the conditional distribution based on observed data. Counterfactual path as in @fig-faithful.](www/density_true.png)</span>{#fig-plausible}</span>
<span id="cb1-84"><a href="#cb1-84"></a>:::</span>
<span id="cb1-85"><a href="#cb1-85"></a></span>
<span id="cb1-86"><a href="#cb1-86"></a>@fig-plausible illustrates the same concept for the same JEM as in @fig-faithful. The KDE in @fig-plausible shows the conditional distribution based on the observed data. The counterfactual path is the same as in @fig-faithful. The counterfactual is plausible in this case since it is not easily distinguishable from the observed data in the target domain.</span>
<span id="cb1-87"><a href="#cb1-87"></a></span>
<span id="cb1-88"><a href="#cb1-88"></a>Looking at both @fig-faithful and @fig-plausible, it becomes evident why the interplay between faithfulness and plausibility need not necessary be a trade-off. In this case, the counterfactual is neither terribly unfaithful nor implausible. This is because the learned conditional distribution is broadly consistent with the observed distribution of the data.</span>
<span id="cb1-89"><a href="#cb1-89"></a></span>
<span id="cb1-90"><a href="#cb1-90"></a><span class="fu">## Our approach: *ECCCo*</span></span>
<span id="cb1-91"><a href="#cb1-91"></a></span>
<span id="cb1-92"><a href="#cb1-92"></a>Now that we have covered the two major concepts in our paper, we can move on to our proposed approach for generating faithful counterfactuals: *ECCCo*. As the title of the paper suggests, *ECCCo* is an acronym for *E*nergy-*C*onstrained *C*onformal *Co*unterfactuals. We leverage ideas from energy-based modelling and conformal prediction, in particular from @grathwohl2020your and @stutz2022learning, respectively. Our proposed counterfactual generation process involves little to no overhead and is broadly applicable to any model that can be trained using stochastic gradient descent. Technical details can be found in the paper. For now, let us focus on the high-level idea.</span>
<span id="cb1-93"><a href="#cb1-93"></a></span>
<span id="cb1-94"><a href="#cb1-94"></a>@fig-poc compares the counterfactual path generated by *Wachter* [@wachter2017counterfactual] to those generated by *ECCCo*, where we use ablation to remove the energy constraint---*ECCCo (no EBM)*---and the conformal prediction component---*ECCCo (no CP)*. In this case, the counterfactual generated by *Wachter* is neither faithful nor plausible. It does, however, minimize the distance between the counterfactual and the original input. </span>
<span id="cb1-95"><a href="#cb1-95"></a></span>
<span id="cb1-96"><a href="#cb1-96"></a>The counterfactual generated by *ECCCo (no EBM)* is deeper inside the blue class and has avoided points near the decision boundary on its path to its final destination. This is because *ECCCo (no EBM)* involves a penalty term for predictive uncertainty, which is high near the decision boundary. Intuitively, we would expect that avoiding regions of high predictive uncertainty in our counterfactual search should help with plausibility [@schut2021generating]. In this particular case, the final counterfactual is neither more faithful nor more plausible than the one generated by *Wachter*, but in our experiments we have generally found that penalizing predictive uncertainty alone can help to generate more faithful and plausible counterfactuals.</span>
<span id="cb1-97"><a href="#cb1-97"></a></span>
<span id="cb1-98"><a href="#cb1-98"></a>The counterfactual generated by *ECCCo (no CP)* is more faithful than the one generated by *Wachter* and *ECCCo (no EBM)*. This is because the energy constraint induces counterfactuals that are more consistent with the learned conditional distribution (as in @fig-faithful). Since the model has learned something meaningful about the data, the counterfactual is also more plausible than the one generated by *Wachter* and *ECCCo (no EBM)* in this case. </span>
<span id="cb1-99"><a href="#cb1-99"></a></span>
<span id="cb1-100"><a href="#cb1-100"></a>The counterfactual path generated by *ECCCo* combines benefits from both the energy constraint and the conformal prediction component. It avoids regions of high predictive uncertainty and ends up at a point that is consistent with the learned conditional distribution. </span>
<span id="cb1-101"><a href="#cb1-101"></a></span>
<span id="cb1-102"><a href="#cb1-102"></a><span class="al">![Gradient fields and counterfactual paths for different generators.](www/poc_gradient_fields.png)</span>{#fig-poc}</span>
<span id="cb1-103"><a href="#cb1-103"></a></span>
<span id="cb1-104"><a href="#cb1-104"></a><span class="fu">## Results</span></span>
<span id="cb1-105"><a href="#cb1-105"></a></span>
<span id="cb1-106"><a href="#cb1-106"></a>In the paper, we present results from extensive empirical studies involving eight datasets from different domains and a variety of models. We compare *ECCCo* to state-of-the-art counterfactual generators and show that it consistently outperforms these in terms of faithfulness and often achieves the highest degrees of plausibility. Here we will highlight some visual results from the MNIST dataset.</span>
<span id="cb1-107"><a href="#cb1-107"></a></span>
<span id="cb1-108"><a href="#cb1-108"></a>@fig-mnist-benchmark shows counterfactuals generated using different counterfactual generators on the MNIST dataset. In this example, the goal is to generate a counterfactual in class 'five' for the factual 'three'. The *ECCCo+* generator is a variant of *ECCCo* that performs gradient search in the space spanned by the first few principal components. This reduces computational costs and often helps with plausibility, sometimes at a small cost of faithfulness. The counterfactuals generated by *ECCCo* and *ECCCo+* are visibly more plausible than those generated by the other generators. In the paper, we quantify this using custom metrics for plausibility and faithfulness that we propose. </span>
<span id="cb1-109"><a href="#cb1-109"></a></span>
<span id="cb1-110"><a href="#cb1-110"></a><span class="al">![Results for different generators (from 3 to 5).](www/mnist_benchmark.png)</span>{#fig-mnist-benchmark width="100%"}</span>
<span id="cb1-111"><a href="#cb1-111"></a></span>
<span id="cb1-112"><a href="#cb1-112"></a>We also find that the counterfactuals generated by *ECCCo* are more faithful in this case. The underlying model is a LeNet-5 convolutional neural network <span class="co">[</span><span class="ot">@lecun1998gradient</span><span class="co">]</span>. Even today, convolutional neural networks are still among the most popular neural network architectures for image classification. Contrary to the simple multi-layer perceptron (MLP) used in @fig-cf-example, the LeNet-5 model is a bit more complex and it is not surprising that it has distilled more meaningful representations in the data. </span>
<span id="cb1-113"><a href="#cb1-113"></a></span>
<span id="cb1-114"><a href="#cb1-114"></a>More generally, we find that *ECCCo* is particularly effective at producing plausible counterfactuals for models that we would expect to have learned more meaningful representations of the data. This is consistent with our claim that *ECCCo* generates faithful counterfactuals. @fig-mnist-benchmark shows the results for applying *ECCCo* to the same factual 'nine' as in @fig-cf-example for different models from left to right and top to bottom: (a) an MLP, (b) a deep ensemble of MLPs, (c) a JEM, and, (d) a deep ensemble of JEMs. The plausibility of the generated counterfactual gradually improves from left to right and top to bottom as we get more rigorous about model complexity and training: deep ensembling can help to capture predictive uncertainty and joint-energy modelling is explicitly concerned with learning meaningful representations in the data. </span>
<span id="cb1-115"><a href="#cb1-115"></a></span>
<span id="cb1-116"><a href="#cb1-116"></a>::: {.column-margin}</span>
<span id="cb1-117"><a href="#cb1-117"></a><span class="al">![Turning a 9 into a 7. *ECCCo* applied to MLP (a), Ensemble (b), JEM (c), JEM Ensemble (d).](www/mnist_eccco.png)</span>{#fig-mnist-eccco}</span>
<span id="cb1-118"><a href="#cb1-118"></a>:::</span>
<span id="cb1-119"><a href="#cb1-119"></a></span>
<span id="cb1-120"><a href="#cb1-120"></a>We would argue that in general, this is a desirable property of a counterfactual explainer, because it helps to distinguish trustworthy from unreliable models. The generated counterfactual for the MLP in (a) in @fig-mnist-eccco is grainy and altogether not very plausible. But this is precisely because the MLP is not very trustworthy: it is sensitive to input perturbations that are not meaningful. We think that explanations should reflect these kinds of shortcomings of models instead of hiding them.</span>
<span id="cb1-121"><a href="#cb1-121"></a></span>
<span id="cb1-122"><a href="#cb1-122"></a><span class="fu">## Conclusion</span></span>
<span id="cb1-123"><a href="#cb1-123"></a></span>
<span id="cb1-124"><a href="#cb1-124"></a>This post has provided a brief and accessible overview of our AAAI 2024 <span class="co">[</span><span class="ot">paper</span><span class="co">](https://arxiv.org/abs/2312.10648)</span> that introduces *ECCCo*: a new way to generate faithful model explanations through energy-constrained conformal counterfactuals. The post has covered some of the main points from the paper:</span>
<span id="cb1-125"><a href="#cb1-125"></a></span>
<span id="cb1-126"><a href="#cb1-126"></a><span class="ss">- </span>We have argued that explanations should be faithful first, and plausible second. </span>
<span id="cb1-127"><a href="#cb1-127"></a><span class="ss">- </span>We show that *ECCCo* consistently outperforms state-of-the-art counterfactual generators in terms of faithfulness and often achieves the highest degrees of plausibility. </span>
<span id="cb1-128"><a href="#cb1-128"></a><span class="ss">- </span>We believe that *ECCCo* opens avenues for researchers and practitioners seeking tools to better distinguish trustworthy from unreliable models.</span>
<span id="cb1-129"><a href="#cb1-129"></a></span>
<span id="cb1-130"><a href="#cb1-130"></a><span class="fu">## Software</span></span>
<span id="cb1-131"><a href="#cb1-131"></a></span>
<span id="cb1-132"><a href="#cb1-132"></a>The code for the experiments in the paper is available on GitHub: <span class="co">[</span><span class="ot">https://github.com/pat-alt/ECCCo.jl</span><span class="co">](https://github.com/pat-alt/ECCCo.jl)</span>. The repo contains job scripts for running the experiments on a SLURM cluster, as well as the source code for the *ECCCo* package. The package is written in Julia and built on top of [CounterfactualExplanations.jl](https://github.com/JuliaTrustworthyAI/CounterfactualExplanations.jl), which will eventually absorb the functionality of *ECCCo*.</span>
<span id="cb1-133"><a href="#cb1-133"></a></span>
<span id="cb1-134"><a href="#cb1-134"></a><span class="fu">## References</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">© 2023, Patrick Altmeyer</div>   
    <div class="nav-footer-center">
      &nbsp;
    </div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.paltmeyer.com/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://twitter.com/paltmey">
      <i class="bi bi-twitter" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/\@patrick.altmeyer">
      <i class="bi bi-medium" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>



</body></html>