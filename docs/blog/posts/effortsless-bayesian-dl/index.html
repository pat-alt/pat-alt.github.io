<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick Altmeyer">
<meta name="dcterms.date" content="2022-02-18">
<meta name="description" content="An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia.">

<title>Patrick Altmeyer - Go deep, but also … go Bayesian!</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../..//www/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BEEZ30787D"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BEEZ30787D', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Patrick Altmeyer - Go deep, but also … go Bayesian!">
<meta property="og:description" content="An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia.">
<meta property="og:image" content="https://www.paltmeyer.com/blog/blog/posts/effortsless-bayesian-dl/www/intro.gif">
<meta property="og:site_name" content="Patrick Altmeyer">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../www/icon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Patrick Altmeyer</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/publications/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/talks/index.html"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog/index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://julialang.social/@patalt" rel="me"> <i class="bi bi-mastodon" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#the-case-for-bayesian-deep-learning" id="toc-the-case-for-bayesian-deep-learning" class="nav-link active" data-scroll-target="#the-case-for-bayesian-deep-learning">The case for Bayesian Deep Learning</a></li>
  <li><a href="#laplace-approximation" id="toc-laplace-approximation" class="nav-link" data-scroll-target="#laplace-approximation">Laplace Approximation</a>
  <ul class="collapse">
  <li><a href="#from-bayesian-logistic-regression" id="toc-from-bayesian-logistic-regression" class="nav-link" data-scroll-target="#from-bayesian-logistic-regression">From Bayesian Logistic Regression …</a></li>
  <li><a href="#to-bayesian-neural-networks" id="toc-to-bayesian-neural-networks" class="nav-link" data-scroll-target="#to-bayesian-neural-networks">… to Bayesian Neural Networks</a></li>
  </ul></li>
  <li><a href="#wrapping-up" id="toc-wrapping-up" class="nav-link" data-scroll-target="#wrapping-up">Wrapping up</a></li>
  <li><a href="#resources" id="toc-resources" class="nav-link" data-scroll-target="#resources">Resources</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">Go deep, but also … go Bayesian!</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">Effortless Bayesian Deep Learning in Julia — Part I</p>
  <div class="quarto-categories">
    <div class="quarto-category">bayes</div>
    <div class="quarto-category">deep learning</div>
    <div class="quarto-category">Julia</div>
  </div>
  </div>

<div>
  <div class="description">
    An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.tudelft.nl/en/">
            Delft University of Technology
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 18, 2022</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="intro-gif">
<figure class="figure">
<img src="www/intro.gif" class="figure-img">
<figcaption>
A Bayesian Neural Network gradually learns.
</figcaption>
</figure>
</div>
<p>Deep learning has dominated AI research in recent years<a href="#fn1" class="footnote-ref" id="fnref1" role="doc-noteref"><sup>1</sup></a> - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on <a href="https://twitter.com/ilyasut/status/1491554478243258368">Twitter</a>. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slightly conscious - that’s “may” and “slightly” and only if you just go deep enough? On the other side you have prominent skeptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent <span class="citation" data-cites="pearl2018book">(<a href="#ref-pearl2018book" role="doc-biblioref">Pearl and Mackenzie 2018</a>)</span>.</p>
<section id="the-case-for-bayesian-deep-learning" class="level2">
<h2 class="anchored" data-anchor-id="the-case-for-bayesian-deep-learning">The case for Bayesian Deep Learning</h2>
<p>Whatever side of this entertaining twitter dispute you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization <span class="citation" data-cites="raghunathan2019adversarial">(<a href="#ref-raghunathan2019adversarial" role="doc-biblioref">Raghunathan et al. 2019</a>)</span> and does ultimately not guarantee stability <span class="citation" data-cites="bastounis2021mathematics">(<a href="#ref-bastounis2021mathematics" role="doc-biblioref">Bastounis, Hansen, and Vlačić 2021</a>)</span>. With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness <span class="citation" data-cites="slack2020fooling">(<a href="#ref-slack2020fooling" role="doc-biblioref">Slack et al. 2020</a>)</span>.</p>
<p>Exactly why are deep neural networks unstable and in-transparent? Let <span class="math inline">\(\mathcal{D}=\{x,y\}_{n=1}^N\)</span> denote our feature-label pairs and let <span class="math inline">\(f(x;\theta)=y\)</span> denote some deep neural network specified by its parameters <span class="math inline">\(\theta\)</span>. Then the first thing to note is that the number of free parameters <span class="math inline">\(\theta\)</span> is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters <em>relative</em> to the size of <span class="math inline">\(\mathcal{D}\)</span> is generally huge:</p>
<blockquote class="blockquote">
<p>[…] deep neural networks are typically very underspecified by the available data, and […] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. <span class="citation" data-cites="wilson2020case">(<a href="#ref-wilson2020case" role="doc-biblioref">Wilson 2020</a>)</span></p>
</blockquote>
<p>In other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such “compelling explanation” for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:</p>
<blockquote class="blockquote">
<p>How compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?</p>
</blockquote>
<p>A scenario like this very much calls for treating predictions from deep learning models probabilistically <span class="citation" data-cites="wilson2020case">(<a href="#ref-wilson2020case" role="doc-biblioref">Wilson 2020</a>)</span><a href="#fn2" class="footnote-ref" id="fnref2" role="doc-noteref"><sup>2</sup></a><a href="#fn3" class="footnote-ref" id="fnref3" role="doc-noteref"><sup>3</sup></a>.</p>
<p>Formally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):</p>
<p><span class="math display">\[
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
\]</span></p>
<p>The integral implies that we essentially need many predictions from many different specifications of <span class="math inline">\(\theta\)</span>. Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above in recent years: <span class="citation" data-cites="gal2016dropout">Gal and Ghahramani (<a href="#ref-gal2016dropout" role="doc-biblioref">2016</a>)</span> propose using dropout at test time while <span class="citation" data-cites="lakshminarayanan2016simple">Lakshminarayanan, Pritzel, and Blundell (<a href="#ref-lakshminarayanan2016simple" role="doc-biblioref">2017</a>)</span> show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: <strong>Laplace approximation</strong> (LA).</p>
<p>If you have read my <a href="https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f">previous post</a> on Bayesian Logistic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning.</p>
</section>
<section id="laplace-approximation" class="level2">
<h2 class="anchored" data-anchor-id="laplace-approximation">Laplace Approximation</h2>
<p>While LA was first proposed in the 18th century, it has so far not attracted serious attention from the deep learning community largely because it involves a possibly large Hessian computation. <span class="citation" data-cites="daxberger2021laplace">Daxberger et al. (<a href="#ref-daxberger2021laplace" role="doc-biblioref">2021</a>)</span> are on a mission to change the perception that LA has no use in DL: in their <a href="https://arxiv.org/pdf/2106.14806.pdf">NeurIPS 2021 paper</a> they demonstrate empirically that LA can be used to produce Bayesian model averages that are at least at par with existing approaches in terms of uncertainty quantification and out-of-distribution detection and significantly cheaper to compute. They show that recent advancements in autodifferentation can be leveraged to produce fast and accurate approximations of the Hessian and even provide a fully-fledged <a href="https://aleximmer.github.io/Laplace/">Python library</a> that can be used with any pretrained Torch model. For this post, I have built a much less comprehensive, pure-play equivalent of their package in Julia - <a href="https://www.paltmeyer.com/LaplaceRedux.jl/dev/">LaplaceRedux.jl</a> can be used with deep learning models built in <a href="https://fluxml.ai/">Flux.jl</a>, which is Julia’s main DL library. As in the previous post on Bayesian logistic regression I will rely on Julia code snippits instead of equations to convey the underlying maths. If you’re curious about the maths, the <a href="https://arxiv.org/pdf/2106.14806.pdf">NeurIPS 2021 paper</a> provides all the detail you need.</p>
<section id="from-bayesian-logistic-regression" class="level3">
<h3 class="anchored" data-anchor-id="from-bayesian-logistic-regression">From Bayesian Logistic Regression …</h3>
<p>Let’s recap: in the case of logistic regression we had a assumed a zero-mean Gaussian prior <span class="math inline">\(p(\mathbf{w}) \sim \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)=\mathcal{N} \left( \mathbf{w} | \mathbf{0}, \mathbf{H}_0^{-1} \right)\)</span> for the weights that are used to compute logits <span class="math inline">\(\mu_n=\mathbf{w}^T\mathbf{x}_n\)</span>, which in turn are fed to a sigmoid function to produce probabilities <span class="math inline">\(p(y_n=1)=\sigma(\mu_n)\)</span>. We saw that under this assumption solving the logistic regression problem corresponds to minimizing the following differentiable loss function:</p>
<p><span class="math display">\[
\ell(\mathbf{w})= - \sum_{n}^N [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \\ \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0)
\]</span></p>
<p>As our first step towards Bayesian deep learning, we observe the following: the loss function above corresponds to the objective faced by a single-layer artificial neural network with sigmoid activation and weight decay<a href="#fn4" class="footnote-ref" id="fnref4" role="doc-noteref"><sup>4</sup></a>. In other words, regularized logistic regression is equivalent to a very simple neural network architecture and hence it is not surprising that underlying concepts can in theory be applied in much the same way.</p>
<p>So let’s quickly recap the next core concept: LA relies on the fact that the second-order Taylor expansion of our loss function <span class="math inline">\(\ell\)</span> evaluated at the <strong>maximum a posteriori</strong> (MAP) estimate <span class="math inline">\(\mathbf{\hat{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})\)</span> amounts to a multi-variate Gaussian distribution. In particular, that Gaussian is centered around the MAP estimate with covariance equal to the inverse Hessian evaluated at the mode <span class="math inline">\(\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{w}}))^{-1}\)</span> <span class="citation" data-cites="murphy2022probabilistic">(<a href="#ref-murphy2022probabilistic" role="doc-biblioref">Murphy 2022</a>)</span>.</p>
<p>That is basically all there is to the story: if we have a good estimate of <span class="math inline">\(\mathbf{H}(\mathbf{\hat{w}})\)</span> we have an analytical expression for an (approximate) posterior over parameters. So let’s go ahead and start by run Bayesian Logistic regression using <a href="https://fluxml.ai/">Flux.jl</a>. We begin by loading some required packages including <a href="https://www.paltmeyer.com/LaplaceRedux.jl/dev/">LaplaceRedux.jl</a>. It ships with a helper function <code>toy_data_linear</code> that creates a toy data set composed of linearly separable samples evenly balanced across the two classes.</p>
<div class="cell" data-execution_count="1">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1"></a><span class="co"># Import libraries.</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="im">using</span> <span class="bu">Flux</span>, <span class="bu">Plots</span>, <span class="bu">Random</span>, <span class="bu">PlotThemes</span>, <span class="bu">Statistics</span>, <span class="bu">LaplaceRedux</span></span>
<span id="cb1-3"><a href="#cb1-3"></a><span class="fu">theme</span>(<span class="op">:</span>wong)</span>
<span id="cb1-4"><a href="#cb1-4"></a><span class="co"># Number of points to generate.</span></span>
<span id="cb1-5"><a href="#cb1-5"></a>xs, y <span class="op">=</span> <span class="fu">toy_data_linear</span>(<span class="fl">100</span>)</span>
<span id="cb1-6"><a href="#cb1-6"></a>X <span class="op">=</span> <span class="fu">hcat</span>(xs<span class="op">...</span>); <span class="co"># bring into tabular format</span></span>
<span id="cb1-7"><a href="#cb1-7"></a>data <span class="op">=</span> <span class="fu">zip</span>(xs,y);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Then we proceed to prepare the single-layer neural network with weight decay. The term <span class="math inline">\(\lambda\)</span> determines the strength of the <span class="math inline">\(\ell2\)</span> penalty: we regularize parameters <span class="math inline">\(\theta\)</span> more heavily for higher values. Equivalently, we can say that from the Bayesian perspective it governs the strength of the prior <span class="math inline">\(p(\theta) \sim \mathcal{N} \left( \theta | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)= \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \lambda_0^{-2} \mathbf{I} \right)\)</span>: a higher value of <span class="math inline">\(\lambda\)</span> indicates a higher conviction about our prior belief that <span class="math inline">\(\theta=\mathbf{0}\)</span>, which is of course equivalent to regularizing more heavily. The exact choice of <span class="math inline">\(\lambda=0.5\)</span> for this toy example is somewhat arbitrary (it made for good visualizations below). Note that I have used <span class="math inline">\(\theta\)</span> to denote our neural parameters to distinguish the case from Bayesian logistic regression, but we are in fact still solving the same problem.</p>
<div class="cell" data-execution_count="2">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1"></a>nn <span class="op">=</span> <span class="fu">Chain</span>(<span class="fu">Dense</span>(<span class="fl">2</span>,<span class="fl">1</span>))</span>
<span id="cb2-2"><a href="#cb2-2"></a>λ <span class="op">=</span> <span class="fl">0.5</span></span>
<span id="cb2-3"><a href="#cb2-3"></a><span class="fu">sqnorm</span>(x) <span class="op">=</span> <span class="fu">sum</span>(abs2, x)</span>
<span id="cb2-4"><a href="#cb2-4"></a><span class="fu">weight_regularization</span>(λ<span class="op">=</span>λ) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> λ<span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="fu">sum</span>(sqnorm, Flux.<span class="fu">params</span>(nn))</span>
<span id="cb2-5"><a href="#cb2-5"></a><span class="fu">loss</span>(x, y) <span class="op">=</span> Flux.Losses.<span class="fu">logitbinarycrossentropy</span>(<span class="fu">nn</span>(x), y) <span class="op">+</span> <span class="fu">weight_regularization</span>();</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Before we apply Laplace approximation we train our model:</p>
<div class="cell" data-execution_count="3">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">using</span> <span class="bu">Flux.Optimise</span>: update!, ADAM</span>
<span id="cb3-2"><a href="#cb3-2"></a>opt <span class="op">=</span> <span class="fu">ADAM</span>()</span>
<span id="cb3-3"><a href="#cb3-3"></a>epochs <span class="op">=</span> <span class="fl">50</span></span>
<span id="cb3-4"><a href="#cb3-4"></a></span>
<span id="cb3-5"><a href="#cb3-5"></a><span class="cf">for</span> epoch <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb3-6"><a href="#cb3-6"></a>  <span class="cf">for</span> d <span class="kw">in</span> data</span>
<span id="cb3-7"><a href="#cb3-7"></a>    gs <span class="op">=</span> <span class="fu">gradient</span>(<span class="fu">params</span>(nn)) <span class="cf">do</span></span>
<span id="cb3-8"><a href="#cb3-8"></a>      l <span class="op">=</span> <span class="fu">loss</span>(d<span class="op">...</span>)</span>
<span id="cb3-9"><a href="#cb3-9"></a>    <span class="cf">end</span></span>
<span id="cb3-10"><a href="#cb3-10"></a>    <span class="fu">update!</span>(opt, <span class="fu">params</span>(nn), gs)</span>
<span id="cb3-11"><a href="#cb3-11"></a>  <span class="cf">end</span></span>
<span id="cb3-12"><a href="#cb3-12"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Up until this point we have just followed the standard recipe for training a regularized artificial neural network in <a href="https://fluxml.ai/">Flux.jl</a> for a simple binary classification task. To compute the Laplace approximation using <a href="https://www.paltmeyer.com/LaplaceRedux.jl/dev/">LaplaceRedux.jl</a> we need just two more lines of code:</p>
<div class="cell" data-execution_count="4">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb4-1"><a href="#cb4-1"></a>la <span class="op">=</span> <span class="fu">laplace</span>(nn, λ<span class="op">=</span>λ)</span>
<span id="cb4-2"><a href="#cb4-2"></a><span class="fu">fit!</span>(la, data);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Under the hood the Hessian is approximated through the <strong>empirical Fisher</strong>, which can be computed using only the gradients of our loss function <span class="math inline">\(\nabla_{\theta}\ell(f(\mathbf{x}_n;\theta,y_n))\)</span> where <span class="math inline">\(\{\mathbf{x}_n,y_n\}\)</span> are training data (see <a href="https://arxiv.org/pdf/2106.14806.pdf">NeurIPS 2021 paper</a> for details). Finally, <a href="https://www.paltmeyer.com/LaplaceRedux.jl/dev/">LaplaceRedux.jl</a> ships with a function <code>predict(𝑳::LaplaceRedux, X::AbstractArray; link_approx=:probit)</code> that computes the posterior predictive using a probit approximation, much like we saw in the previous post. That function is used under the hood of the <code>plot_contour</code> function below to create the right panel of <a href="#fig-logit" class="quarto-xref">Figure&nbsp;1</a>. It visualizes the posterior predictive distribution in the 2D feature space. For comparison I have added the corresponding plugin estimate as well. Note how for the Laplace approximation the predicted probabilities fan out indicating that confidence decreases in regions scarce of data.</p>
<div class="cell" data-execution_count="5">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1"></a>p_plugin <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Plugin"</span>,<span class="kw">type</span><span class="op">=:</span>plugin);</span>
<span id="cb5-2"><a href="#cb5-2"></a>p_laplace <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Laplace"</span>)</span>
<span id="cb5-3"><a href="#cb5-3"></a><span class="co"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb5-4"><a href="#cb5-4"></a>plt <span class="op">=</span> <span class="fu">plot</span>(p_plugin, p_laplace, layout<span class="op">=</span>(<span class="fl">1</span>,<span class="fl">2</span>), size<span class="op">=</span>(<span class="fl">1000</span>,<span class="fl">400</span>))</span>
<span id="cb5-5"><a href="#cb5-5"></a><span class="fu">savefig</span>(plt, <span class="st">"www/posterior_predictive_logit.png"</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fig-logit" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-logit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="www/posterior_predictive_logit.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-logit-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).
</figcaption>
</figure>
</div>
</section>
<section id="to-bayesian-neural-networks" class="level3">
<h3 class="anchored" data-anchor-id="to-bayesian-neural-networks">… to Bayesian Neural Networks</h3>
<p>Now let’s step it up a notch: we will repeat the exercise from above, but this time for data that is not linearly separable using a simple MLP instead of the single-layer neural network we used above. The code below is almost the same as above, so I will not go through the various steps again.</p>
<div class="cell" data-execution_count="6">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb6-1"><a href="#cb6-1"></a><span class="co"># Number of points to generate:</span></span>
<span id="cb6-2"><a href="#cb6-2"></a>xs, y <span class="op">=</span> <span class="fu">toy_data_non_linear</span>(<span class="fl">200</span>)</span>
<span id="cb6-3"><a href="#cb6-3"></a>X <span class="op">=</span> <span class="fu">hcat</span>(xs<span class="op">...</span>); <span class="co"># bring into tabular format</span></span>
<span id="cb6-4"><a href="#cb6-4"></a>data <span class="op">=</span> <span class="fu">zip</span>(xs,y)</span>
<span id="cb6-5"><a href="#cb6-5"></a></span>
<span id="cb6-6"><a href="#cb6-6"></a><span class="co"># Build MLP:</span></span>
<span id="cb6-7"><a href="#cb6-7"></a>n_hidden <span class="op">=</span> <span class="fl">32</span></span>
<span id="cb6-8"><a href="#cb6-8"></a>D <span class="op">=</span> <span class="fu">size</span>(X)[<span class="fl">1</span>]</span>
<span id="cb6-9"><a href="#cb6-9"></a>nn <span class="op">=</span> <span class="fu">Chain</span>(</span>
<span id="cb6-10"><a href="#cb6-10"></a>    <span class="fu">Dense</span>(D, n_hidden, σ),</span>
<span id="cb6-11"><a href="#cb6-11"></a>    <span class="fu">Dense</span>(n_hidden, <span class="fl">1</span>)</span>
<span id="cb6-12"><a href="#cb6-12"></a>)  </span>
<span id="cb6-13"><a href="#cb6-13"></a>λ <span class="op">=</span> <span class="fl">0.01</span></span>
<span id="cb6-14"><a href="#cb6-14"></a><span class="fu">sqnorm</span>(x) <span class="op">=</span> <span class="fu">sum</span>(abs2, x)</span>
<span id="cb6-15"><a href="#cb6-15"></a><span class="fu">weight_regularization</span>(λ<span class="op">=</span>λ) <span class="op">=</span> <span class="fl">1</span><span class="op">/</span><span class="fl">2</span> <span class="op">*</span> λ<span class="op">^</span><span class="fl">2</span> <span class="op">*</span> <span class="fu">sum</span>(sqnorm, Flux.<span class="fu">params</span>(nn))</span>
<span id="cb6-16"><a href="#cb6-16"></a><span class="fu">loss</span>(x, y) <span class="op">=</span> Flux.Losses.<span class="fu">logitbinarycrossentropy</span>(<span class="fu">nn</span>(x), y) <span class="op">+</span> <span class="fu">weight_regularization</span>()</span>
<span id="cb6-17"><a href="#cb6-17"></a></span>
<span id="cb6-18"><a href="#cb6-18"></a><span class="co"># Training:</span></span>
<span id="cb6-19"><a href="#cb6-19"></a>epochs <span class="op">=</span> <span class="fl">200</span></span>
<span id="cb6-20"><a href="#cb6-20"></a><span class="cf">for</span> epoch <span class="op">=</span> <span class="fl">1</span><span class="op">:</span>epochs</span>
<span id="cb6-21"><a href="#cb6-21"></a>  <span class="cf">for</span> d <span class="kw">in</span> data</span>
<span id="cb6-22"><a href="#cb6-22"></a>    gs <span class="op">=</span> <span class="fu">gradient</span>(<span class="fu">params</span>(nn)) <span class="cf">do</span></span>
<span id="cb6-23"><a href="#cb6-23"></a>      l <span class="op">=</span> <span class="fu">loss</span>(d<span class="op">...</span>)</span>
<span id="cb6-24"><a href="#cb6-24"></a>    <span class="cf">end</span></span>
<span id="cb6-25"><a href="#cb6-25"></a>    <span class="fu">update!</span>(opt, <span class="fu">params</span>(nn), gs)</span>
<span id="cb6-26"><a href="#cb6-26"></a>  <span class="cf">end</span></span>
<span id="cb6-27"><a href="#cb6-27"></a><span class="cf">end</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Fitting the Laplace approximation is also analogous, but note that this we have added an argument: <code>subset_of_weights=:last_layer</code>. This specifies that we only want to use the parameters of the last layer of our MLP. While we could have used all of them (<code>subset_of_weights=:all</code>), <span class="citation" data-cites="daxberger2021laplace">Daxberger et al. (<a href="#ref-daxberger2021laplace" role="doc-biblioref">2021</a>)</span> find that the last-layer Laplace approximation produces satisfying results, while be computationally cheaper. <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;2</a> demonstrates that once again the Laplace approximation yields a posterior predictive distribution that is more conservative than the over-confident plugin estimate.</p>
<div class="cell" data-execution_count="7">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1"></a>la <span class="op">=</span> <span class="fu">laplace</span>(nn, λ<span class="op">=</span>λ, subset_of_weights<span class="op">=:</span>last_layer)</span>
<span id="cb7-2"><a href="#cb7-2"></a><span class="fu">fit!</span>(la, data);</span>
<span id="cb7-3"><a href="#cb7-3"></a>p_plugin <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Plugin"</span>,<span class="kw">type</span><span class="op">=:</span>plugin)</span>
<span id="cb7-4"><a href="#cb7-4"></a>p_laplace <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Laplace"</span>)</span>
<span id="cb7-5"><a href="#cb7-5"></a><span class="co"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb7-6"><a href="#cb7-6"></a>plt <span class="op">=</span> <span class="fu">plot</span>(p_plugin, p_laplace, layout<span class="op">=</span>(<span class="fl">1</span>,<span class="fl">2</span>), size<span class="op">=</span>(<span class="fl">1000</span>,<span class="fl">400</span>))</span>
<span id="cb7-7"><a href="#cb7-7"></a><span class="fu">savefig</span>(plt, <span class="st">"www/posterior_predictive_mlp.png"</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fig-mlp" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="www/posterior_predictive_mlp.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).
</figcaption>
</figure>
</div>
<p>To see why this is a desirable outcome consider the zoomed out version of <a href="#fig-mlp" class="quarto-xref">Figure&nbsp;2</a> below: the plugin estimator classifies with full confidence in regions completely scarce of any data. Arguably Laplace approximation produces a much more reasonable picture, even though it too could likely be improved by fine-tuning our choice of <span class="math inline">\(\lambda\)</span> and the neural network architecture.</p>
<div class="cell" data-execution_count="8">
<details class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb8-1"><a href="#cb8-1"></a>zoom<span class="op">=-</span><span class="fl">50</span></span>
<span id="cb8-2"><a href="#cb8-2"></a>p_plugin <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Plugin"</span>,<span class="kw">type</span><span class="op">=:</span>plugin,zoom<span class="op">=</span>zoom);</span>
<span id="cb8-3"><a href="#cb8-3"></a>p_laplace <span class="op">=</span> <span class="fu">plot_contour</span>(X<span class="op">'</span>,y,la;title<span class="op">=</span><span class="st">"Laplace"</span>,zoom<span class="op">=</span>zoom);</span>
<span id="cb8-4"><a href="#cb8-4"></a><span class="co"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb8-5"><a href="#cb8-5"></a>plt <span class="op">=</span> <span class="fu">plot</span>(p_plugin, p_laplace, layout<span class="op">=</span>(<span class="fl">1</span>,<span class="fl">2</span>), size<span class="op">=</span>(<span class="fl">1000</span>,<span class="fl">400</span>));</span>
<span id="cb8-6"><a href="#cb8-6"></a><span class="fu">savefig</span>(plt, <span class="st">"www/posterior_predictive_mlp_zoom.png"</span>);</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div id="fig-mlp-zoom" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-mlp-zoom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="www/posterior_predictive_mlp_zoom.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-mlp-zoom-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="wrapping-up" class="level2">
<h2 class="anchored" data-anchor-id="wrapping-up">Wrapping up</h2>
<p>Recent state-of-the-art research on neural information processing suggests that Bayesian deep learning can be effortless: Laplace approximation for deep neural networks appears to work very well and it does so at minimal computational cost <span class="citation" data-cites="daxberger2021laplace">(<a href="#ref-daxberger2021laplace" role="doc-biblioref">Daxberger et al. 2021</a>)</span>. This is great news, because the case for turning Bayesian is strong: society increasingly relies on complex automated decision-making systems that need to be trustworthy. More and more of these systems involve deep learning which in and of itself is not trustworthy. We have seen that typically there exist various viable parameterizations of deep neural networks each with their own distinct and compelling explanation for the data at hand. When faced with many viable options, don’t put all of your eggs in one basket. In other words, go Bayesian!</p>
</section>
<section id="resources" class="level2">
<h2 class="anchored" data-anchor-id="resources">Resources</h2>
<p>To get started with Bayesian deep learning I have found many useful and free resources online, some of which are listed below:</p>
<ul>
<li><a href="https://turing.ml/dev/tutorials/03-bayesian-neural-network/"><code>Turing.jl</code> tutorial</a> on Bayesian deep learning in Julia.</li>
<li>Various RStudio AI blog posts including <a href="https://blogs.rstudio.com/ai/posts/2018-11-12-uncertainty_estimates_dropout/">this one</a> and <a href="https://blogs.rstudio.com/ai/posts/2019-06-05-uncertainty-estimates-tfprobability/">this one</a>.</li>
<li><a href="https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf">TensorFlow blog post</a> on regression with probabilistic layers.</li>
<li>Kevin Murphy’s <a href="https://probml.github.io/pml-book/book1.html">draft text book</a>, now also available as print.</li>
</ul>
</section>
<section id="references" class="level2">



<!-- -->


</section>


<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-bastounis2021mathematics" class="csl-entry" role="listitem">
Bastounis, Alexander, Anders C Hansen, and Verner Vlačić. 2021. <span>“The Mathematics of Adversarial Attacks in <span>AI</span>–<span>Why</span> Deep Learning Is Unstable Despite the Existence of Stable Neural Networks.”</span> <a href="https://arxiv.org/abs/2109.06098">https://arxiv.org/abs/2109.06098</a>.
</div>
<div id="ref-daxberger2021laplace" class="csl-entry" role="listitem">
Daxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. 2021. <span>“Laplace <span>Redux-Effortless Bayesian Deep Learning</span>.”</span> <em>Advances in Neural Information Processing Systems</em> 34.
</div>
<div id="ref-gal2016dropout" class="csl-entry" role="listitem">
Gal, Yarin, and Zoubin Ghahramani. 2016. <span>“Dropout as a Bayesian Approximation: <span>Representing</span> Model Uncertainty in Deep Learning.”</span> In <em>International Conference on Machine Learning</em>, 1050–59. <span>PMLR</span>.
</div>
<div id="ref-lakshminarayanan2016simple" class="csl-entry" role="listitem">
Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2017. <span>“Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.”</span> <em>Advances in Neural Information Processing Systems</em> 30.
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry" role="listitem">
Murphy, Kevin P. 2022. <em>Probabilistic <span>Machine Learning</span>: <span>An</span> Introduction</em>. <span>MIT Press</span>.
</div>
<div id="ref-pearl2018book" class="csl-entry" role="listitem">
Pearl, Judea, and Dana Mackenzie. 2018. <em>The Book of Why: The New Science of Cause and Effect</em>. <span>Basic books</span>.
</div>
<div id="ref-raghunathan2019adversarial" class="csl-entry" role="listitem">
Raghunathan, Aditi, Sang Michael Xie, Fanny Yang, John C Duchi, and Percy Liang. 2019. <span>“Adversarial Training Can Hurt Generalization.”</span> <a href="https://arxiv.org/abs/1906.06032">https://arxiv.org/abs/1906.06032</a>.
</div>
<div id="ref-slack2020fooling" class="csl-entry" role="listitem">
Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. <span>“Fooling Lime and Shap: <span>Adversarial</span> Attacks on Post Hoc Explanation Methods.”</span> In <em>Proceedings of the <span>AAAI</span>/<span>ACM Conference</span> on <span>AI</span>, <span>Ethics</span>, and <span>Society</span></em>, 180–86.
</div>
<div id="ref-wilson2020case" class="csl-entry" role="listitem">
Wilson, Andrew Gordon. 2020. <span>“The Case for Bayesian Deep Learning.”</span> <a href="https://arxiv.org/abs/2001.10995">https://arxiv.org/abs/2001.10995</a>.
</div>
</div></section><section id="footnotes" class="footnotes footnotes-end-of-document" role="doc-endnotes"><h2 class="anchored quarto-appendix-heading">Footnotes</h2>

<ol>
<li id="fn1"><p>See for example <a href="https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/">this article</a> in the MIT Technology Review<a href="#fnref1" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn2"><p>In fact, not treating probabilistic deep learning models as such is sheer madness because remember that the underlying parameters <span class="math inline">\(\theta\)</span> are random variables. Frequentists and Bayesians alike will tell you that relying on a single point estimate of random variables is just nuts!<a href="#fnref2" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn3"><p>Proponents of Causal AI like Judea Pearl would argue that the Bayesian treatment still does not go far enough: in their view model explanations can only be truly compelling if they are causally found.<a href="#fnref3" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
<li id="fn4"><p>See this <a href="https://stats.stackexchange.com/a/500973/288736">answer</a> on Stack Exchange for a detailed discussion.<a href="#fnref4" class="footnote-back" role="doc-backlink">↩︎</a></p></li>
</ol>
</section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{altmeyer2022,
  author = {Altmeyer, Patrick},
  title = {Go Deep, but Also ... Go {Bayesian!}},
  date = {2022-02-18},
  url = {https://www.paltmeyer.com/blog//blog/posts/effortsless-bayesian-dl},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-altmeyer2022" class="csl-entry quarto-appendix-citeas" role="listitem">
Altmeyer, Patrick. 2022. <span>“Go Deep, but Also ... Go
Bayesian!”</span> February 18, 2022. <a href="https://www.paltmeyer.com/blog//blog/posts/effortsless-bayesian-dl">https://www.paltmeyer.com/blog//blog/posts/effortsless-bayesian-dl</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="pat-alt/pat-alt.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb9" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb9-1"><a href="#cb9-1"></a><span class="co">---</span></span>
<span id="cb9-2"><a href="#cb9-2"></a><span class="an">title:</span><span class="co"> 'Go deep, but also ... go Bayesian!'</span></span>
<span id="cb9-3"><a href="#cb9-3"></a><span class="an">subtitle:</span><span class="co"> Effortless Bayesian Deep Learning in Julia --- Part I</span></span>
<span id="cb9-4"><a href="#cb9-4"></a><span class="an">date:</span><span class="co"> '2022-02-18'</span></span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="an">categories:</span></span>
<span id="cb9-6"><a href="#cb9-6"></a><span class="co">  - bayes</span></span>
<span id="cb9-7"><a href="#cb9-7"></a><span class="co">  - deep learning</span></span>
<span id="cb9-8"><a href="#cb9-8"></a><span class="co">  - Julia</span></span>
<span id="cb9-9"><a href="#cb9-9"></a><span class="an">description:</span><span class="co"> &gt;-</span></span>
<span id="cb9-10"><a href="#cb9-10"></a><span class="co">  An introduction to effortless Bayesian deep learning through Laplace</span></span>
<span id="cb9-11"><a href="#cb9-11"></a><span class="co">  approximation coded from scratch in Julia. </span></span>
<span id="cb9-12"><a href="#cb9-12"></a><span class="an">image:</span><span class="co"> www/intro.gif</span></span>
<span id="cb9-13"><a href="#cb9-13"></a><span class="an">jupyter:</span><span class="co"> julia-1.7</span></span>
<span id="cb9-14"><a href="#cb9-14"></a><span class="an">execute:</span><span class="co"> </span></span>
<span id="cb9-15"><a href="#cb9-15"></a><span class="co">  eval: false</span></span>
<span id="cb9-16"><a href="#cb9-16"></a><span class="co">---</span></span>
<span id="cb9-17"><a href="#cb9-17"></a></span>
<span id="cb9-18"><a href="#cb9-18"></a>&lt;div class="intro-gif"&gt;</span>
<span id="cb9-19"><a href="#cb9-19"></a>  &lt;figure&gt;</span>
<span id="cb9-20"><a href="#cb9-20"></a>    &lt;img src="www/intro.gif"&gt;</span>
<span id="cb9-21"><a href="#cb9-21"></a>    &lt;figcaption&gt;A Bayesian Neural Network gradually learns.&lt;/figcaption&gt;</span>
<span id="cb9-22"><a href="#cb9-22"></a>  &lt;/figure&gt;</span>
<span id="cb9-23"><a href="#cb9-23"></a>&lt;/div&gt;</span>
<span id="cb9-24"><a href="#cb9-24"></a></span>
<span id="cb9-25"><a href="#cb9-25"></a>Deep learning has dominated AI research in recent years^<span class="co">[</span><span class="ot">See for example [this article](https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/) in the MIT Technology Review</span><span class="co">]</span> - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on <span class="co">[</span><span class="ot">Twitter</span><span class="co">](https://twitter.com/ilyasut/status/1491554478243258368)</span>. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slightly conscious - that's "may" and "slightly" and only if you just go deep enough? On the other side you have prominent skeptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent <span class="co">[</span><span class="ot">@pearl2018book</span><span class="co">]</span>. </span>
<span id="cb9-26"><a href="#cb9-26"></a></span>
<span id="cb9-27"><a href="#cb9-27"></a><span class="fu">## The case for Bayesian Deep Learning</span></span>
<span id="cb9-28"><a href="#cb9-28"></a></span>
<span id="cb9-29"><a href="#cb9-29"></a>Whatever side of this entertaining twitter dispute you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization <span class="co">[</span><span class="ot">@raghunathan2019adversarial</span><span class="co">]</span> and does ultimately not guarantee stability <span class="co">[</span><span class="ot">@bastounis2021mathematics</span><span class="co">]</span>. With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness <span class="co">[</span><span class="ot">@slack2020fooling</span><span class="co">]</span>. </span>
<span id="cb9-30"><a href="#cb9-30"></a></span>
<span id="cb9-31"><a href="#cb9-31"></a>Exactly why are deep neural networks unstable and in-transparent? Let $\mathcal{D}=<span class="sc">\{</span>x,y<span class="sc">\}</span>_{n=1}^N$ denote our feature-label pairs and let $f(x;\theta)=y$ denote some deep neural network specified by its parameters $\theta$. Then the first thing to note is that the number of free parameters $\theta$ is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters *relative* to the size of $\mathcal{D}$ is generally huge: </span>
<span id="cb9-32"><a href="#cb9-32"></a></span>
<span id="cb9-33"><a href="#cb9-33"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">...</span><span class="co">]</span><span class="at"> deep neural networks are typically very underspecified by the available data, and </span><span class="co">[</span><span class="ot">...</span><span class="co">]</span><span class="at"> parameters </span><span class="co">[</span><span class="ot">therefore</span><span class="co">]</span><span class="at"> correspond to a diverse variety of compelling explanations for the data. </span></span>
<span id="cb9-34"><a href="#cb9-34"></a><span class="at">&gt; </span><span class="co">[</span><span class="ot">@wilson2020case</span><span class="co">]</span></span>
<span id="cb9-35"><a href="#cb9-35"></a></span>
<span id="cb9-36"><a href="#cb9-36"></a>In other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such "compelling explanation" for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:</span>
<span id="cb9-37"><a href="#cb9-37"></a></span>
<span id="cb9-38"><a href="#cb9-38"></a><span class="at">&gt; How compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations? </span></span>
<span id="cb9-39"><a href="#cb9-39"></a> </span>
<span id="cb9-40"><a href="#cb9-40"></a>A scenario like this very much calls for treating predictions from deep learning models probabilistically <span class="co">[</span><span class="ot">@wilson2020case</span><span class="co">][^fn1]</span><span class="ot">[^fn2]</span>.</span>
<span id="cb9-41"><a href="#cb9-41"></a></span>
<span id="cb9-42"><a href="#cb9-42"></a><span class="ot">[^fn1]: </span>In fact, not treating probabilistic deep learning models as such is sheer madness because remember that the underlying parameters $\theta$ are random variables. Frequentists and Bayesians alike will tell you that relying on a single point estimate of random variables is just nuts!</span>
<span id="cb9-43"><a href="#cb9-43"></a></span>
<span id="cb9-44"><a href="#cb9-44"></a><span class="ot">[^fn2]: </span>Proponents of Causal AI like Judea Pearl would argue that the Bayesian treatment still does not go far enough: in their view model explanations can only be truly compelling if they are causally found.</span>
<span id="cb9-45"><a href="#cb9-45"></a></span>
<span id="cb9-46"><a href="#cb9-46"></a>Formally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):</span>
<span id="cb9-47"><a href="#cb9-47"></a></span>
<span id="cb9-48"><a href="#cb9-48"></a>$$</span>
<span id="cb9-49"><a href="#cb9-49"></a>p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta</span>
<span id="cb9-50"><a href="#cb9-50"></a>$$</span>
<span id="cb9-51"><a href="#cb9-51"></a></span>
<span id="cb9-52"><a href="#cb9-52"></a>The integral implies that we essentially need many predictions from many different specifications of $\theta$. Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above in recent years: @gal2016dropout propose using dropout at test time while @lakshminarayanan2016simple show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: **Laplace approximation** (LA).</span>
<span id="cb9-53"><a href="#cb9-53"></a></span>
<span id="cb9-54"><a href="#cb9-54"></a>If you have read my <span class="co">[</span><span class="ot">previous post</span><span class="co">](https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f)</span> on Bayesian Logistic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning.</span>
<span id="cb9-55"><a href="#cb9-55"></a></span>
<span id="cb9-56"><a href="#cb9-56"></a><span class="fu">## Laplace Approximation</span></span>
<span id="cb9-57"><a href="#cb9-57"></a></span>
<span id="cb9-58"><a href="#cb9-58"></a>While LA was first proposed in the 18th century, it has so far not attracted serious attention from the deep learning community largely because it involves a possibly large Hessian computation. @daxberger2021laplace are on a mission to change the perception that LA has no use in DL: in their <span class="co">[</span><span class="ot">NeurIPS 2021 paper</span><span class="co">](https://arxiv.org/pdf/2106.14806.pdf)</span> they demonstrate empirically that LA can be used to produce Bayesian model averages that are at least at par with existing approaches in terms of uncertainty quantification and out-of-distribution detection and significantly cheaper to compute. They show that recent advancements in autodifferentation can be leveraged to produce fast and accurate approximations of the Hessian and even provide a fully-fledged <span class="co">[</span><span class="ot">Python library</span><span class="co">](https://aleximmer.github.io/Laplace/)</span> that can be used with any pretrained Torch model. For this post, I have built a much less comprehensive, pure-play equivalent of their package in Julia - <span class="co">[</span><span class="ot">LaplaceRedux.jl</span><span class="co">](https://www.paltmeyer.com/LaplaceRedux.jl/dev/)</span> can be used with deep learning models built in <span class="co">[</span><span class="ot">Flux.jl</span><span class="co">](https://fluxml.ai/)</span>, which is Julia's main DL library. As in the previous post on Bayesian logistic regression I will rely on Julia code snippits instead of equations to convey the underlying maths. If you're curious about the maths, the <span class="co">[</span><span class="ot">NeurIPS 2021 paper</span><span class="co">](https://arxiv.org/pdf/2106.14806.pdf)</span> provides all the detail you need.</span>
<span id="cb9-59"><a href="#cb9-59"></a></span>
<span id="cb9-60"><a href="#cb9-60"></a><span class="fu">### From Bayesian Logistic Regression ...</span></span>
<span id="cb9-61"><a href="#cb9-61"></a></span>
<span id="cb9-62"><a href="#cb9-62"></a>Let's recap: in the case of logistic regression we had a assumed a zero-mean Gaussian prior $p(\mathbf{w}) \sim \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)=\mathcal{N} \left( \mathbf{w} | \mathbf{0}, \mathbf{H}_0^{-1} \right)$ for the weights that are used to compute logits $\mu_n=\mathbf{w}^T\mathbf{x}_n$, which in turn are fed to a sigmoid function to produce probabilities $p(y_n=1)=\sigma(\mu_n)$. We saw that under this assumption solving the logistic regression problem corresponds to minimizing the following differentiable loss function:</span>
<span id="cb9-63"><a href="#cb9-63"></a></span>
<span id="cb9-64"><a href="#cb9-64"></a>$$</span>
<span id="cb9-65"><a href="#cb9-65"></a>\ell(\mathbf{w})= - \sum_{n}^N <span class="co">[</span><span class="ot">y_n \log \mu_n + (1-y_n)\log (1-\mu_n)</span><span class="co">]</span> + <span class="sc">\\</span> \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0)</span>
<span id="cb9-66"><a href="#cb9-66"></a>$$</span>
<span id="cb9-67"><a href="#cb9-67"></a></span>
<span id="cb9-68"><a href="#cb9-68"></a>As our first step towards Bayesian deep learning, we observe the following: the loss function above corresponds to the objective faced by a single-layer artificial neural network with sigmoid activation and weight decay^<span class="co">[</span><span class="ot">See this [answer](https://stats.stackexchange.com/a/500973/288736) on Stack Exchange for a detailed discussion.</span><span class="co">]</span>. In other words, regularized logistic regression is equivalent to a very simple neural network architecture and hence it is not surprising that underlying concepts can in theory be applied in much the same way. </span>
<span id="cb9-69"><a href="#cb9-69"></a></span>
<span id="cb9-70"><a href="#cb9-70"></a>So let's quickly recap the next core concept: LA relies on the fact that the second-order Taylor expansion of our loss function $\ell$ evaluated at the **maximum a posteriori** (MAP) estimate $\mathbf{\hat{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})$ amounts to a multi-variate Gaussian distribution. In particular, that Gaussian is centered around the MAP estimate with covariance equal to the inverse Hessian evaluated at the mode $\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{w}}))^{-1}$ <span class="co">[</span><span class="ot">@murphy2022probabilistic</span><span class="co">]</span>. </span>
<span id="cb9-71"><a href="#cb9-71"></a></span>
<span id="cb9-72"><a href="#cb9-72"></a>That is basically all there is to the story: if we have a good estimate of $\mathbf{H}(\mathbf{\hat{w}})$ we have an analytical expression for an (approximate) posterior over parameters. So let's go ahead and start by run Bayesian Logistic regression using <span class="co">[</span><span class="ot">Flux.jl</span><span class="co">](https://fluxml.ai/)</span>. We begin by loading some required packages including <span class="co">[</span><span class="ot">LaplaceRedux.jl</span><span class="co">](https://www.paltmeyer.com/LaplaceRedux.jl/dev/)</span>. It ships with a helper function <span class="in">`toy_data_linear`</span> that creates a toy data set composed of linearly separable samples evenly balanced across the two classes.</span>
<span id="cb9-73"><a href="#cb9-73"></a></span>
<span id="cb9-76"><a href="#cb9-76"></a><span class="in">```{julia}</span></span>
<span id="cb9-77"><a href="#cb9-77"></a><span class="in"># Import libraries.</span></span>
<span id="cb9-78"><a href="#cb9-78"></a><span class="in">using Flux, Plots, Random, PlotThemes, Statistics, LaplaceRedux</span></span>
<span id="cb9-79"><a href="#cb9-79"></a><span class="in">theme(:wong)</span></span>
<span id="cb9-80"><a href="#cb9-80"></a><span class="in"># Number of points to generate.</span></span>
<span id="cb9-81"><a href="#cb9-81"></a><span class="in">xs, y = toy_data_linear(100)</span></span>
<span id="cb9-82"><a href="#cb9-82"></a><span class="in">X = hcat(xs...); # bring into tabular format</span></span>
<span id="cb9-83"><a href="#cb9-83"></a><span class="in">data = zip(xs,y);</span></span>
<span id="cb9-84"><a href="#cb9-84"></a><span class="in">```</span></span>
<span id="cb9-85"><a href="#cb9-85"></a></span>
<span id="cb9-86"><a href="#cb9-86"></a>Then we proceed to prepare the single-layer neural network with weight decay. The term $\lambda$ determines the strength of the $\ell2$ penalty: we regularize parameters $\theta$ more heavily for higher values. Equivalently, we can say that from the Bayesian perspective it governs the strength of the prior $p(\theta) \sim \mathcal{N} \left( \theta | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)= \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \lambda_0^{-2} \mathbf{I} \right)$: a higher value of $\lambda$ indicates a higher conviction about our prior belief that $\theta=\mathbf{0}$, which is of course equivalent to regularizing more heavily. The exact choice of $\lambda=0.5$ for this toy example is somewhat arbitrary (it made for good visualizations below). Note that I have used $\theta$ to denote our neural parameters to distinguish the case from Bayesian logistic regression, but we are in fact still solving the same problem.</span>
<span id="cb9-87"><a href="#cb9-87"></a></span>
<span id="cb9-90"><a href="#cb9-90"></a><span class="in">```{julia}</span></span>
<span id="cb9-91"><a href="#cb9-91"></a><span class="in">nn = Chain(Dense(2,1))</span></span>
<span id="cb9-92"><a href="#cb9-92"></a><span class="in">λ = 0.5</span></span>
<span id="cb9-93"><a href="#cb9-93"></a><span class="in">sqnorm(x) = sum(abs2, x)</span></span>
<span id="cb9-94"><a href="#cb9-94"></a><span class="in">weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))</span></span>
<span id="cb9-95"><a href="#cb9-95"></a><span class="in">loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();</span></span>
<span id="cb9-96"><a href="#cb9-96"></a><span class="in">```</span></span>
<span id="cb9-97"><a href="#cb9-97"></a></span>
<span id="cb9-98"><a href="#cb9-98"></a>Before we apply Laplace approximation we train our model:</span>
<span id="cb9-99"><a href="#cb9-99"></a></span>
<span id="cb9-102"><a href="#cb9-102"></a><span class="in">```{julia}</span></span>
<span id="cb9-103"><a href="#cb9-103"></a><span class="in">using Flux.Optimise: update!, ADAM</span></span>
<span id="cb9-104"><a href="#cb9-104"></a><span class="in">opt = ADAM()</span></span>
<span id="cb9-105"><a href="#cb9-105"></a><span class="in">epochs = 50</span></span>
<span id="cb9-106"><a href="#cb9-106"></a></span>
<span id="cb9-107"><a href="#cb9-107"></a><span class="in">for epoch = 1:epochs</span></span>
<span id="cb9-108"><a href="#cb9-108"></a><span class="in">  for d in data</span></span>
<span id="cb9-109"><a href="#cb9-109"></a><span class="in">    gs = gradient(params(nn)) do</span></span>
<span id="cb9-110"><a href="#cb9-110"></a><span class="in">      l = loss(d...)</span></span>
<span id="cb9-111"><a href="#cb9-111"></a><span class="in">    end</span></span>
<span id="cb9-112"><a href="#cb9-112"></a><span class="in">    update!(opt, params(nn), gs)</span></span>
<span id="cb9-113"><a href="#cb9-113"></a><span class="in">  end</span></span>
<span id="cb9-114"><a href="#cb9-114"></a><span class="in">end</span></span>
<span id="cb9-115"><a href="#cb9-115"></a><span class="in">```</span></span>
<span id="cb9-116"><a href="#cb9-116"></a></span>
<span id="cb9-117"><a href="#cb9-117"></a>Up until this point we have just followed the standard recipe for training a regularized artificial neural network in <span class="co">[</span><span class="ot">Flux.jl</span><span class="co">](https://fluxml.ai/)</span> for a simple binary classification task. To compute the Laplace approximation using <span class="co">[</span><span class="ot">LaplaceRedux.jl</span><span class="co">](https://www.paltmeyer.com/LaplaceRedux.jl/dev/)</span> we need just two more lines of code:</span>
<span id="cb9-118"><a href="#cb9-118"></a></span>
<span id="cb9-121"><a href="#cb9-121"></a><span class="in">```{julia}</span></span>
<span id="cb9-122"><a href="#cb9-122"></a><span class="in">la = laplace(nn, λ=λ)</span></span>
<span id="cb9-123"><a href="#cb9-123"></a><span class="in">fit!(la, data);</span></span>
<span id="cb9-124"><a href="#cb9-124"></a><span class="in">```</span></span>
<span id="cb9-125"><a href="#cb9-125"></a></span>
<span id="cb9-126"><a href="#cb9-126"></a>Under the hood the Hessian is approximated through the **empirical Fisher**, which can be computed using only the gradients of our loss function $\nabla_{\theta}\ell(f(\mathbf{x}_n;\theta,y_n))$ where $<span class="sc">\{</span>\mathbf{x}_n,y_n<span class="sc">\}</span>$ are training data (see <span class="co">[</span><span class="ot">NeurIPS 2021 paper</span><span class="co">](https://arxiv.org/pdf/2106.14806.pdf)</span> for details). Finally, <span class="co">[</span><span class="ot">LaplaceRedux.jl</span><span class="co">](https://www.paltmeyer.com/LaplaceRedux.jl/dev/)</span> ships with a function <span class="in">`predict(𝑳::LaplaceRedux, X::AbstractArray; link_approx=:probit)`</span> that computes the posterior predictive using a probit approximation, much like we saw in the previous post. That function is used under the hood of the <span class="in">`plot_contour`</span> function below to create the right panel of @fig-logit. It visualizes the posterior predictive distribution in the 2D feature space. For comparison I have added the corresponding plugin estimate as well. Note how for the Laplace approximation the predicted probabilities fan out indicating that confidence decreases in regions scarce of data.</span>
<span id="cb9-127"><a href="#cb9-127"></a></span>
<span id="cb9-130"><a href="#cb9-130"></a><span class="in">```{julia}</span></span>
<span id="cb9-131"><a href="#cb9-131"></a><span class="in">p_plugin = plot_contour(X',y,la;title="Plugin",type=:plugin);</span></span>
<span id="cb9-132"><a href="#cb9-132"></a><span class="in">p_laplace = plot_contour(X',y,la;title="Laplace")</span></span>
<span id="cb9-133"><a href="#cb9-133"></a><span class="in"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb9-134"><a href="#cb9-134"></a><span class="in">plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))</span></span>
<span id="cb9-135"><a href="#cb9-135"></a><span class="in">savefig(plt, "www/posterior_predictive_logit.png");</span></span>
<span id="cb9-136"><a href="#cb9-136"></a><span class="in">```</span></span>
<span id="cb9-137"><a href="#cb9-137"></a></span>
<span id="cb9-138"><a href="#cb9-138"></a><span class="al">![Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).](www/posterior_predictive_logit.png)</span>{#fig-logit}</span>
<span id="cb9-139"><a href="#cb9-139"></a></span>
<span id="cb9-140"><a href="#cb9-140"></a><span class="fu">### ... to Bayesian Neural Networks</span></span>
<span id="cb9-141"><a href="#cb9-141"></a></span>
<span id="cb9-142"><a href="#cb9-142"></a>Now let's step it up a notch: we will repeat the exercise from above, but this time for data that is not linearly separable using a simple MLP instead of the single-layer neural network we used above. The code below is almost the same as above, so I will not go through the various steps again. </span>
<span id="cb9-143"><a href="#cb9-143"></a></span>
<span id="cb9-146"><a href="#cb9-146"></a><span class="in">```{julia}</span></span>
<span id="cb9-147"><a href="#cb9-147"></a><span class="in"># Number of points to generate:</span></span>
<span id="cb9-148"><a href="#cb9-148"></a><span class="in">xs, y = toy_data_non_linear(200)</span></span>
<span id="cb9-149"><a href="#cb9-149"></a><span class="in">X = hcat(xs...); # bring into tabular format</span></span>
<span id="cb9-150"><a href="#cb9-150"></a><span class="in">data = zip(xs,y)</span></span>
<span id="cb9-151"><a href="#cb9-151"></a></span>
<span id="cb9-152"><a href="#cb9-152"></a><span class="in"># Build MLP:</span></span>
<span id="cb9-153"><a href="#cb9-153"></a><span class="in">n_hidden = 32</span></span>
<span id="cb9-154"><a href="#cb9-154"></a><span class="in">D = size(X)[1]</span></span>
<span id="cb9-155"><a href="#cb9-155"></a><span class="in">nn = Chain(</span></span>
<span id="cb9-156"><a href="#cb9-156"></a><span class="in">    Dense(D, n_hidden, σ),</span></span>
<span id="cb9-157"><a href="#cb9-157"></a><span class="in">    Dense(n_hidden, 1)</span></span>
<span id="cb9-158"><a href="#cb9-158"></a><span class="in">)  </span></span>
<span id="cb9-159"><a href="#cb9-159"></a><span class="in">λ = 0.01</span></span>
<span id="cb9-160"><a href="#cb9-160"></a><span class="in">sqnorm(x) = sum(abs2, x)</span></span>
<span id="cb9-161"><a href="#cb9-161"></a><span class="in">weight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))</span></span>
<span id="cb9-162"><a href="#cb9-162"></a><span class="in">loss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()</span></span>
<span id="cb9-163"><a href="#cb9-163"></a></span>
<span id="cb9-164"><a href="#cb9-164"></a><span class="in"># Training:</span></span>
<span id="cb9-165"><a href="#cb9-165"></a><span class="in">epochs = 200</span></span>
<span id="cb9-166"><a href="#cb9-166"></a><span class="in">for epoch = 1:epochs</span></span>
<span id="cb9-167"><a href="#cb9-167"></a><span class="in">  for d in data</span></span>
<span id="cb9-168"><a href="#cb9-168"></a><span class="in">    gs = gradient(params(nn)) do</span></span>
<span id="cb9-169"><a href="#cb9-169"></a><span class="in">      l = loss(d...)</span></span>
<span id="cb9-170"><a href="#cb9-170"></a><span class="in">    end</span></span>
<span id="cb9-171"><a href="#cb9-171"></a><span class="in">    update!(opt, params(nn), gs)</span></span>
<span id="cb9-172"><a href="#cb9-172"></a><span class="in">  end</span></span>
<span id="cb9-173"><a href="#cb9-173"></a><span class="in">end</span></span>
<span id="cb9-174"><a href="#cb9-174"></a><span class="in">```</span></span>
<span id="cb9-175"><a href="#cb9-175"></a></span>
<span id="cb9-176"><a href="#cb9-176"></a>Fitting the Laplace approximation is also analogous, but note that this we have added an argument: <span class="in">`subset_of_weights=:last_layer`</span>. This specifies that we only want to use the parameters of the last layer of our MLP. While we could have used all of them (<span class="in">`subset_of_weights=:all`</span>), @daxberger2021laplace find that the last-layer Laplace approximation produces satisfying results, while be computationally cheaper. @fig-mlp demonstrates that once again the Laplace approximation yields a posterior predictive distribution that is more conservative than the over-confident plugin estimate.</span>
<span id="cb9-177"><a href="#cb9-177"></a></span>
<span id="cb9-180"><a href="#cb9-180"></a><span class="in">```{julia}</span></span>
<span id="cb9-181"><a href="#cb9-181"></a><span class="in">la = laplace(nn, λ=λ, subset_of_weights=:last_layer)</span></span>
<span id="cb9-182"><a href="#cb9-182"></a><span class="in">fit!(la, data);</span></span>
<span id="cb9-183"><a href="#cb9-183"></a><span class="in">p_plugin = plot_contour(X',y,la;title="Plugin",type=:plugin)</span></span>
<span id="cb9-184"><a href="#cb9-184"></a><span class="in">p_laplace = plot_contour(X',y,la;title="Laplace")</span></span>
<span id="cb9-185"><a href="#cb9-185"></a><span class="in"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb9-186"><a href="#cb9-186"></a><span class="in">plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))</span></span>
<span id="cb9-187"><a href="#cb9-187"></a><span class="in">savefig(plt, "www/posterior_predictive_mlp.png");</span></span>
<span id="cb9-188"><a href="#cb9-188"></a><span class="in">```</span></span>
<span id="cb9-189"><a href="#cb9-189"></a></span>
<span id="cb9-190"><a href="#cb9-190"></a><span class="al">![Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).](www/posterior_predictive_mlp.png)</span>{#fig-mlp}</span>
<span id="cb9-191"><a href="#cb9-191"></a></span>
<span id="cb9-192"><a href="#cb9-192"></a>To see why this is a desirable outcome consider the zoomed out version of @fig-mlp below: the plugin estimator classifies with full confidence in regions completely scarce of any data. Arguably Laplace approximation produces a much more reasonable picture, even though it too could likely be improved by fine-tuning our choice of $\lambda$ and the neural network architecture.</span>
<span id="cb9-193"><a href="#cb9-193"></a></span>
<span id="cb9-196"><a href="#cb9-196"></a><span class="in">```{julia}</span></span>
<span id="cb9-197"><a href="#cb9-197"></a><span class="in">zoom=-50</span></span>
<span id="cb9-198"><a href="#cb9-198"></a><span class="in">p_plugin = plot_contour(X',y,la;title="Plugin",type=:plugin,zoom=zoom);</span></span>
<span id="cb9-199"><a href="#cb9-199"></a><span class="in">p_laplace = plot_contour(X',y,la;title="Laplace",zoom=zoom);</span></span>
<span id="cb9-200"><a href="#cb9-200"></a><span class="in"># Plot the posterior distribution with a contour plot.</span></span>
<span id="cb9-201"><a href="#cb9-201"></a><span class="in">plt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400));</span></span>
<span id="cb9-202"><a href="#cb9-202"></a><span class="in">savefig(plt, "www/posterior_predictive_mlp_zoom.png");</span></span>
<span id="cb9-203"><a href="#cb9-203"></a><span class="in">```</span></span>
<span id="cb9-204"><a href="#cb9-204"></a></span>
<span id="cb9-205"><a href="#cb9-205"></a><span class="al">![Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out.](www/posterior_predictive_mlp_zoom.png)</span>{#fig-mlp-zoom}</span>
<span id="cb9-206"><a href="#cb9-206"></a></span>
<span id="cb9-207"><a href="#cb9-207"></a><span class="fu">## Wrapping up</span></span>
<span id="cb9-208"><a href="#cb9-208"></a></span>
<span id="cb9-209"><a href="#cb9-209"></a>Recent state-of-the-art research on neural information processing suggests that Bayesian deep learning can be effortless: Laplace approximation for deep neural networks appears to work very well and it does so at minimal computational cost <span class="co">[</span><span class="ot">@daxberger2021laplace</span><span class="co">]</span>. This is great news, because the case for turning Bayesian is strong: society increasingly relies on complex automated decision-making systems that need to be trustworthy. More and more of these systems involve deep learning which in and of itself is not trustworthy. We have seen that typically there exist various viable parameterizations of deep neural networks each with their own distinct and compelling explanation for the data at hand. When faced with many viable options, don't put all of your eggs in one basket. In other words, go Bayesian!</span>
<span id="cb9-210"><a href="#cb9-210"></a></span>
<span id="cb9-211"><a href="#cb9-211"></a></span>
<span id="cb9-212"><a href="#cb9-212"></a><span class="fu">## Resources</span></span>
<span id="cb9-213"><a href="#cb9-213"></a></span>
<span id="cb9-214"><a href="#cb9-214"></a>To get started with Bayesian deep learning I have found many useful and free resources online, some of which are listed below:</span>
<span id="cb9-215"><a href="#cb9-215"></a></span>
<span id="cb9-216"><a href="#cb9-216"></a><span class="ss">- </span><span class="co">[</span><span class="ot">`Turing.jl` tutorial</span><span class="co">](https://turing.ml/dev/tutorials/03-bayesian-neural-network/)</span> on Bayesian deep learning in Julia.</span>
<span id="cb9-217"><a href="#cb9-217"></a><span class="ss">- </span>Various RStudio AI blog posts including <span class="co">[</span><span class="ot">this one</span><span class="co">](https://blogs.rstudio.com/ai/posts/2018-11-12-uncertainty_estimates_dropout/)</span> and <span class="co">[</span><span class="ot">this one</span><span class="co">](https://blogs.rstudio.com/ai/posts/2019-06-05-uncertainty-estimates-tfprobability/)</span>.</span>
<span id="cb9-218"><a href="#cb9-218"></a><span class="ss">- </span><span class="co">[</span><span class="ot">TensorFlow blog post</span><span class="co">](https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf)</span> on regression with probabilistic layers.</span>
<span id="cb9-219"><a href="#cb9-219"></a><span class="ss">- </span>Kevin Murphy's <span class="co">[</span><span class="ot">draft text book</span><span class="co">](https://probml.github.io/pml-book/book1.html)</span>, now also available as print.</span>
<span id="cb9-220"><a href="#cb9-220"></a></span>
<span id="cb9-221"><a href="#cb9-221"></a><span class="fu">## References</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024, Patrick Altmeyer</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.paltmeyer.com/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://julialang.social/@patalt" rel="me">
      <i class="bi bi-mastodon" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/\@patrick.altmeyer">
      <i class="bi bi-medium" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>