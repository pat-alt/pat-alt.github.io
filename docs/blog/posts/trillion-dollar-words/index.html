<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.4.550">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="author" content="Patrick Altmeyer">
<meta name="dcterms.date" content="2024-02-18">
<meta name="description" content="A short post introducing a small new Julia package that facilitates working with the Trillion Dollar Words dataset and model published in a recent ACL 2023 paper.">

<title>Patrick Altmeyer - TrillionDollarWords.jl</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
/* CSS for citations */
div.csl-bib-body { }
div.csl-entry {
  clear: both;
  margin-bottom: 0em;
}
.hanging-indent div.csl-entry {
  margin-left:2em;
  text-indent:-2em;
}
div.csl-left-margin {
  min-width:2em;
  float:left;
}
div.csl-right-inline {
  margin-left:2em;
  padding-left:1em;
}
div.csl-indent {
  margin-left: 2em;
}</style>


<script src="../../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../../">
<link href="../../..//www/favicon.ico" rel="icon">
<script src="../../../site_libs/quarto-html/quarto.js"></script>
<script src="../../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" id="quarto-text-highlighting-styles">
<script src="../../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" id="quarto-bootstrap" data-mode="light">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 50,
  "keyboard-shortcut": [
    "f",
    "/",
    "s"
  ],
  "show-item-context": false,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-text-placeholder": "",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit",
    "search-label": "Search"
  }
}</script>
<script async="" src="https://www.googletagmanager.com/gtag/js?id=G-BEEZ30787D"></script>

<script type="text/javascript">

window.dataLayer = window.dataLayer || [];
function gtag(){dataLayer.push(arguments);}
gtag('js', new Date());
gtag('config', 'G-BEEZ30787D', { 'anonymize_ip': true});
</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<script type="text/javascript">
const typesetMath = (el) => {
  if (window.MathJax) {
    // MathJax Typeset
    window.MathJax.typeset([el]);
  } else if (window.katex) {
    // KaTeX Render
    var mathElements = el.getElementsByClassName("math");
    var macros = [];
    for (var i = 0; i < mathElements.length; i++) {
      var texText = mathElements[i].firstChild;
      if (mathElements[i].tagName == "SPAN") {
        window.katex.render(texText.data, mathElements[i], {
          displayMode: mathElements[i].classList.contains('display'),
          throwOnError: false,
          macros: macros,
          fleqn: false
        });
      }
    }
  }
}
window.Quarto = {
  typesetMath
};
</script>

<link rel="stylesheet" href="../../../styles.css">
<link rel="stylesheet" href="styles.css">
<meta property="og:title" content="Patrick Altmeyer - TrillionDollarWords.jl">
<meta property="og:description" content="A short post introducing a small new Julia package that facilitates working with the Trillion Dollar Words dataset and model published in a recent ACL 2023 paper.">
<meta property="og:image" content="https://www.paltmeyer.com/blog/blog/posts/trillion-dollar-words/intro.jpeg">
<meta property="og:site_name" content="Patrick Altmeyer">
</head>

<body class="nav-fixed">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg " data-bs-theme="dark">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container mx-auto">
    <a href="../../../index.html" class="navbar-brand navbar-brand-logo">
    <img src="../../../www/icon.png" alt="" class="navbar-logo">
    </a>
    <a class="navbar-brand" href="../../../index.html">
    <span class="navbar-title">Patrick Altmeyer</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll me-auto">
  <li class="nav-item">
    <a class="nav-link" href="../../../index.html"> 
<span class="menu-text">Home</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/publications/index.html"> 
<span class="menu-text">Publications</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/talks/index.html"> 
<span class="menu-text">Talks</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../blog/index.html"> 
<span class="menu-text">Blog</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/software.html"> 
<span class="menu-text">Software</span></a>
  </li>  
  <li class="nav-item">
    <a class="nav-link" href="../../../content/about/index.html"> 
<span class="menu-text">About</span></a>
  </li>  
</ul>
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="../../../blog/index.xml"> <i class="bi bi-rss" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt"> <i class="bi bi-github" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
  <li class="nav-item compact">
    <a class="nav-link" href="https://julialang.social/@patalt" rel="me"> <i class="bi bi-mastodon" role="img">
</i> 
<span class="menu-text"></span></a>
  </li>  
</ul>
          </div> <!-- /navcollapse -->
          <div class="quarto-navbar-tools">
</div>
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    <div id="quarto-margin-sidebar" class="sidebar margin-sidebar">
        <nav id="TOC" role="doc-toc" class="toc-active">
    <h2 id="toc-title">On this page</h2>
   
  <ul>
  <li><a href="#trilliondollarwords.jl" id="toc-trilliondollarwords.jl" class="nav-link active" data-scroll-target="#trilliondollarwords.jl">TrillionDollarWords.jl</a>
  <ul class="collapse">
  <li><a href="#basic-functionality" id="toc-basic-functionality" class="nav-link" data-scroll-target="#basic-functionality">Basic Functionality</a></li>
  <li><a href="#loading-the-data" id="toc-loading-the-data" class="nav-link" data-scroll-target="#loading-the-data">Loading the Data</a></li>
  <li><a href="#loading-the-model" id="toc-loading-the-model" class="nav-link" data-scroll-target="#loading-the-model">Loading the Model</a></li>
  <li><a href="#basic-model-inference" id="toc-basic-model-inference" class="nav-link" data-scroll-target="#basic-model-inference">Basic Model Inference</a></li>
  <li><a href="#probe-findings" id="toc-probe-findings" class="nav-link" data-scroll-target="#probe-findings">Probe Findings</a></li>
  </ul></li>
  <li><a href="#intended-purpose-and-goals" id="toc-intended-purpose-and-goals" class="nav-link" data-scroll-target="#intended-purpose-and-goals">Intended Purpose and Goals</a></li>
  <li><a href="#references" id="toc-references" class="nav-link" data-scroll-target="#references">References</a></li>
  </ul>
<div class="toc-actions"><ul><li><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></nav>
    </div>
<!-- main -->
<main class="content" id="quarto-document-content">

<header id="title-block-header" class="quarto-title-block default">
<div class="quarto-title">
<div class="quarto-title-block"><div><h1 class="title">TrillionDollarWords.jl</h1><button type="button" class="btn code-tools-button dropdown-toggle" id="quarto-code-tools-menu" data-bs-toggle="dropdown" aria-expanded="false"><i class="bi"></i> Code</button><ul class="dropdown-menu dropdown-menu-end" aria-labelelledby="quarto-code-tools-menu"><li><a id="quarto-show-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Show All Code</a></li><li><a id="quarto-hide-all-code" class="dropdown-item" href="javascript:void(0)" role="button">Hide All Code</a></li><li><hr class="dropdown-divider"></li><li><a id="quarto-view-source" class="dropdown-item" href="javascript:void(0)" role="button">View Source</a></li></ul></div></div>
<p class="subtitle lead">The Trillion Dollar Words dataset and model in Julia</p>
  <div class="quarto-categories">
    <div class="quarto-category">llm</div>
    <div class="quarto-category">mechanistic interpretability</div>
    <div class="quarto-category">finance</div>
    <div class="quarto-category">Julia</div>
  </div>
  </div>

<div>
  <div class="description">
    A short post introducing a small new Julia package that facilitates working with the Trillion Dollar Words dataset and model published in a recent ACL 2023 <a href="https://arxiv.org/abs/2305.07972">paper</a>.
  </div>
</div>

<div class="quarto-title-meta-author">
  <div class="quarto-title-meta-heading">Author</div>
  <div class="quarto-title-meta-heading">Affiliation</div>
  
    <div class="quarto-title-meta-contents">
    <p class="author"><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a> </p>
  </div>
  <div class="quarto-title-meta-contents">
        <p class="affiliation">
            <a href="https://www.tudelft.nl/en/">
            Delft University of Technology
            </a>
          </p>
      </div>
  </div>

<div class="quarto-title-meta">

      
    <div>
    <div class="quarto-title-meta-heading">Published</div>
    <div class="quarto-title-meta-contents">
      <p class="date">February 18, 2024</p>
    </div>
  </div>
  
    
  </div>
  


</header>


<div class="intro-gif">
<figure class="figure">
<img src="intro.jpeg" class="figure-img">
<figcaption>
Photo by <a href="https://unsplash.com/@neonbrand?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Kenny Eliason</a> on <a href="https://unsplash.com/photos/1-us-dollar-banknote-8fDhgAN5zG0?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash">Unsplash</a>
</figcaption>
</figure>
</div>
<p>In a recent <a href="../../../blog/posts/spurious-sparks/index.html">post</a>, I questioned the idea that finding patterns in latent embeddings of models is indicative of AGI or even surprising. One of the models we investigate in our related <a href="https://arxiv.org/abs/2402.03962">paper</a> <span class="citation" data-cites="altmeyer2024position">(<a href="#ref-altmeyer2024position" role="doc-biblioref">Altmeyer et al. 2024</a>)</span> is the <em>FOMC-RoBERTa</em> model trained on the Trillion Dollar Words dataset, both of which were published by <span class="citation" data-cites="shah2023trillion">Shah, Paturi, and Chava (<a href="#ref-shah2023trillion" role="doc-biblioref">2023</a>)</span> in a recent ACL 2023 paper: <a href="https://arxiv.org/abs/2305.07972">Trillion Dollar Words: A New Financial Dataset, Task &amp; Market Analysis</a> <span class="citation" data-cites="shah2023trillion">(<a href="#ref-shah2023trillion" role="doc-biblioref">Shah, Paturi, and Chava 2023</a>)</span>. To run our experiments and facilitate working with the data and model in Julia, I have developed a small package: <a href="https://github.com/pat-alt/TrillionDollarWords.jl">TrillionDollarWords.jl</a>. This short post introduces the package and its basic functionality.</p>
<section id="trilliondollarwords.jl" class="level2">
<h2 class="anchored" data-anchor-id="trilliondollarwords.jl">TrillionDollarWords.jl</h2>
<p><a href="https://pat-alt.github.io/TrillionDollarWords.jl/stable/"><img src="https://img.shields.io/badge/docs-stable-blue.svg" class="img-fluid" alt="Stable"></a> <a href="https://pat-alt.github.io/TrillionDollarWords.jl/dev/"><img src="https://img.shields.io/badge/docs-dev-blue.svg" class="img-fluid" alt="Dev"></a> <a href="https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml?query=branch%3Amain"><img src="https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml/badge.svg?branch=main" class="img-fluid" alt="Build Status"></a> <a href="https://codecov.io/gh/pat-alt/TrillionDollarWords.jl"><img src="https://codecov.io/gh/pat-alt/TrillionDollarWords.jl/branch/main/graph/badge.svg" class="img-fluid" alt="Coverage"></a> <a href="https://github.com/invenia/BlueStyle"><img src="https://img.shields.io/badge/code%20style-blue-4495d1.svg" class="img-fluid" alt="Code Style: Blue"></a></p>
<p><a href="https://github.com/pat-alt/TrillionDollarWords.jl">TrillionDollarWords.jl</a> is a light-weight package that provides Julia useres easy access to the Trillion Dollar Words dataset and model <span class="citation" data-cites="shah2023trillion">(<a href="#ref-shah2023trillion" role="doc-biblioref">Shah, Paturi, and Chava 2023</a>)</span>.</p>
<div class="callout callout-style-default callout-note callout-titled">
<div class="callout-header d-flex align-content-center">
<div class="callout-icon-container">
<i class="callout-icon"></i>
</div>
<div class="callout-title-container flex-fill">
Disclaimer
</div>
</div>
<div class="callout-body-container callout-body">
<p>Please note that I am not the author of the Trillion Dollar Words paper nor am I affiliated with the authors. The package was developed as a by-product of our research and is not officially endorsed by the authors of the paper.</p>
</div>
</div>
<p>You can install the package from Julia’s general registry as follows:</p>
<div class="sourceCode" id="cb1"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb1-1"><a href="#cb1-1"></a><span class="im">using</span> <span class="bu">Pkg</span></span>
<span id="cb1-2"><a href="#cb1-2"></a><span class="bu">Pkg</span>.<span class="fu">add</span>(url<span class="op">=</span><span class="st">"TrillionDollarWords.jl"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>To install the development version, use the following command:</p>
<div class="sourceCode" id="cb2"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb2-1"><a href="#cb2-1"></a><span class="im">using</span> <span class="bu">Pkg</span></span>
<span id="cb2-2"><a href="#cb2-2"></a><span class="bu">Pkg</span>.<span class="fu">add</span>(url<span class="op">=</span><span class="st">"https://github.com/pat-alt/TrillionDollarWords.jl"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<section id="basic-functionality" class="level3">
<h3 class="anchored" data-anchor-id="basic-functionality">Basic Functionality</h3>
<p>The package provides the following functionality:</p>
<ul>
<li>Load pre-processed data.</li>
<li>Load the model proposed in the paper.</li>
<li>Basic model inference: compute forward passes and layer-wise activations.</li>
<li>Download pre-computed activations for probing the model.</li>
</ul>
<p>The latter two are particularly useful for downstream tasks related to <a href="https://en.wikipedia.org/wiki/Large_language_model#Interpretation">mechanistic interpretability</a>. In times of increasing scrutiny of AI models, it is important to understand how they work and what they have learned. Mechanistic interpretability is a promising approach to this end, as it aims to understand the model’s internal representations and how they relate to the task at hand. As we make abundantly clear in our own <a href="https://arxiv.org/abs/2402.03962">paper</a> <span class="citation" data-cites="altmeyer2024position">(<a href="#ref-altmeyer2024position" role="doc-biblioref">Altmeyer et al. 2024</a>)</span>, interpretability is not a silver bullet, but merely a step towards understanding, monitoring and improving AI models.</p>
</section>
<section id="loading-the-data" class="level3">
<h3 class="anchored" data-anchor-id="loading-the-data">Loading the Data</h3>
<p>The Trillion Dollar Words dataset is a collection of preprocessed sentences around 40,000 time-stamped sentences from meeting minutes, press conferences and speeches by members of the Federal Open Market Committee (FOMC) <span class="citation" data-cites="shah2023trillion">(<a href="#ref-shah2023trillion" role="doc-biblioref">Shah, Paturi, and Chava 2023</a>)</span>. The total sample period spans from January, 1996, to October, 2022. In order to train various rule-based models and large language models (LLM) to classify sentences as either ‘hawkish’, ‘dovish’ or ‘neutral’, they have manually annotated a subset of around 2,500 sentences. The best-performing model, a large BERT model with around 355 million parameters, was open-sourced on <a href="https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves">HuggingFace</a>. The authors also link the sentences to market data, which makes it possible to study the relationship between language and financial markets. While the authors of the paper did publish their data, much of it is unfortunately scattered across CSV and Excel files stored in a public GitHub <a href="https://github.com/gtfintechlab/fomc-hawkish-dovish">repo</a>. I have collected and merged that data, yielding a combined dataset with indexed sentences and additional metadata that may be useful for downstream tasks.</p>
<p>The entire dataset of all available sentences used in the paper can be loaded as follows:</p>
<div id="7ce0b5af" class="cell" data-execution_count="2">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb3-1"><a href="#cb3-1"></a><span class="im">using</span> <span class="bu">TrillionDollarWords</span></span>
<span id="cb3-2"><a href="#cb3-2"></a><span class="fu">load_all_sentences</span>() <span class="op">|&gt;</span> show</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>38358×8 DataFrame
   Row │ sentence_id  doc_id  date        event_type        label    sentence  ⋯
       │ Int64        Int64   Date        String31          String7  String    ⋯
───────┼────────────────────────────────────────────────────────────────────────
     1 │           1       1  1996-01-30  meeting minutes   neutral  The Commi ⋯
     2 │           2       1  1996-01-30  meeting minutes   neutral  Consumer
     3 │           3       1  1996-01-30  meeting minutes   dovish   Slower gr
     4 │           4       1  1996-01-30  meeting minutes   hawkish  The deman
     5 │           5       1  1996-01-30  meeting minutes   neutral  The recen ⋯
     6 │           6       1  1996-01-30  meeting minutes   neutral  Nonfarm p
     7 │           7       1  1996-01-30  meeting minutes   hawkish  Job growt
     8 │           8       1  1996-01-30  meeting minutes   hawkish  Elsewhere
     9 │           9       1  1996-01-30  meeting minutes   neutral  The outpu ⋯
    10 │          10       1  1996-01-30  meeting minutes   neutral  Recent in
    11 │          11       1  1996-01-30  meeting minutes   hawkish  Incoming
   ⋮   │      ⋮         ⋮         ⋮              ⋮             ⋮               ⋱
 38349 │       38349      63  2015-09-17  press conference  dovish   monetary
 38350 │       38350      63  2015-09-17  press conference  neutral  When we—w ⋯
 38351 │       38351      63  2015-09-17  press conference  neutral  It’s one
 38352 │       38352      63  2015-09-17  press conference  neutral  1 Chair Y
 38353 │       38353      63  2015-09-17  press conference  neutral  It remain
 38354 │       38354      63  2015-09-17  press conference  neutral  And, reme ⋯
 38355 │       38355      63  2015-09-17  press conference  neutral  It is tru
 38356 │       38356      63  2015-09-17  press conference  dovish   To me, th
 38357 │       38357      63  2015-09-17  press conference  hawkish  And since
 38358 │       38358      63  2015-09-17  press conference  neutral  There hav ⋯
                                                3 columns and 38337 rows omitted</code></pre>
</div>
</div>
<p>The combined dataset is also available as a <code>DataFrame</code> and can be loaded as follows:</p>
<div id="2392f6c4" class="cell" data-execution_count="3">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb5-1"><a href="#cb5-1"></a><span class="fu">load_all_data</span>() <span class="op">|&gt;</span> show</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>524395×11 DataFrame
    Row │ sentence_id  doc_id  date        event_type       label    sentence  ⋯
        │ Int64        Int64   Date        String31         String7  String    ⋯
────────┼───────────────────────────────────────────────────────────────────────
      1 │           1       1  1996-01-30  meeting minutes  neutral  The Commi ⋯
      2 │           2       1  1996-01-30  meeting minutes  neutral  Consumer
      3 │           3       1  1996-01-30  meeting minutes  dovish   Slower gr
      4 │           4       1  1996-01-30  meeting minutes  hawkish  The deman
      5 │           5       1  1996-01-30  meeting minutes  neutral  The recen ⋯
      6 │           6       1  1996-01-30  meeting minutes  neutral  Nonfarm p
      7 │           7       1  1996-01-30  meeting minutes  hawkish  Job growt
      8 │           8       1  1996-01-30  meeting minutes  hawkish  Elsewhere
      9 │           9       1  1996-01-30  meeting minutes  neutral  The outpu ⋯
     10 │          10       1  1996-01-30  meeting minutes  neutral  Recent in
     11 │          11       1  1996-01-30  meeting minutes  hawkish  Incoming
   ⋮    │      ⋮         ⋮         ⋮              ⋮            ⋮               ⋱
 524386 │       29435     125  2022-10-12  speech           hawkish  However,
 524387 │       29436     125  2022-10-12  speech           hawkish  My genera ⋯
 524388 │       29429     125  2022-10-12  speech           neutral  I will fo
 524389 │       29430     125  2022-10-12  speech           hawkish  Inflation
 524390 │       29431     125  2022-10-12  speech           neutral  At this p
 524391 │       29432     125  2022-10-12  speech           hawkish  If we do  ⋯
 524392 │       29433     125  2022-10-12  speech           hawkish  However,
 524393 │       29434     125  2022-10-12  speech           hawkish  To bring
 524394 │       29435     125  2022-10-12  speech           hawkish  However,
 524395 │       29436     125  2022-10-12  speech           hawkish  My genera ⋯
                                               6 columns and 524374 rows omitted</code></pre>
</div>
</div>
<p>Additional functionality for data loading is available (see <a href="https://www.paltmeyer.com/TrillionDollarWords.jl/dev/">docs</a>).</p>
</section>
<section id="loading-the-model" class="level3">
<h3 class="anchored" data-anchor-id="loading-the-model">Loading the Model</h3>
<p>The model can be loaded with or without the classifier head (below without the head). Under the hood, this function uses <a href="https://github.com/chengchingwen/Transformers.jl">Transformers.jl</a> to retrieve the model from <a href="https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves">HuggingFace</a>. Any keyword arguments accepted by <code>Transformers.HuggingFace.HGFConfig</code> can also be passed. For example, to load the model without the classifier head and enable access to layer-wise activations, the following command can be used:</p>
<div id="9e7d7954" class="cell" data-execution_count="4">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb7-1"><a href="#cb7-1"></a><span class="fu">load_model</span>(; load_head<span class="op">=</span><span class="cn">false</span>, output_hidden_states<span class="op">=</span><span class="cn">true</span>) <span class="op">|&gt;</span> show</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>BaselineModel(GPT2TextEncoder(
├─ TextTokenizer(MatchTokenization(CodeNormalizer(BPETokenization(GPT2Tokenization, bpe = CachedBPE(BPE(50000 merges))), codemap = CodeMap{UInt8 =&gt; UInt16}(3 code-ranges)), 5 patterns)),
├─ vocab = Vocab{String, SizedArray}(size = 50265, unk = &lt;unk&gt;, unki = 4),
├─ codemap = CodeMap{UInt8 =&gt; UInt16}(3 code-ranges),
├─ startsym = &lt;s&gt;,
├─ endsym = &lt;/s&gt;,
├─ padsym = &lt;pad&gt;,
├─ trunc = 256,
└─ process = Pipelines:
  ╰─ target[token] := TextEncodeBase.nestedcall(string_getvalue, source)
  ╰─ target[token] := Transformers.TextEncoders.grouping_sentence(target.token)
  ╰─ target[(token, segment)] := SequenceTemplate{String}(&lt;s&gt;:&lt;type=1&gt; Input:&lt;type=1&gt; &lt;/s&gt;:&lt;type=1&gt; (&lt;/s&gt;:&lt;type=1&gt; Input:&lt;type=1&gt; &lt;/s&gt;:&lt;type=1&gt;)...)(target.token)
  ╰─ target[attention_mask] := (NeuralAttentionlib.LengthMask ∘ Transformers.TextEncoders.getlengths(256))(target.token)
  ╰─ target[token] := TextEncodeBase.trunc_or_pad(256, &lt;pad&gt;, tail, tail)(target.token)
  ╰─ target[token] := TextEncodeBase.nested2batch(target.token)
  ╰─ target := (target.token, target.attention_mask)
), HGFRobertaModel(Chain(CompositeEmbedding(token = Embed(1024, 50265), position = ApplyEmbed(.+, FixedLenPositionEmbed(1024, 514), Transformers.HuggingFace.roberta_pe_indices(1,)), segment = ApplyEmbed(.+, Embed(1024, 1), Transformers.HuggingFace.bert_ones_like)), DropoutLayer&lt;nothing&gt;(LayerNorm(1024, ϵ = 1.0e-5))), Transformer&lt;24&gt;(PostNormTransformerBlock(DropoutLayer&lt;nothing&gt;(SelfAttention(MultiheadQKVAttenOp(head = 16, p = nothing), Fork&lt;3&gt;(Dense(W = (1024, 1024), b = true)), Dense(W = (1024, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5), DropoutLayer&lt;nothing&gt;(Chain(Dense(σ = NNlib.gelu, W = (1024, 4096), b = true), Dense(W = (4096, 1024), b = true))), LayerNorm(1024, ϵ = 1.0e-5))), Branch{(:pooled,) = (:hidden_state,)}(BertPooler(Dense(σ = NNlib.tanh_fast, W = (1024, 1024), b = true)))), Transformers.HuggingFace.HGFConfig{:roberta, JSON3.Object{Vector{UInt8}, Vector{UInt64}}, Dict{Symbol, Any}}(:use_cache =&gt; true, :torch_dtype =&gt; "float32", :vocab_size =&gt; 50265, :output_hidden_states =&gt; true, :hidden_act =&gt; "gelu", :num_hidden_layers =&gt; 24, :num_attention_heads =&gt; 16, :classifier_dropout =&gt; nothing, :type_vocab_size =&gt; 1, :intermediate_size =&gt; 4096, :max_position_embeddings =&gt; 514, :model_type =&gt; "roberta", :layer_norm_eps =&gt; 1.0e-5, :id2label =&gt; Dict(0 =&gt; "LABEL_0", 2 =&gt; "LABEL_2", 1 =&gt; "LABEL_1"), :_name_or_path =&gt; "roberta-large", :hidden_size =&gt; 1024, :transformers_version =&gt; "4.21.2", :attention_probs_dropout_prob =&gt; 0.1, :bos_token_id =&gt; 0, :problem_type =&gt; "single_label_classification", :eos_token_id =&gt; 2, :initializer_range =&gt; 0.02, :hidden_dropout_prob =&gt; 0.1, :label2id =&gt; Dict("LABEL_1" =&gt; 1, "LABEL_2" =&gt; 2, "LABEL_0" =&gt; 0), :pad_token_id =&gt; 1, :position_embedding_type =&gt; "absolute", :architectures =&gt; ["RobertaForSequenceClassification"]))</code></pre>
</div>
</div>
</section>
<section id="basic-model-inference" class="level3">
<h3 class="anchored" data-anchor-id="basic-model-inference">Basic Model Inference</h3>
<p>Using the model and data, layer-wise activations can be computed as below (here for the first 5 sentences). When called on a <code>DataFrame</code>, the <code>layerwise_activations</code> returns a data frame that links activations to sentence identifiers. This makes it possible to relate activations to market data by using the <code>sentence_id</code> key. Alternatively, <code>layerwise_activations</code> also accepts a vector of sentences.</p>
<div id="51ba0a6e" class="cell" data-execution_count="5">
<details open="" class="code-fold">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb9-1"><a href="#cb9-1"></a>df <span class="op">=</span> <span class="fu">load_all_sentences</span>()</span>
<span id="cb9-2"><a href="#cb9-2"></a>mod <span class="op">=</span> <span class="fu">load_model</span>(; load_head<span class="op">=</span><span class="cn">false</span>, output_hidden_states<span class="op">=</span><span class="cn">true</span>)</span>
<span id="cb9-3"><a href="#cb9-3"></a>n <span class="op">=</span> <span class="fl">5</span></span>
<span id="cb9-4"><a href="#cb9-4"></a>queries <span class="op">=</span> df[<span class="fl">1</span><span class="op">:</span>n, <span class="op">:</span>]</span>
<span id="cb9-5"><a href="#cb9-5"></a><span class="fu">layerwise_activations</span>(mod, queries) <span class="op">|&gt;</span> show</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>122880×4 DataFrame
    Row │ sentence_id  activations  layer  activation_id 
        │ Int64        Float32      Int64  Int64         
────────┼────────────────────────────────────────────────
      1 │           1   0.202931        1              1
      2 │           1  -0.00693996      1              2
      3 │           1   0.12731         1              3
      4 │           1  -0.0129803       1              4
      5 │           1   0.122843        1              5
      6 │           1   0.258675        1              6
      7 │           1   0.0466324       1              7
      8 │           1   0.0318548       1              8
      9 │           1   1.18888         1              9
     10 │           1  -0.0386651       1             10
     11 │           1  -0.116031        1             11
   ⋮    │      ⋮            ⋮         ⋮          ⋮
 122871 │           5  -0.769513       24           1015
 122872 │           5   0.834678       24           1016
 122873 │           5   0.212098       24           1017
 122874 │           5  -0.556661       24           1018
 122875 │           5   0.0957697      24           1019
 122876 │           5   1.04358        24           1020
 122877 │           5   1.71445        24           1021
 122878 │           5   1.162          24           1022
 122879 │           5  -1.58513        24           1023
 122880 │           5  -1.01479        24           1024
                                      122859 rows omitted</code></pre>
</div>
</div>
</section>
<section id="probe-findings" class="level3">
<h3 class="anchored" data-anchor-id="probe-findings">Probe Findings</h3>
<p>For our own <a href="https://arxiv.org/abs/2402.03962">research</a> <span class="citation" data-cites="altmeyer2024position">(<a href="#ref-altmeyer2024position" role="doc-biblioref">Altmeyer et al. 2024</a>)</span>, we have been interested in probing the model. This involves using linear models to estimate the relationship between layer-wise transformer embeddings and some outcome variable of interest <span class="citation" data-cites="alain2018understanding">(<a href="#ref-alain2018understanding" role="doc-biblioref">Alain and Bengio 2018</a>)</span>. To do this, we first had to run a single forward pass for each sentence through the RoBERTa model and store the layerwise emeddings. As we have seen above, the package ships with functionality for doing just that, but to save others valuable GPU hours we have archived activations of the hidden state on the first entity token for each layer as <a href="https://github.com/pat-alt/TrillionDollarWords.jl/releases/tag/activations_2024-01-17">artifacts</a>. To download the last-layer activations in an interactive Julia session, for example, users can proceed as follows:</p>
<div class="sourceCode" id="cb11"><pre class="sourceCode numberSource julia number-lines code-with-copy"><code class="sourceCode julia"><span id="cb11-1"><a href="#cb11-1"></a><span class="im">using</span> <span class="bu">LazyArtifacts</span></span>
<span id="cb11-2"><a href="#cb11-2"></a></span>
<span id="cb11-3"><a href="#cb11-3"></a>julia<span class="op">&gt;</span> artifact<span class="st">"activations_layer_24"</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
<p>We have found that despite the small sample size, the <em>FOMC-RoBERTa</em> model appears to have distilled useful representations for downstream tasks that it was not explicitly trained for. <a href="#fig-rmse-pca-128" class="quarto-xref">Figure&nbsp;1</a> below shows the average out-of-sample root mean squared error for predicting various market indicators from layer activations. Consistent with findings in related work <span class="citation" data-cites="alain2018understanding">(<a href="#ref-alain2018understanding" role="doc-biblioref">Alain and Bengio 2018</a>)</span>, we find that performance typically improves for layers closer to the final output layer of the transformer model. The measured performance is at least on par with baseline autoregressive models. For more information on this, see also my other recent <a href="../../../blog/posts/spurious-sparks/index.html">post</a>.</p>
<div id="fig-rmse-pca-128" class="quarto-figure quarto-figure-center quarto-float anchored">
<figure class="quarto-float quarto-float-fig figure">
<div aria-describedby="fig-rmse-pca-128-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
<img src="https://raw.githubusercontent.com/pat-alt/TrillionDollarWords.jl/11-activations-for-cls-head/dev/juliacon/rmse_pca_128.png" class="img-fluid figure-img">
</div>
<figcaption class="quarto-float-caption-bottom quarto-float-caption quarto-float-fig" id="fig-rmse-pca-128-caption-0ceaefa1-69ba-4598-a22c-09a6ac19f8ca">
Figure&nbsp;1: Out-of-sample root mean squared error (RMSE) for the linear probe plotted against <em>FOMC-RoBERTa</em>’s <span class="math inline">\(n\)</span>-th layer for different indicators. The values correspond to averages computed across cross-validation folds, where we have used an expanding window approach to split the time series.
</figcaption>
</figure>
</div>
</section>
</section>
<section id="intended-purpose-and-goals" class="level2">
<h2 class="anchored" data-anchor-id="intended-purpose-and-goals">Intended Purpose and Goals</h2>
<p>I hope that this small package may be useful to members of the Julia community who are interested in the interplay between Economics, Finance and Artificial Intelligence. It should serve as a good starting point for the following ideas:</p>
<ul>
<li>Fine-tune additional models on the classification task or other tasks of interest.</li>
<li>Further model probing, e.g.&nbsp;using other market indicators not discussed in the original paper.</li>
<li>Improve and extend the label annotations.</li>
</ul>
<p>Any contributions are very much welcome.</p>
</section>
<section id="references" class="level2">



<!-- -->


</section>

<div id="quarto-appendix" class="default"><section class="quarto-appendix-contents" role="doc-bibliography" id="quarto-bibliography"><h2 class="anchored quarto-appendix-heading">References</h2><div id="refs" class="references csl-bib-body hanging-indent" data-entry-spacing="0" role="list">
<div id="ref-alain2018understanding" class="csl-entry" role="listitem">
Alain, Guillaume, and Yoshua Bengio. 2018. <span>“Understanding Intermediate Layers Using Linear Classifier Probes.”</span> <a href="https://arxiv.org/abs/1610.01644">https://arxiv.org/abs/1610.01644</a>.
</div>
<div id="ref-altmeyer2024position" class="csl-entry" role="listitem">
Altmeyer, Patrick, Andrew M. Demetriou, Antony Bartlett, and Cynthia C. S. Liem. 2024. <span>“Position Paper: Against Spurious Sparks-Dovelating Inflated AI Claims.”</span> <a href="https://arxiv.org/abs/2402.03962">https://arxiv.org/abs/2402.03962</a>.
</div>
<div id="ref-shah2023trillion" class="csl-entry" role="listitem">
Shah, Agam, Suvan Paturi, and Sudheer Chava. 2023. <span>“Trillion Dollar Words: A New Financial Dataset, Task &amp; Market Analysis.”</span> <a href="https://arxiv.org/abs/2305.07972">https://arxiv.org/abs/2305.07972</a>.
</div>
</div></section><section class="quarto-appendix-contents" id="quarto-citation"><h2 class="anchored quarto-appendix-heading">Citation</h2><div><div class="quarto-appendix-secondary-label">BibTeX citation:</div><pre class="sourceCode code-with-copy quarto-appendix-bibtex"><code class="sourceCode bibtex">@online{altmeyer2024,
  author = {Altmeyer, Patrick},
  title = {TrillionDollarWords.jl},
  date = {2024-02-18},
  url = {https://www.paltmeyer.com/blog//blog/posts/trillion-dollar-words},
  langid = {en}
}
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre><div class="quarto-appendix-secondary-label">For attribution, please cite this work as:</div><div id="ref-altmeyer2024" class="csl-entry quarto-appendix-citeas" role="listitem">
Altmeyer, Patrick. 2024. <span>“TrillionDollarWords.jl.”</span> February
18, 2024. <a href="https://www.paltmeyer.com/blog//blog/posts/trillion-dollar-words">https://www.paltmeyer.com/blog//blog/posts/trillion-dollar-words</a>.
</div></div></section></div></main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  const viewSource = window.document.getElementById('quarto-view-source') ||
                     window.document.getElementById('quarto-code-tools-source');
  if (viewSource) {
    const sourceUrl = viewSource.getAttribute("data-quarto-source-url");
    viewSource.addEventListener("click", function(e) {
      if (sourceUrl) {
        // rstudio viewer pane
        if (/\bcapabilities=\b/.test(window.location)) {
          window.open(sourceUrl);
        } else {
          window.location.href = sourceUrl;
        }
      } else {
        const modal = new bootstrap.Modal(document.getElementById('quarto-embedded-source-code-modal'));
        modal.show();
      }
      return false;
    });
  }
  function toggleCodeHandler(show) {
    return function(e) {
      const detailsSrc = window.document.querySelectorAll(".cell > details > .sourceCode");
      for (let i=0; i<detailsSrc.length; i++) {
        const details = detailsSrc[i].parentElement;
        if (show) {
          details.open = true;
        } else {
          details.removeAttribute("open");
        }
      }
      const cellCodeDivs = window.document.querySelectorAll(".cell > .sourceCode");
      const fromCls = show ? "hidden" : "unhidden";
      const toCls = show ? "unhidden" : "hidden";
      for (let i=0; i<cellCodeDivs.length; i++) {
        const codeDiv = cellCodeDivs[i];
        if (codeDiv.classList.contains(fromCls)) {
          codeDiv.classList.remove(fromCls);
          codeDiv.classList.add(toCls);
        } 
      }
      return false;
    }
  }
  const hideAllCode = window.document.getElementById("quarto-hide-all-code");
  if (hideAllCode) {
    hideAllCode.addEventListener("click", toggleCodeHandler(false));
  }
  const showAllCode = window.document.getElementById("quarto-show-all-code");
  if (showAllCode) {
    showAllCode.addEventListener("click", toggleCodeHandler(true));
  }
  function tippyHover(el, contentFn, onTriggerFn, onUntriggerFn) {
    const config = {
      allowHTML: true,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start',
    };
    if (contentFn) {
      config.content = contentFn;
    }
    if (onTriggerFn) {
      config.onTrigger = onTriggerFn;
    }
    if (onUntriggerFn) {
      config.onUntrigger = onUntriggerFn;
    }
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
  const xrefs = window.document.querySelectorAll('a.quarto-xref');
  const processXRef = (id, note) => {
    // Strip column container classes
    const stripColumnClz = (el) => {
      el.classList.remove("page-full", "page-columns");
      if (el.children) {
        for (const child of el.children) {
          stripColumnClz(child);
        }
      }
    }
    stripColumnClz(note)
    if (id === null || id.startsWith('sec-')) {
      // Special case sections, only their first couple elements
      const container = document.createElement("div");
      if (note.children && note.children.length > 2) {
        container.appendChild(note.children[0].cloneNode(true));
        for (let i = 1; i < note.children.length; i++) {
          const child = note.children[i];
          if (child.tagName === "P" && child.innerText === "") {
            continue;
          } else {
            container.appendChild(child.cloneNode(true));
            break;
          }
        }
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(container);
        }
        return container.innerHTML
      } else {
        if (window.Quarto?.typesetMath) {
          window.Quarto.typesetMath(note);
        }
        return note.innerHTML;
      }
    } else {
      // Remove any anchor links if they are present
      const anchorLink = note.querySelector('a.anchorjs-link');
      if (anchorLink) {
        anchorLink.remove();
      }
      if (window.Quarto?.typesetMath) {
        window.Quarto.typesetMath(note);
      }
      // TODO in 1.5, we should make sure this works without a callout special case
      if (note.classList.contains("callout")) {
        return note.outerHTML;
      } else {
        return note.innerHTML;
      }
    }
  }
  for (var i=0; i<xrefs.length; i++) {
    const xref = xrefs[i];
    tippyHover(xref, undefined, function(instance) {
      instance.disable();
      let url = xref.getAttribute('href');
      let hash = undefined; 
      if (url.startsWith('#')) {
        hash = url;
      } else {
        try { hash = new URL(url).hash; } catch {}
      }
      if (hash) {
        const id = hash.replace(/^#\/?/, "");
        const note = window.document.getElementById(id);
        if (note !== null) {
          try {
            const html = processXRef(id, note.cloneNode(true));
            instance.setContent(html);
          } finally {
            instance.enable();
            instance.show();
          }
        } else {
          // See if we can fetch this
          fetch(url.split('#')[0])
          .then(res => res.text())
          .then(html => {
            const parser = new DOMParser();
            const htmlDoc = parser.parseFromString(html, "text/html");
            const note = htmlDoc.getElementById(id);
            if (note !== null) {
              const html = processXRef(id, note);
              instance.setContent(html);
            } 
          }).finally(() => {
            instance.enable();
            instance.show();
          });
        }
      } else {
        // See if we can fetch a full url (with no hash to target)
        // This is a special case and we should probably do some content thinning / targeting
        fetch(url)
        .then(res => res.text())
        .then(html => {
          const parser = new DOMParser();
          const htmlDoc = parser.parseFromString(html, "text/html");
          const note = htmlDoc.querySelector('main.content');
          if (note !== null) {
            // This should only happen for chapter cross references
            // (since there is no id in the URL)
            // remove the first header
            if (note.children.length > 0 && note.children[0].tagName === "HEADER") {
              note.children[0].remove();
            }
            const html = processXRef(null, note);
            instance.setContent(html);
          } 
        }).finally(() => {
          instance.enable();
          instance.show();
        });
      }
    }, function(instance) {
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            div.style.left = 0;
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
        // Handle positioning of the toggle
    window.addEventListener(
      "resize",
      throttle(() => {
        elRect = undefined;
        if (selectedAnnoteEl) {
          selectCodeLines(selectedAnnoteEl);
        }
      }, 10)
    );
    function throttle(fn, ms) {
    let throttle = false;
    let timer;
      return (...args) => {
        if(!throttle) { // first call gets through
            fn.apply(this, args);
            throttle = true;
        } else { // all the others get throttled
            if(timer) clearTimeout(timer); // cancel #2
            timer = setTimeout(() => {
              fn.apply(this, args);
              timer = throttle = false;
            }, ms);
        }
      };
    }
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
<script src="https://utteranc.es/client.js" repo="pat-alt/pat-alt.github.io" issue-term="pathname" theme="github-light" crossorigin="anonymous" async="">
</script><div class="modal fade" id="quarto-embedded-source-code-modal" tabindex="-1" aria-labelledby="quarto-embedded-source-code-modal-label" aria-hidden="true"><div class="modal-dialog modal-dialog-scrollable"><div class="modal-content"><div class="modal-header"><h5 class="modal-title" id="quarto-embedded-source-code-modal-label">Source Code</h5><button class="btn-close" data-bs-dismiss="modal"></button></div><div class="modal-body"><div class="">
<div class="sourceCode" id="cb12" data-shortcodes="false"><pre class="sourceCode numberSource markdown number-lines code-with-copy"><code class="sourceCode markdown"><span id="cb12-1"><a href="#cb12-1"></a><span class="co">---</span></span>
<span id="cb12-2"><a href="#cb12-2"></a><span class="an">title:</span><span class="co"> TrillionDollarWords.jl</span></span>
<span id="cb12-3"><a href="#cb12-3"></a><span class="an">subtitle:</span><span class="co"> The Trillion Dollar Words dataset and model in Julia</span></span>
<span id="cb12-4"><a href="#cb12-4"></a><span class="an">date:</span><span class="co"> '2024-02-18'</span></span>
<span id="cb12-5"><a href="#cb12-5"></a><span class="an">categories:</span></span>
<span id="cb12-6"><a href="#cb12-6"></a><span class="co">  - llm</span></span>
<span id="cb12-7"><a href="#cb12-7"></a><span class="co">  - mechanistic interpretability</span></span>
<span id="cb12-8"><a href="#cb12-8"></a><span class="co">  - finance</span></span>
<span id="cb12-9"><a href="#cb12-9"></a><span class="co">  - Julia</span></span>
<span id="cb12-10"><a href="#cb12-10"></a><span class="an">description:</span><span class="co"> &gt;-</span></span>
<span id="cb12-11"><a href="#cb12-11"></a><span class="co">  A short post introducing a small new Julia package that facilitates working with the Trillion Dollar Words dataset and model published in a recent ACL 2023 [paper](https://arxiv.org/abs/2305.07972).</span></span>
<span id="cb12-12"><a href="#cb12-12"></a><span class="an">image:</span><span class="co"> intro.jpeg</span></span>
<span id="cb12-13"><a href="#cb12-13"></a><span class="an">draft:</span><span class="co"> false</span></span>
<span id="cb12-14"><a href="#cb12-14"></a><span class="an">code-fold:</span><span class="co"> show</span></span>
<span id="cb12-15"><a href="#cb12-15"></a><span class="co">---</span>  </span>
<span id="cb12-16"><a href="#cb12-16"></a></span>
<span id="cb12-19"><a href="#cb12-19"></a><span class="in">```{julia}</span></span>
<span id="cb12-20"><a href="#cb12-20"></a><span class="in">#| echo: false</span></span>
<span id="cb12-21"><a href="#cb12-21"></a></span>
<span id="cb12-22"><a href="#cb12-22"></a><span class="in">include("$(pwd())/blog/posts/trillion-dollar-words/src/setup.jl")</span></span>
<span id="cb12-23"><a href="#cb12-23"></a><span class="in">```</span></span>
<span id="cb12-24"><a href="#cb12-24"></a></span>
<span id="cb12-25"><a href="#cb12-25"></a>&lt;div class="intro-gif"&gt;</span>
<span id="cb12-26"><a href="#cb12-26"></a>  &lt;figure&gt;</span>
<span id="cb12-27"><a href="#cb12-27"></a>    &lt;img src="intro.jpeg"&gt;</span>
<span id="cb12-28"><a href="#cb12-28"></a>    &lt;figcaption&gt;Photo by &lt;a href="https://unsplash.com/@neonbrand?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"&gt;Kenny Eliason&lt;/a&gt; on &lt;a href="https://unsplash.com/photos/1-us-dollar-banknote-8fDhgAN5zG0?utm_content=creditCopyText&amp;utm_medium=referral&amp;utm_source=unsplash"&gt;Unsplash&lt;/a&gt;&lt;/figcaption&gt;</span>
<span id="cb12-29"><a href="#cb12-29"></a>  &lt;/figure&gt;</span>
<span id="cb12-30"><a href="#cb12-30"></a>&lt;/div&gt; </span>
<span id="cb12-31"><a href="#cb12-31"></a></span>
<span id="cb12-32"><a href="#cb12-32"></a></span>
<span id="cb12-33"><a href="#cb12-33"></a>In a recent <span class="co">[</span><span class="ot">post</span><span class="co">](../spurious-sparks/index.qmd)</span>, I questioned the idea that finding patterns in latent embeddings of models is indicative of AGI or even surprising. One of the models we investigate in our related <span class="co">[</span><span class="ot">paper</span><span class="co">](https://arxiv.org/abs/2402.03962)</span> <span class="co">[</span><span class="ot">@altmeyer2024position</span><span class="co">]</span> is the *FOMC-RoBERTa* model trained on the Trillion Dollar Words dataset, both of which were published by @shah2023trillion in a recent ACL 2023 paper: <span class="co">[</span><span class="ot">Trillion Dollar Words: A New Financial Dataset, Task &amp; Market Analysis</span><span class="co">](https://arxiv.org/abs/2305.07972)</span> <span class="co">[</span><span class="ot">@shah2023trillion</span><span class="co">]</span>. To run our experiments and facilitate working with the data and model in Julia, I have developed a small package: <span class="co">[</span><span class="ot">TrillionDollarWords.jl</span><span class="co">](https://github.com/pat-alt/TrillionDollarWords.jl)</span>. This short post introduces the package and its basic functionality.</span>
<span id="cb12-34"><a href="#cb12-34"></a></span>
<span id="cb12-35"><a href="#cb12-35"></a><span class="fu">## TrillionDollarWords.jl</span></span>
<span id="cb12-36"><a href="#cb12-36"></a></span>
<span id="cb12-37"><a href="#cb12-37"></a><span class="co">[</span><span class="al">![Stable](https://img.shields.io/badge/docs-stable-blue.svg)</span><span class="co">](https://pat-alt.github.io/TrillionDollarWords.jl/stable/)</span></span>
<span id="cb12-38"><a href="#cb12-38"></a><span class="co">[</span><span class="al">![Dev](https://img.shields.io/badge/docs-dev-blue.svg)</span><span class="co">](https://pat-alt.github.io/TrillionDollarWords.jl/dev/)</span></span>
<span id="cb12-39"><a href="#cb12-39"></a><span class="co">[</span><span class="al">![Build Status](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml/badge.svg?branch=main)</span><span class="co">](https://github.com/pat-alt/TrillionDollarWords.jl/actions/workflows/CI.yml?query=branch%3Amain)</span></span>
<span id="cb12-40"><a href="#cb12-40"></a><span class="co">[</span><span class="al">![Coverage](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl/branch/main/graph/badge.svg)</span><span class="co">](https://codecov.io/gh/pat-alt/TrillionDollarWords.jl)</span></span>
<span id="cb12-41"><a href="#cb12-41"></a><span class="co">[</span><span class="al">![Code Style: Blue](https://img.shields.io/badge/code%20style-blue-4495d1.svg)</span><span class="co">](https://github.com/invenia/BlueStyle)</span></span>
<span id="cb12-42"><a href="#cb12-42"></a></span>
<span id="cb12-43"><a href="#cb12-43"></a><span class="co">[</span><span class="ot">TrillionDollarWords.jl</span><span class="co">](https://github.com/pat-alt/TrillionDollarWords.jl)</span> is a light-weight package that provides Julia useres easy access to the Trillion Dollar Words dataset and model <span class="co">[</span><span class="ot">@shah2023trillion</span><span class="co">]</span>.</span>
<span id="cb12-44"><a href="#cb12-44"></a></span>
<span id="cb12-45"><a href="#cb12-45"></a>::: {.callout-note}</span>
<span id="cb12-46"><a href="#cb12-46"></a></span>
<span id="cb12-47"><a href="#cb12-47"></a><span class="fu">## Disclaimer  </span></span>
<span id="cb12-48"><a href="#cb12-48"></a></span>
<span id="cb12-49"><a href="#cb12-49"></a>Please note that I am not the author of the Trillion Dollar Words paper nor am I affiliated with the authors. The package was developed as a by-product of our research and is not officially endorsed by the authors of the paper. </span>
<span id="cb12-50"><a href="#cb12-50"></a></span>
<span id="cb12-51"><a href="#cb12-51"></a>:::</span>
<span id="cb12-52"><a href="#cb12-52"></a></span>
<span id="cb12-53"><a href="#cb12-53"></a>You can install the package from Julia's general registry as follows:</span>
<span id="cb12-54"><a href="#cb12-54"></a></span>
<span id="cb12-55"><a href="#cb12-55"></a><span class="in">``` julia</span></span>
<span id="cb12-56"><a href="#cb12-56"></a><span class="in">using Pkg</span></span>
<span id="cb12-57"><a href="#cb12-57"></a><span class="in">Pkg.add(url="TrillionDollarWords.jl")</span></span>
<span id="cb12-58"><a href="#cb12-58"></a><span class="in">```</span></span>
<span id="cb12-59"><a href="#cb12-59"></a></span>
<span id="cb12-60"><a href="#cb12-60"></a>To install the development version, use the following command:</span>
<span id="cb12-61"><a href="#cb12-61"></a></span>
<span id="cb12-62"><a href="#cb12-62"></a><span class="in">``` julia</span></span>
<span id="cb12-63"><a href="#cb12-63"></a><span class="in">using Pkg</span></span>
<span id="cb12-64"><a href="#cb12-64"></a><span class="in">Pkg.add(url="https://github.com/pat-alt/TrillionDollarWords.jl")</span></span>
<span id="cb12-65"><a href="#cb12-65"></a><span class="in">```</span></span>
<span id="cb12-66"><a href="#cb12-66"></a></span>
<span id="cb12-67"><a href="#cb12-67"></a><span class="fu">### Basic Functionality</span></span>
<span id="cb12-68"><a href="#cb12-68"></a></span>
<span id="cb12-69"><a href="#cb12-69"></a>The package provides the following functionality:</span>
<span id="cb12-70"><a href="#cb12-70"></a></span>
<span id="cb12-71"><a href="#cb12-71"></a><span class="ss">- </span>Load pre-processed data.</span>
<span id="cb12-72"><a href="#cb12-72"></a><span class="ss">- </span>Load the model proposed in the paper. </span>
<span id="cb12-73"><a href="#cb12-73"></a><span class="ss">- </span>Basic model inference: compute forward passes and layer-wise activations.</span>
<span id="cb12-74"><a href="#cb12-74"></a><span class="ss">- </span>Download pre-computed activations for probing the model.</span>
<span id="cb12-75"><a href="#cb12-75"></a></span>
<span id="cb12-76"><a href="#cb12-76"></a>The latter two are particularly useful for downstream tasks related to <span class="co">[</span><span class="ot">mechanistic interpretability</span><span class="co">](https://en.wikipedia.org/wiki/Large_language_model#Interpretation)</span>. In times of increasing scrutiny of AI models, it is important to understand how they work and what they have learned. Mechanistic interpretability is a promising approach to this end, as it aims to understand the model's internal representations and how they relate to the task at hand. As we make abundantly clear in our own <span class="co">[</span><span class="ot">paper</span><span class="co">](https://arxiv.org/abs/2402.03962)</span> <span class="co">[</span><span class="ot">@altmeyer2024position</span><span class="co">]</span>, interpretability is not a silver bullet, but merely a step towards understanding, monitoring and improving AI models.</span>
<span id="cb12-77"><a href="#cb12-77"></a></span>
<span id="cb12-78"><a href="#cb12-78"></a><span class="fu">### Loading the Data</span></span>
<span id="cb12-79"><a href="#cb12-79"></a></span>
<span id="cb12-80"><a href="#cb12-80"></a>The Trillion Dollar Words dataset is a collection of preprocessed sentences around 40,000 time-stamped sentences from meeting minutes, press conferences and speeches by members of the Federal Open Market Committee (FOMC) <span class="co">[</span><span class="ot">@shah2023trillion</span><span class="co">]</span>. The total sample period spans from January, 1996, to October, 2022. In order to train various rule-based models and large language models (LLM) to classify sentences as either ‘hawkish’, ‘dovish’ or ‘neutral’, they have manually annotated a subset of around 2,500 sentences. The best-performing model, a large BERT model with around 355 million parameters, was open-sourced on <span class="co">[</span><span class="ot">HuggingFace</span><span class="co">](https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves)</span>. The authors also link the sentences to market data, which makes it possible to study the relationship between language and financial markets. While the authors of the paper did publish their data, much of it is unfortunately scattered across CSV and Excel files stored in a public GitHub <span class="co">[</span><span class="ot">repo</span><span class="co">](https://github.com/gtfintechlab/fomc-hawkish-dovish)</span>. I have collected and merged that data, yielding a combined dataset with indexed sentences and additional metadata that may be useful for downstream tasks.</span>
<span id="cb12-81"><a href="#cb12-81"></a></span>
<span id="cb12-82"><a href="#cb12-82"></a>The entire dataset of all available sentences used in the paper can be loaded as follows:</span>
<span id="cb12-83"><a href="#cb12-83"></a></span>
<span id="cb12-86"><a href="#cb12-86"></a><span class="in">```{julia}</span></span>
<span id="cb12-87"><a href="#cb12-87"></a><span class="in">#| output: true</span></span>
<span id="cb12-88"><a href="#cb12-88"></a></span>
<span id="cb12-89"><a href="#cb12-89"></a><span class="in">using TrillionDollarWords</span></span>
<span id="cb12-90"><a href="#cb12-90"></a><span class="in">load_all_sentences() |&gt; show</span></span>
<span id="cb12-91"><a href="#cb12-91"></a><span class="in">```</span></span>
<span id="cb12-92"><a href="#cb12-92"></a></span>
<span id="cb12-93"><a href="#cb12-93"></a>The combined dataset is also available as a <span class="in">`DataFrame`</span> and can be loaded as follows:</span>
<span id="cb12-94"><a href="#cb12-94"></a></span>
<span id="cb12-97"><a href="#cb12-97"></a><span class="in">```{julia}</span></span>
<span id="cb12-98"><a href="#cb12-98"></a><span class="in">#| output: true</span></span>
<span id="cb12-99"><a href="#cb12-99"></a></span>
<span id="cb12-100"><a href="#cb12-100"></a><span class="in">load_all_data() |&gt; show</span></span>
<span id="cb12-101"><a href="#cb12-101"></a><span class="in">```</span></span>
<span id="cb12-102"><a href="#cb12-102"></a></span>
<span id="cb12-103"><a href="#cb12-103"></a>Additional functionality for data loading is available (see <span class="co">[</span><span class="ot">docs</span><span class="co">](https://www.paltmeyer.com/TrillionDollarWords.jl/dev/)</span>).</span>
<span id="cb12-104"><a href="#cb12-104"></a></span>
<span id="cb12-105"><a href="#cb12-105"></a><span class="fu">### Loading the Model</span></span>
<span id="cb12-106"><a href="#cb12-106"></a></span>
<span id="cb12-107"><a href="#cb12-107"></a>The model can be loaded with or without the classifier head (below without the head). Under the hood, this function uses <span class="co">[</span><span class="ot">Transformers.jl</span><span class="co">](https://github.com/chengchingwen/Transformers.jl)</span> to retrieve the model from <span class="co">[</span><span class="ot">HuggingFace</span><span class="co">](https://huggingface.co/gtfintechlab/FOMC-RoBERTa?text=A+very+hawkish+stance+excerted+by+the+doves)</span>. Any keyword arguments accepted by <span class="in">`Transformers.HuggingFace.HGFConfig`</span> can also be passed. For example, to load the model without the classifier head and enable access to layer-wise activations, the following command can be used:</span>
<span id="cb12-108"><a href="#cb12-108"></a></span>
<span id="cb12-111"><a href="#cb12-111"></a><span class="in">```{julia}</span></span>
<span id="cb12-112"><a href="#cb12-112"></a><span class="in">#| output: true</span></span>
<span id="cb12-113"><a href="#cb12-113"></a></span>
<span id="cb12-114"><a href="#cb12-114"></a><span class="in">load_model(; load_head=false, output_hidden_states=true) |&gt; show</span></span>
<span id="cb12-115"><a href="#cb12-115"></a><span class="in">```</span></span>
<span id="cb12-116"><a href="#cb12-116"></a></span>
<span id="cb12-117"><a href="#cb12-117"></a><span class="fu">### Basic Model Inference</span></span>
<span id="cb12-118"><a href="#cb12-118"></a></span>
<span id="cb12-119"><a href="#cb12-119"></a>Using the model and data, layer-wise activations can be computed as below (here for the first 5 sentences). When called on a <span class="in">`DataFrame`</span>, the <span class="in">`layerwise_activations`</span> returns a data frame that links activations to sentence identifiers. This makes it possible to relate activations to market data by using the <span class="in">`sentence_id`</span> key. Alternatively, <span class="in">`layerwise_activations`</span> also accepts a vector of sentences.</span>
<span id="cb12-120"><a href="#cb12-120"></a></span>
<span id="cb12-123"><a href="#cb12-123"></a><span class="in">```{julia}</span></span>
<span id="cb12-124"><a href="#cb12-124"></a><span class="in">#| output: true</span></span>
<span id="cb12-125"><a href="#cb12-125"></a></span>
<span id="cb12-126"><a href="#cb12-126"></a><span class="in">df = load_all_sentences()</span></span>
<span id="cb12-127"><a href="#cb12-127"></a><span class="in">mod = load_model(; load_head=false, output_hidden_states=true)</span></span>
<span id="cb12-128"><a href="#cb12-128"></a><span class="in">n = 5</span></span>
<span id="cb12-129"><a href="#cb12-129"></a><span class="in">queries = df[1:n, :]</span></span>
<span id="cb12-130"><a href="#cb12-130"></a><span class="in">layerwise_activations(mod, queries) |&gt; show</span></span>
<span id="cb12-131"><a href="#cb12-131"></a><span class="in">```</span></span>
<span id="cb12-132"><a href="#cb12-132"></a></span>
<span id="cb12-133"><a href="#cb12-133"></a><span class="fu">### Probe Findings</span></span>
<span id="cb12-134"><a href="#cb12-134"></a></span>
<span id="cb12-135"><a href="#cb12-135"></a>For our own <span class="co">[</span><span class="ot">research</span><span class="co">](https://arxiv.org/abs/2402.03962)</span> <span class="co">[</span><span class="ot">@altmeyer2024position</span><span class="co">]</span>, we have been interested in probing the model. This involves using linear models to estimate the relationship between layer-wise transformer embeddings and some outcome variable of interest <span class="co">[</span><span class="ot">@alain2018understanding</span><span class="co">]</span>. To do this, we first had to run a single forward pass for each sentence through the RoBERTa model and store the layerwise emeddings. As we have seen above, the package ships with functionality for doing just that, but to save others valuable GPU hours we have archived activations of the hidden state on the first entity token for each layer as <span class="co">[</span><span class="ot">artifacts</span><span class="co">](https://github.com/pat-alt/TrillionDollarWords.jl/releases/tag/activations_2024-01-17)</span>. To download the last-layer activations in an interactive Julia session, for example, users can proceed as follows:</span>
<span id="cb12-136"><a href="#cb12-136"></a></span>
<span id="cb12-137"><a href="#cb12-137"></a><span class="in">``` julia</span></span>
<span id="cb12-138"><a href="#cb12-138"></a><span class="in">using LazyArtifacts</span></span>
<span id="cb12-139"><a href="#cb12-139"></a></span>
<span id="cb12-140"><a href="#cb12-140"></a><span class="in">julia&gt; artifact"activations_layer_24"</span></span>
<span id="cb12-141"><a href="#cb12-141"></a><span class="in">```</span></span>
<span id="cb12-142"><a href="#cb12-142"></a></span>
<span id="cb12-143"><a href="#cb12-143"></a>We have found that despite the small sample size, the *FOMC-RoBERTa* model appears to have distilled useful representations for downstream tasks that it was not explicitly trained for. @fig-rmse-pca-128 below shows the average out-of-sample root mean squared error for predicting various market indicators from layer activations. Consistent with findings in related work <span class="co">[</span><span class="ot">@alain2018understanding</span><span class="co">]</span>, we find that performance typically improves for layers closer to the final output layer of the transformer model. The measured performance is at least on par with baseline autoregressive models. For more information on this, see also my other recent <span class="co">[</span><span class="ot">post</span><span class="co">](../spurious-sparks/index.qmd)</span>.</span>
<span id="cb12-144"><a href="#cb12-144"></a></span>
<span id="cb12-145"><a href="#cb12-145"></a><span class="al">![Out-of-sample root mean squared error (RMSE) for the linear probe plotted against *FOMC-RoBERTa*'s $n$-th layer for different indicators. The values correspond to averages computed across cross-validation folds, where we have used an expanding window approach to split the time series.](https://raw.githubusercontent.com/pat-alt/TrillionDollarWords.jl/11-activations-for-cls-head/dev/juliacon/rmse_pca_128.png)</span>{#fig-rmse-pca-128}</span>
<span id="cb12-146"><a href="#cb12-146"></a></span>
<span id="cb12-147"><a href="#cb12-147"></a><span class="fu">## Intended Purpose and Goals</span></span>
<span id="cb12-148"><a href="#cb12-148"></a></span>
<span id="cb12-149"><a href="#cb12-149"></a>I hope that this small package may be useful to members of the Julia community who are interested in the interplay between Economics, Finance and Artificial Intelligence. It should serve as a good starting point for the following ideas:</span>
<span id="cb12-150"><a href="#cb12-150"></a></span>
<span id="cb12-151"><a href="#cb12-151"></a><span class="ss">- </span>Fine-tune additional models on the classification task or other tasks of interest.</span>
<span id="cb12-152"><a href="#cb12-152"></a><span class="ss">- </span>Further model probing, e.g. using other market indicators not discussed in the original paper.</span>
<span id="cb12-153"><a href="#cb12-153"></a><span class="ss">- </span>Improve and extend the label annotations. </span>
<span id="cb12-154"><a href="#cb12-154"></a></span>
<span id="cb12-155"><a href="#cb12-155"></a>Any contributions are very much welcome.</span>
<span id="cb12-156"><a href="#cb12-156"></a></span>
<span id="cb12-157"><a href="#cb12-157"></a><span class="fu">## References</span></span>
</code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</div></div></div></div></div>
</div> <!-- /content -->
<footer class="footer">
  <div class="nav-footer">
    <div class="nav-footer-left">
<p>© 2024, Patrick Altmeyer</p>
</div>   
    <div class="nav-footer-center">
      &nbsp;
    <div class="toc-actions d-sm-block d-md-none"><ul><li><a href="https://github.com/pat-alt/pat-alt.github.io/issues/new" class="toc-action"><i class="bi bi-github"></i>Report an issue</a></li></ul></div></div>
    <div class="nav-footer-right">
      <ul class="footer-items list-unstyled">
    <li class="nav-item compact">
    <a class="nav-link" href="https://www.paltmeyer.com/">
      <i class="bi bi-house" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/pat-alt">
      <i class="bi bi-github" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://julialang.social/@patalt" rel="me">
      <i class="bi bi-mastodon" role="img">
</i> 
    </a>
  </li>  
    <li class="nav-item compact">
    <a class="nav-link" href="https://medium.com/\@patrick.altmeyer">
      <i class="bi bi-medium" role="img">
</i> 
    </a>
  </li>  
</ul>
    </div>
  </div>
</footer>




</body></html>