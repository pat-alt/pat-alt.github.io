<!DOCTYPE html><html lang="en-us" >

<head>
  <meta charset="utf-8" />
  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  
  
  
    <meta name="generator" content="Wowchemy 5.3.0 for Hugo" />
  

  

  
  
  
  
  
    
    
    
  
  

  <meta name="author" content="Patrick Altmeyer" />

  
  
  
    
  
  <meta name="description" content="An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia. See also the pure-play companion package [BayesLaplace.jl](https://www.paltmeyer.com/BayesLaplace.jl/dev/)" />

  
  <link rel="alternate" hreflang="en-us" href="https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/" />

  









  




  
  

  
  
  
    <meta name="theme-color" content="#D6EAF8" />
  

  
  
    
    <script src="/js/mathjax-config.js"></script>
  

  

  <link rel="stylesheet" href="/css/vendor-bundle.min.f1ecf783c14edc00c9320c205831ad8e.css" media="print" onload="this.media='all'">

  
  
  
    
    

    
    
    
    
      
      
    
    
    

    
    
    
      
    
    
      
      
        
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/github.min.css" crossorigin="anonymous" title="hl-light" media="print" onload="this.media='all'">
          <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/styles/dracula.min.css" crossorigin="anonymous" title="hl-dark" media="print" onload="this.media='all'" disabled>
        
      
    

    
    
    
      <link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.css" integrity="sha512-1xoFisiGdy9nvho8EgXuXvnpR5GAMSjFwp40gSRE3NwdUdIMIKuPa7bqoUhLD0O/5tPNhteAsE5XyyMi5reQVA==" crossorigin="anonymous" media="print" onload="this.media='all'">
    

    

    
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
    
      
      

      
      

      
        <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml.js" integrity="" crossorigin="anonymous" async></script>
      
    
      
      

      
      

      
    
      
      

      
      

      
    
  

  
  
  

  
  
  
  
  
  <link rel="stylesheet" href="/css/wowchemy.40407f54d4a096e100dee06058251631.css" />

  



  

  

  




  
  
  

  

  
    <link rel="manifest" href="/manifest.webmanifest" />
  

  <link rel="icon" type="image/png" href="/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_32x32_fill_lanczos_center_3.png" />
  <link rel="apple-touch-icon" type="image/png" href="/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_180x180_fill_lanczos_center_3.png" />

  <link rel="canonical" href="https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/" />

  
  
  
  
  
  
  
  
    
    
  
  
  <meta property="twitter:card" content="summary" />
  
    <meta property="twitter:site" content="@`r Sys.getenv(&#34;TWITTER_NAME&#34;)`" />
    <meta property="twitter:creator" content="@`r Sys.getenv(&#34;TWITTER_NAME&#34;)`" />
  
  <meta property="og:site_name" content="Patrick Altmeyer" />
  <meta property="og:url" content="https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/" />
  <meta property="og:title" content="Go deep, but also ... go Bayesian! | Patrick Altmeyer" />
  <meta property="og:description" content="An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia. See also the pure-play companion package [BayesLaplace.jl](https://www.paltmeyer.com/BayesLaplace.jl/dev/)" /><meta property="og:image" content="https://www.paltmeyer.com/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_512x512_fill_lanczos_center_3.png" />
    <meta property="twitter:image" content="https://www.paltmeyer.com/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_512x512_fill_lanczos_center_3.png" /><meta property="og:locale" content="en-us" />
  
    
      <meta
        property="article:published_time"
        content="2022-02-18T00:00:00&#43;00:00"
      />
    
    <meta property="article:modified_time" content="2022-02-18T00:00:00&#43;00:00">
  

  


    






  




<script type="application/ld+json">
{
  "@context": "https://schema.org",
  "@type": "BlogPosting",
  "mainEntityOfPage": {
    "@type": "WebPage",
    "@id": "https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/"
  },
  "headline": "Go deep, but also ... go Bayesian!",
  
  "datePublished": "2022-02-18T00:00:00Z",
  "dateModified": "2022-02-18T00:00:00Z",
  
  "author": {
    "@type": "Person",
    "name": "Patrick Altmeyer"
  },
  
  "publisher": {
    "@type": "Organization",
    "name": "Patrick Altmeyer",
    "logo": {
      "@type": "ImageObject",
      "url": "https://www.paltmeyer.com/media/icon_huf71aeb422d2dbec20c1fa0f7ef6934b2_32911_192x192_fill_lanczos_center_3.png"
    }
  },
  "description": "An introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia. See also the pure-play companion package [BayesLaplace.jl](https://www.paltmeyer.com/BayesLaplace.jl/dev/)"
}
</script>

  

  

  

  





  <title>Go deep, but also ... go Bayesian! | Patrick Altmeyer</title>
</head>


<body id="top" data-spy="scroll" data-offset="70" data-target="#TableOfContents" class="page-wrapper   " data-wc-page-id="8958c091e16bd2348c273651392f7ef9" >

  
  
  
  
  
  
  
  
  
  <script src="/js/wowchemy-init.min.8988fb2a4bba758785868cfcb5244555.js"></script>

  


<aside class="search-modal" id="search">
  <div class="container">
    <section class="search-header">

      <div class="row no-gutters justify-content-between mb-3">
        <div class="col-6">
          <h1>Search</h1>
        </div>
        <div class="col-6 col-search-close">
          <a class="js-search" href="#" aria-label="Close"><i class="fas fa-times-circle text-muted" aria-hidden="true"></i></a>
        </div>
      </div>

      <div id="search-box">
        
        <input name="q" id="search-query" placeholder="Search..." autocapitalize="off"
        autocomplete="off" autocorrect="off" spellcheck="false" type="search" class="form-control"
        aria-label="Search...">
        
      </div>

      
      

      

    </section>
    <section class="section-search-results">

      <div id="search-hits">
        
      </div>

    </section>
  </div>
</aside>



  <div class="page-header">
    












<nav class="navbar navbar-expand-lg navbar-light compensate-for-scrollbar" id="navbar-main">
  <div class="container-xl">

    
    <div class="d-none d-lg-inline-flex">
      <a class="navbar-brand" href="/">Patrick Altmeyer</a>
    </div>
    

    
    <button type="button" class="navbar-toggler" data-toggle="collapse"
            data-target="#navbar-content" aria-controls="navbar-content" aria-expanded="false" aria-label="Toggle navigation">
    <span><i class="fas fa-bars"></i></span>
    </button>
    

    
    <div class="navbar-brand-mobile-wrapper d-inline-flex d-lg-none">
      <a class="navbar-brand" href="/">Patrick Altmeyer</a>
    </div>
    

    
    
    <div class="navbar-collapse main-menu-item collapse justify-content-start" id="navbar-content">

      
      <ul class="navbar-nav d-md-inline-flex">
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
            
            
            
              
            
            
          
        

        <li class="nav-item">
          <a class="nav-link " href="/#hero"><span>Home</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/work/"><span>Work and Outreach</span></a>
        </li>

        
        

        

        
        
        
          
        

        

        
        
        
        

        
          
            
            
          
          
        

        <li class="nav-item">
          <a class="nav-link " href="/about/"><span>About me</span></a>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Code</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/AlgorithmicRecourse.jl/dev/"><span>AlgorithmicRecourse.jl</span></a>
            
              <a class="dropdown-item" href="/BayesLaplace.jl/dev/"><span>BayesLaplace.jl</span></a>
            
          </div>
        </li>

        
        

        
        <li class="nav-item dropdown">
          <a href="#" class="nav-link dropdown-toggle" data-toggle="dropdown" aria-haspopup="true"><span>Other</span><span class="caret"></span>
          </a>
          <div class="dropdown-menu">
            
              <a class="dropdown-item" href="/tai/"><span>Trustworthy AI</span></a>
            
          </div>
        </li>

        
        

      

        
      </ul>
    </div>

    <ul class="nav-icons navbar-nav flex-row ml-auto d-flex pl-md-2">

      
      
        
          
          <li class="nav-item d-none d-lg-inline-flex">
            <a class="nav-link" href="https://twitter.com/paltmey" data-toggle="tooltip" data-placement="bottom" title="Follow me on Twitter" target="_blank" rel="noopener" aria-label="Follow me on Twitter">
              <i class="fab fa-twitter" aria-hidden="true"></i>
            </a>
          </li>
        
      

      
      
      <li class="nav-item">
        <a class="nav-link js-search" href="#" aria-label="Search"><i class="fas fa-search" aria-hidden="true"></i></a>
      </li>
      

      
      
      <li class="nav-item dropdown theme-dropdown">
        <a href="#" class="nav-link" data-toggle="dropdown" aria-haspopup="true" aria-label="Display preferences">
          <i class="fas fa-moon" aria-hidden="true"></i>
        </a>
        <div class="dropdown-menu">
          <a href="#" class="dropdown-item js-set-theme-light">
            <span>Light</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-dark">
            <span>Dark</span>
          </a>
          <a href="#" class="dropdown-item js-set-theme-auto">
            <span>Automatic</span>
          </a>
        </div>
      </li>
      

      
      

    </ul>

  </div>
</nav>


  </div>

  <div class="page-body">
    <article class="article">

  












  

  
  
  
<div class="article-container pt-3">
  <h1>Go deep, but also ... go Bayesian!</h1>

  
  <p class="page-subtitle">Effortless Bayesian Deep Learning in Julia</p>
  

  
    


<div class="article-metadata">

  
  

  
  <span class="article-date">
    
    
      
    
    Feb 18, 2022
  </span>
  

  

  
  <span class="middot-divider"></span>
  <span class="article-reading-time">
    12 min read
  </span>
  

  
  
  
  
  
  

  
  
  <span class="middot-divider"></span>
  <span class="article-categories">
    <i class="fas fa-folder mr-1"></i><a href="/category/trustworthy-ai/">Trustworthy AI</a></span>
  

</div>

    





  
</div>



  <div class="article-container">

    <div class="article-style">
      <div class="intro-gif">
<figure>
<img src="www/anim.gif">
<figcaption>
A Bayesian Neural Network gradually learns.
</figcaption>
</figure>
</div>
<p>Deep learning has dominated AI research in recent years<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup> - but how
much promise does it really hold? That is very much an ongoing and
increasingly polarising debate that you can follow live on
<a href="https://twitter.com/ilyasut/status/1491554478243258368" target="_blank" rel="noopener">Twitter</a>. On
one side you have optimists like Ilya Sutskever, chief scientist of
OpenAI, who believes that large deep neural networks may already be
slightly conscious - that&rsquo;s &ldquo;may&rdquo; and &ldquo;slightly&rdquo; and only if you just go
deep enough? On the other side you have prominent skeptics like Judea
Pearl who has long since argued that deep learning still boils down to
curve fitting - purely associations and not even remotely intelligent
(Pearl and Mackenzie 2018).</p>
<h2 id="the-case-for-bayesian-deep-learning">The case for Bayesian Deep Learning</h2>
<p>Whatever side of this entertaining debate you find yourself on, the
reality is that deep-learning systems have already been deployed at
large scale both in academia and industry. More pressing debates
therefore revolve around the trustworthiness of these existing systems.
How robust are they and in what way exactly do they arrive at decisions
that affect each and every one of us? Robustifying deep neural networks
generally involves some form of adversarial training, which is costly,
can hurt generalization (Raghunathan et al. 2019) and does ultimately
not guarantee stability (Bastounis, Hansen, and Vlačić 2021). With
respect to interpretability, surrogate explainers like LIME and SHAP are
among the most popular tools, but they too have been shown to lack
robustness (Slack et al. 2020).</p>
<p>Exactly why are deep neural networks unstable and in-transparent? Let
$\mathcal{D}={x,y}_{n=1}^N$ denote our feature-label pairs and let
$f(x;\theta)=y$ denote some deep neural network specified by its
parameters $\theta$. Then the first thing to note is that the number of
free parameters $\theta$ is typically huge (if you ask Mr Sutskever it
really probably cannot be huge enough!). That alone makes it very hard
to monitor and interpret the inner workings of deep-learning algorithms.
Perhaps more importantly though, the number of parameters <em>relative</em> to
the size of $\mathcal{D}$ is generally huge:</p>
<blockquote>
<p>[&hellip;] deep neural networks are typically very underspecified by the
available data, and [&hellip;] parameters [therefore] correspond to a
diverse variety of compelling explanations for the data. (Wilson 2020)</p>
</blockquote>
<p>In other words, training a single deep neural network may (and usually
does) lead to one random parameter specification that fits the
underlying data very well. But in all likelihood there are many other
specifications that also fit the data very well. This is both a strength
and vulnerability of deep learning: it is a strength because it
typically allows us to find one such &ldquo;compelling explanation&rdquo; for the
data with ease through stochastic optimization; it is a vulnerability
because one has to wonder:</p>
<blockquote>
<p>How compelling is an explanation really if it competes with many other
equally compelling, but potentially very different explanations?</p>
</blockquote>
<p>A scenario like this very much calls for treating predictions from deep
learning models probabilistically [Wilson (2020)]<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup><sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup>.</p>
<p>Formally, we are interested in estimating the posterior predictive
distribution as the following Bayesian model average (BMA):</p>
<p>$$
p(y|x,\mathcal{D}) = \int p(y|x,\theta)p(\theta|\mathcal{D})d\theta
$$</p>
<p>The integral implies that we essentially need many predictions from many
different specifications of $\theta$. Unfortunately, this means more
work for us or rather our computers. Fortunately though, researchers
have proposed many ingenious ways to approximate the equation above: Gal
and Ghahramani (2016) propose using dropout at test time while
Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over
an ensemble of just five models seems to do the trick. Still, despite
their simplicity and usefulness these approaches involve additional
computational costs compared to training just a single network. As we
shall see now though, another promising approach has recently entered
the limelight: <strong>Laplace approximation</strong> (LA).</p>
<p>If you have read my <a href="https://towardsdatascience.com/bayesian-logistic-regression-53df017ba90f" target="_blank" rel="noopener">previous
post</a>
on Bayesian Logistic Regression, then the term Laplace should already
sound familiar to you. As a matter of fact, we will see that all
concepts covered in that previous post can be naturally extended to deep
learning. While some of these concepts will be revisited below, I
strongly recommend you check out the previous post before reading on
here. Without further ado let us now see how LA can be used for truly
effortless deep learning.</p>
<h2 id="laplace-approximation">Laplace Approximation</h2>
<p>While LA was first proposed in the 18th century, it has so far not
attracted serious attention from the deep learning community largely
because it involves a possibly large Hessian computation. Daxberger et
al. (2021) are on a mission to change the perception that LA has no use
in DL: in their <a href="https://arxiv.org/pdf/2106.14806.pdf" target="_blank" rel="noopener">NeurIPS 2021
paper</a> they demonstrate
empirically that LA can be used to produce Bayesian model averages that
are at least at par with existing approaches in terms of uncertainty
quantification and out-of-distribution detection. They show that recent
advancements in autodifferentation can be leveraged to produce fast and
accurate approximations of the Hessian and even provide a fully-fledged
<a href="https://aleximmer.github.io/Laplace/" target="_blank" rel="noopener">Python library</a> that can be used
with any pretrained Torch model. For this post, I have built a much less
comprehensive, pure-play equivalent of their package in Julia -
<a href="https://www.paltmeyer.com/BayesLaplace.jl/dev/" target="_blank" rel="noopener">BayesLaplace.jl</a> can be
used with deep learning models built in <a href="https://fluxml.ai/" target="_blank" rel="noopener">Flux.jl</a>,
which is Julia&rsquo;s main DL library. As in the previous post on Bayesian
logistic regression I will rely on Julia code snippits instead of
equations to convey the underlying maths. If you&rsquo;re curious about the
maths, the <a href="https://arxiv.org/pdf/2106.14806.pdf" target="_blank" rel="noopener">NeurIPS 2021 paper</a>
provides all the detail you need.</p>
<h3 id="from-bayesian-logistic-regression-">From Bayesian Logistic Regression &hellip;</h3>
<p>Let&rsquo;s recap: in the case of logistic regression we had a assumed a
zero-mean Gaussian prior
$p(\mathbf{w}) \sim \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)=\mathcal{N} \left( \mathbf{w} | \mathbf{0}, \mathbf{H}_0^{-1} \right)$
for the weights that are used to compute logits
$\mu_n=\mathbf{w}^T\mathbf{x}_n$, which in turn are fed to a sigmoid
function to produce probabilities $p(y_n=1)=\sigma(\mu_n)$. We saw that
under this assumption solving the logistic regression problem
corresponds to minimizing the following differentiable loss function:</p>
<p>$$
\ell(\mathbf{w})= - \sum_{n}^N [y_n \log \mu_n + (1-y_n)\log (1-\mu_n)] + \frac{1}{2} (\mathbf{w}-\mathbf{w}_0)^T\mathbf{H}_0(\mathbf{w}-\mathbf{w}_0)
$$</p>
<p>As our first step towards Bayesian deep learning, we observe the
following: the loss function above corresponds to the objective faced by
a single-layer artificial neural network with sigmoid activation and
weight decay<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup>. In other words, regularized logistic regression is
equivalent to a very simple neural network architecture and hence it is
not surprising that underlying concepts can in theory be applied in much
the same way.</p>
<p>So let&rsquo;s quickly recap the next core concept: LA relies on the fact that
the second-order Taylor expansion of our loss function $\ell$ evaluated
at the <strong>maximum a posteriori</strong> (MAP) estimate
$\mathbf{\hat{w}}=\arg\max_{\mathbf{w}} p(\mathbf{w}|\mathcal{D})$
amounts to a multi-variate Gaussian distribution. In particular, that
Gaussian is centered around the MAP estimate with covariance equal to
the inverse Hessian evaluated at the mode
$\hat{\Sigma}=(\mathbf{H}(\mathbf{\hat{w}}))^{-1}$ (Murphy 2022).</p>
<p>That is basically all there is to the story: if we have a good estimate
of $\mathbf{H}(\mathbf{\hat{w}})$ we have an analytical expression for
an (approximate) posterior over parameters. So let&rsquo;s go ahead and start
by run Bayesian Logistic regression using <a href="https://fluxml.ai/" target="_blank" rel="noopener">Flux.jl</a>.
We begin by loading some required packages including
<a href="https://www.paltmeyer.com/BayesLaplace.jl/dev/" target="_blank" rel="noopener">BayesLaplace.jl</a>. It
ships with a helper function <code>toy_data_linear</code> that creates a toy data
set composed of linearly separable samples evenly balanced across the
two classes.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#6272a4"># Import libraries.</span>
<span style="color:#ff79c6">using</span> Flux, Plots, Random, PlotThemes, Statistics, BayesLaplace
theme(<span style="color:#f1fa8c">:juno</span>)
<span style="color:#6272a4"># Number of points to generate.</span>
xs, y <span style="color:#ff79c6">=</span> toy_data_linear(<span style="color:#bd93f9">100</span>)
X <span style="color:#ff79c6">=</span> hcat(xs<span style="color:#ff79c6">...</span>); <span style="color:#6272a4"># bring into tabular format</span>
data <span style="color:#ff79c6">=</span> zip(xs,y);
</code></pre></div></div>
<p>Then we proceed to prepare the single-layer neural network with weight
decay. The term $\lambda$ determines the strength of the $\ell2$
penalty: we regularize parameters $\theta$ more heavily for higher
values. Equivalently, we can say that from the Bayesian perspective it
governs the strength of the prior
$p(\theta) \sim \mathcal{N} \left( \theta | \mathbf{0}, \sigma_0^2 \mathbf{I} \right)= \mathcal{N} \left( \mathbf{w} | \mathbf{0}, \lambda_0^{-2} \mathbf{I} \right)$:
a higher value of $\lambda$ indicates a higher conviction about our
prior belief that $\theta=\mathbf{0}$, which is of course equivalent to
regularizing more heavily. The exact choice of $\lambda=0.5$ for this
toy example is somewhat arbitrary (it made for good visualizations
below). Note that I have used $\theta$ to denote our neural parameters
to distinguish the case from Bayesian logistic regression, but we are in
fact still solving the same problem.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">nn <span style="color:#ff79c6">=</span> Chain(Dense(<span style="color:#bd93f9">2</span>,<span style="color:#bd93f9">1</span>))
λ <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0.5</span>
sqnorm(x) <span style="color:#ff79c6">=</span> sum(abs2, x)
weight_regularization(λ<span style="color:#ff79c6">=</span>λ) <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">1</span><span style="color:#ff79c6">/</span><span style="color:#bd93f9">2</span> <span style="color:#ff79c6">*</span> λ<span style="color:#ff79c6">^</span><span style="color:#bd93f9">2</span> <span style="color:#ff79c6">*</span> sum(sqnorm, Flux<span style="color:#ff79c6">.</span>params(nn))
loss(x, y) <span style="color:#ff79c6">=</span> Flux<span style="color:#ff79c6">.</span>Losses<span style="color:#ff79c6">.</span>logitbinarycrossentropy(nn(x), y) <span style="color:#ff79c6">+</span> weight_regularization();
</code></pre></div></div>
<p>Before we apply Laplace approximation we train our model:</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#ff79c6">using</span> Flux<span style="color:#ff79c6">.</span>Optimise<span style="color:#ff79c6">:</span> update!, ADAM
opt <span style="color:#ff79c6">=</span> ADAM()
epochs <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">50</span>

<span style="color:#ff79c6">for</span> epoch <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">1</span><span style="color:#ff79c6">:</span>epochs
  <span style="color:#ff79c6">for</span> d <span style="color:#ff79c6">in</span> data
    gs <span style="color:#ff79c6">=</span> gradient(params(nn)) <span style="color:#ff79c6">do</span>
      l <span style="color:#ff79c6">=</span> loss(d<span style="color:#ff79c6">...</span>)
    <span style="color:#ff79c6">end</span>
    update!(opt, params(nn), gs)
  <span style="color:#ff79c6">end</span>
<span style="color:#ff79c6">end</span>
</code></pre></div></div>
<p>Up until this point we have just followed the standard recipe for
training a regularized artificial neural network in
<a href="https://fluxml.ai/" target="_blank" rel="noopener">Flux.jl</a> for a simple binary classification task.
To compute the Laplace approximation using
<a href="https://www.paltmeyer.com/BayesLaplace.jl/dev/" target="_blank" rel="noopener">BayesLaplace.jl</a> we
need just two more lines of code:</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">la <span style="color:#ff79c6">=</span> laplace(nn, λ<span style="color:#ff79c6">=</span>λ)
fit!(la, data);
</code></pre></div></div>
<p>Under the hood the Hessian is approximated through the <strong>empirical
Fisher</strong>, which can be computed using only the gradients of our loss
function $\nabla_{\theta}\ell(f(\mathbf{x}_n;\theta,y_n))$ where
${\mathbf{x}_n,y_n}$ are training data (see <a href="https://arxiv.org/pdf/2106.14806.pdf" target="_blank" rel="noopener">NeurIPS 2021
paper</a> for details). Finally,
<a href="https://www.paltmeyer.com/BayesLaplace.jl/dev/" target="_blank" rel="noopener">BayesLaplace.jl</a> ships
with a function
<code>predict(𝑳::LaplaceRedux, X::AbstractArray; link_approx=:probit)</code> that
computes the posterior predictive using a probit approximation, much
like we saw in the previous post. That function is used under the hood
of the <code>plot_contour</code> function below to create the right panel of
<a href="#fig-logit">Figure 1</a>. It visualizes the posterior predictive
distribution in the 2D feature space. For comparison I have added the
corresponding plugin estimate as well. Note how for the Laplace
approximation the predicted probabilities fan out indicating that
confidence decrease in regions scarce of data.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">p_plugin <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Plugin&#34;</span>,type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">:plugin</span>);
p_laplace <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Laplace&#34;</span>)
<span style="color:#6272a4"># Plot the posterior distribution with a contour plot.</span>
plt <span style="color:#ff79c6">=</span> plot(p_plugin, p_laplace, layout<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1</span>,<span style="color:#bd93f9">2</span>), size<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1000</span>,<span style="color:#bd93f9">400</span>))
savefig(plt, <span style="color:#f1fa8c">&#34;www/posterior_predictive_logit.png&#34;</span>);
</code></pre></div></div>
<figure>
<img src="www/posterior_predictive_logit.png" id="fig-logit"
alt="Figure 1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right)." />
<figcaption aria-hidden="true">Figure 1: Posterior predictive
distribution of Logistic regression in the 2D feature space using plugin
estimator (left) and Laplace approximation (right).</figcaption>
</figure>
<h3 id="-to-bayesian-neural-networks">&hellip; to Bayesian Neural Networks</h3>
<p>Now let&rsquo;s step it up a notch: we will repeat the exercise from above,
but this time for data that is not linearly separable using a simple MLP
instead of the single-layer neural network we used above. The code below
is almost the same as above, so I will not go through the various steps
again.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia"><span style="color:#6272a4"># Number of points to generate:</span>
xs, y <span style="color:#ff79c6">=</span> toy_data_non_linear(<span style="color:#bd93f9">200</span>)
X <span style="color:#ff79c6">=</span> hcat(xs<span style="color:#ff79c6">...</span>); <span style="color:#6272a4"># bring into tabular format</span>
data <span style="color:#ff79c6">=</span> zip(xs,y)

<span style="color:#6272a4"># Build MLP:</span>
n_hidden <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">32</span>
D <span style="color:#ff79c6">=</span> size(X)[<span style="color:#bd93f9">1</span>]
nn <span style="color:#ff79c6">=</span> Chain(
    Dense(D, n_hidden, σ),
    Dense(n_hidden, <span style="color:#bd93f9">1</span>)
)  
λ <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">0.01</span>
sqnorm(x) <span style="color:#ff79c6">=</span> sum(abs2, x)
weight_regularization(λ<span style="color:#ff79c6">=</span>λ) <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">1</span><span style="color:#ff79c6">/</span><span style="color:#bd93f9">2</span> <span style="color:#ff79c6">*</span> λ<span style="color:#ff79c6">^</span><span style="color:#bd93f9">2</span> <span style="color:#ff79c6">*</span> sum(sqnorm, Flux<span style="color:#ff79c6">.</span>params(nn))
loss(x, y) <span style="color:#ff79c6">=</span> Flux<span style="color:#ff79c6">.</span>Losses<span style="color:#ff79c6">.</span>logitbinarycrossentropy(nn(x), y) <span style="color:#ff79c6">+</span> weight_regularization()

<span style="color:#6272a4"># Training:</span>
epochs <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">200</span>
<span style="color:#ff79c6">for</span> epoch <span style="color:#ff79c6">=</span> <span style="color:#bd93f9">1</span><span style="color:#ff79c6">:</span>epochs
  <span style="color:#ff79c6">for</span> d <span style="color:#ff79c6">in</span> data
    gs <span style="color:#ff79c6">=</span> gradient(params(nn)) <span style="color:#ff79c6">do</span>
      l <span style="color:#ff79c6">=</span> loss(d<span style="color:#ff79c6">...</span>)
    <span style="color:#ff79c6">end</span>
    update!(opt, params(nn), gs)
  <span style="color:#ff79c6">end</span>
<span style="color:#ff79c6">end</span>
</code></pre></div></div>
<p>Fitting the Laplace approximation is also analogous, but note that this
we have added an argument: <code>subset_of_weights=:last_layer</code>. This
specifies that we only want to use the parameters of the last layer of
our MLP. While we could have used all of them
(<code>subset_of_weights=:all</code>), Daxberger et al. (2021) find that the
last-layer Laplace approximation produces satisfying results, while be
computationally cheaper. <a href="#fig-mlp">Figure 2</a> demonstrates that once
again the Laplace approximation yields a posterior predictive
distribution that is more conservative than the over-confident plugin
estimate.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">la <span style="color:#ff79c6">=</span> laplace(nn, λ<span style="color:#ff79c6">=</span>λ, subset_of_weights<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">:last_layer</span>)
fit!(la, data);
p_plugin <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Plugin&#34;</span>,type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">:plugin</span>)
p_laplace <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Laplace&#34;</span>)
<span style="color:#6272a4"># Plot the posterior distribution with a contour plot.</span>
plt <span style="color:#ff79c6">=</span> plot(p_plugin, p_laplace, layout<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1</span>,<span style="color:#bd93f9">2</span>), size<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1000</span>,<span style="color:#bd93f9">400</span>))
savefig(plt, <span style="color:#f1fa8c">&#34;www/posterior_predictive_mlp.png&#34;</span>);
</code></pre></div></div>
<figure>
<img src="www/posterior_predictive_mlp.png" id="fig-mlp"
alt="Figure 2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right)." />
<figcaption aria-hidden="true">Figure 2: Posterior predictive
distribution of MLP in the 2D feature space using plugin estimator
(left) and Laplace approximation (right).</figcaption>
</figure>
<p>To see why this is a desirable outcome consider the zoomed out version
of <a href="#fig-mlp">Figure 2</a> below: the plugin estimator classifies with full
confidence in regions completely scarce of any data. Arguably Laplace
approximation produces a much more reasonable picture, even though it
too could likely be improved by fine-tuning our choice of $\lambda$ and
the neural network architecture.</p>
<div class="cell">
<div class="highlight"><pre tabindex="0" style="color:#f8f8f2;background-color:#282a36;-moz-tab-size:4;-o-tab-size:4;tab-size:4"><code class="language-julia" data-lang="julia">zoom<span style="color:#ff79c6">=-</span><span style="color:#bd93f9">50</span>
p_plugin <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Plugin&#34;</span>,type<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">:plugin</span>,zoom<span style="color:#ff79c6">=</span>zoom);
p_laplace <span style="color:#ff79c6">=</span> plot_contour(X<span style="color:#ff79c6">&#39;</span>,y,la;title<span style="color:#ff79c6">=</span><span style="color:#f1fa8c">&#34;Laplace&#34;</span>,zoom<span style="color:#ff79c6">=</span>zoom);
<span style="color:#6272a4"># Plot the posterior distribution with a contour plot.</span>
plt <span style="color:#ff79c6">=</span> plot(p_plugin, p_laplace, layout<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1</span>,<span style="color:#bd93f9">2</span>), size<span style="color:#ff79c6">=</span>(<span style="color:#bd93f9">1000</span>,<span style="color:#bd93f9">400</span>));
savefig(plt, <span style="color:#f1fa8c">&#34;www/posterior_predictive_mlp_zoom.png&#34;</span>);
</code></pre></div></div>
<figure>
<img src="www/posterior_predictive_mlp_zoom.png" id="fig-mlp-zoom"
alt="Figure 3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out." />
<figcaption aria-hidden="true">Figure 3: Posterior predictive
distribution of MLP in the 2D feature space using plugin estimator
(left) and Laplace approximation (right). Zoomed out.</figcaption>
</figure>
<h2 id="wrapping-up">Wrapping up</h2>
<p>Recent state-of-the-art research on neural information processing
suggests that Bayesian deep learning can be effortless: Laplace
approximation for deep neural networks appears to work very well and it
does so at minimal computational cost (Daxberger et al. 2021). This is
great news, because the case for turning Bayesian is strong: society
increasingly relies on complex automated decision-making systems that
need to be trustworthy. More and more of these systems involve deep
learning which in and of itself is not trustworthy. We have seen that
typically there exist various viable parameterizations of deep neural
networks each with their own distinct and compelling explanation for the
data at hand. When faced with many viable options, don&rsquo;t put all of your
eggs in one basket. In other words, go Bayesian!</p>
<h2 id="resources">Resources</h2>
<p>To get started with Bayesian deep learning I have found many useful and
free resources online, some of which are listed below:</p>
<ul>
<li><a href="https://turing.ml/dev/tutorials/03-bayesian-neural-network/" target="_blank" rel="noopener"><code>Turing.jl</code>
tutorial</a>
on Bayesian deep learning in Julia</li>
<li>Various RStudio AI blog posts including <a href="https://blogs.rstudio.com/ai/posts/2018-11-12-uncertainty_estimates_dropout/" target="_blank" rel="noopener">this
one</a>
and <a href="https://blogs.rstudio.com/ai/posts/2019-06-05-uncertainty-estimates-tfprobability/" target="_blank" rel="noopener">this
one</a></li>
<li><a href="https://medium.com/tensorflow/regression-with-probabilistic-layers-in-tensorflow-probability-e46ff5d37baf" target="_blank" rel="noopener">TensorFlow blog
post</a>
on regression with probabilistic layers</li>
<li>Kevin Murphy&rsquo;s <a href="https://probml.github.io/pml-book/book1.html" target="_blank" rel="noopener">draft text
book</a>, now also
available as print</li>
</ul>
<h2 id="references">References</h2>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-bastounis2021mathematics" class="csl-entry">
<p>Bastounis, Alexander, Anders C Hansen, and Verner Vlačić. 2021. &ldquo;The
Mathematics of Adversarial Attacks in AI&ndash;Why Deep Learning Is Unstable
Despite the Existence of Stable Neural Networks.&rdquo; <em>arXiv Preprint
arXiv:2109.06098</em>.</p>
</div>
<div id="ref-daxberger2021laplace" class="csl-entry">
<p>Daxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen,
Matthias Bauer, and Philipp Hennig. 2021. &ldquo;Laplace Redux-Effortless
Bayesian Deep Learning.&rdquo; <em>Advances in Neural Information Processing
Systems</em> 34.</p>
</div>
<div id="ref-gal2016dropout" class="csl-entry">
<p>Gal, Yarin, and Zoubin Ghahramani. 2016. &ldquo;Dropout as a Bayesian
Approximation: Representing Model Uncertainty in Deep Learning.&rdquo; In
<em>International Conference on Machine Learning</em>, 1050&ndash;59. PMLR.</p>
</div>
<div id="ref-lakshminarayanan2016simple" class="csl-entry">
<p>Lakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016.
&ldquo;Simple and Scalable Predictive Uncertainty Estimation Using Deep
Ensembles.&rdquo; <em>arXiv Preprint arXiv:1612.01474</em>.</p>
</div>
<div id="ref-murphy2022probabilistic" class="csl-entry">
<p>Murphy, Kevin P. 2022. <em>Probabilistic Machine Learning: An
Introduction</em>. MIT Press.</p>
</div>
<div id="ref-pearl2018book" class="csl-entry">
<p>Pearl, Judea, and Dana Mackenzie. 2018. <em>The Book of Why: The New
Science of Cause and Effect</em>. Basic books.</p>
</div>
<div id="ref-raghunathan2019adversarial" class="csl-entry">
<p>Raghunathan, Aditi, Sang Michael Xie, Fanny Yang, John C Duchi, and
Percy Liang. 2019. &ldquo;Adversarial Training Can Hurt Generalization.&rdquo;
<em>arXiv Preprint arXiv:1906.06032</em>.</p>
</div>
<div id="ref-slack2020fooling" class="csl-entry">
<p>Slack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu
Lakkaraju. 2020. &ldquo;Fooling Lime and Shap: Adversarial Attacks on Post Hoc
Explanation Methods.&rdquo; In <em>Proceedings of the AAAI/ACM Conference on AI,
Ethics, and Society</em>, 180&ndash;86.</p>
</div>
<div id="ref-wilson2020case" class="csl-entry">
<p>Wilson, Andrew Gordon. 2020. &ldquo;The Case for Bayesian Deep Learning.&rdquo;
<em>arXiv Preprint arXiv:2001.10995</em>.</p>
</div>
</div>
<section class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1" role="doc-endnote">
<p>See for example <a href="https://www.technologyreview.com/2019/01/25/1436/we-analyzed-16625-papers-to-figure-out-where-ai-is-headed-next/" target="_blank" rel="noopener">this
article</a>
in the MIT Technology Review&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2" role="doc-endnote">
<p>In fact, not treating probabilistic deep learning models as such
is sheer madness because remember that the underlying parameters
$\theta$ are random variables. Frequentists and Bayesians alike will
tell you that relying on a single point estimate of random variables
is just nuts!&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3" role="doc-endnote">
<p>Proponents of Causal AI like Judea Pearl would argue that the
Bayesian treatment still does not go far enough: in their view model
explanations can only be truly compelling if they are causally
found.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4" role="doc-endnote">
<p>See this <a href="https://stats.stackexchange.com/a/500973/288736" target="_blank" rel="noopener">answer</a>
on Stack Exchange for a detailed discussion.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</section>

    </div>

    






<div class="article-tags">
  
  <a class="badge badge-light" href="/tag/bayes/">bayes</a>
  
  <a class="badge badge-light" href="/tag/deeplearning/">deeplearning</a>
  
  <a class="badge badge-light" href="/tag/julialang/">julialang</a>
  
</div>



<div class="share-box" aria-hidden="true">
  <ul class="share">
    
      
      
      
        
      
      
      
      <li>
        <a href="https://twitter.com/intent/tweet?url=https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/&amp;text=Go%20deep,%20but%20also%20...%20go%20Bayesian!" target="_blank" rel="noopener" class="share-btn-twitter">
          <i class="fab fa-twitter"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.facebook.com/sharer.php?u=https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/&amp;t=Go%20deep,%20but%20also%20...%20go%20Bayesian!" target="_blank" rel="noopener" class="share-btn-facebook">
          <i class="fab fa-facebook"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="mailto:?subject=Go%20deep,%20but%20also%20...%20go%20Bayesian!&amp;body=https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/" target="_blank" rel="noopener" class="share-btn-email">
          <i class="fas fa-envelope"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://www.linkedin.com/shareArticle?url=https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/&amp;title=Go%20deep,%20but%20also%20...%20go%20Bayesian!" target="_blank" rel="noopener" class="share-btn-linkedin">
          <i class="fab fa-linkedin-in"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="whatsapp://send?text=Go%20deep,%20but%20also%20...%20go%20Bayesian!%20https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/" target="_blank" rel="noopener" class="share-btn-whatsapp">
          <i class="fab fa-whatsapp"></i>
        </a>
      </li>
    
      
      
      
        
      
      
      
      <li>
        <a href="https://service.weibo.com/share/share.php?url=https://www.paltmeyer.com/post/effortless-bayesian-deep-learning-in-julia/&amp;title=Go%20deep,%20but%20also%20...%20go%20Bayesian!" target="_blank" rel="noopener" class="share-btn-weibo">
          <i class="fab fa-weibo"></i>
        </a>
      </li>
    
  </ul>
</div>











  
  



  
  
  
    
  
  
  
  <div class="media author-card content-widget-hr">
    
      
      <a href="https://www.paltmeyer.com/"><img class="avatar mr-3 avatar-circle" src="/author/patrick-altmeyer/avatar_hu3cfca0813412e5d3acdebb52b60777cc_515532_270x270_fill_q75_lanczos_center.jpg" alt="Patrick Altmeyer"></a>
    

    <div class="media-body">
      <h5 class="card-title"><a href="https://www.paltmeyer.com/">Patrick Altmeyer</a></h5>
      <h6 class="card-subtitle">PhD Candidate</h6>
      <p class="card-text">My research interests include Causal Inference, Probabilistic Machine Learning and Algorithmic Recourse.</p>
      <ul class="network-icon" aria-hidden="true">
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="/#contact" >
        <i class="fas fa-envelope"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://twitter.com/paltmey" target="_blank" rel="noopener">
        <i class="fab fa-twitter"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://scholar.google.com/citations?user=e7KRRa8AAAAJ&amp;hl=en" target="_blank" rel="noopener">
        <i class="fas fa-graduation-cap"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://github.com/pat-alt" target="_blank" rel="noopener">
        <i class="fab fa-github"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://www.linkedin.com/in/patrick-altmeyer-a2a25494/" target="_blank" rel="noopener">
        <i class="fab fa-linkedin"></i>
      </a>
    </li>
  
    
    
    
      
    
    
    
    
    
      
    
    <li>
      <a href="https://medium.com/@patrick.altmeyer" target="_blank" rel="noopener">
        <i class="fab fa-medium"></i>
      </a>
    </li>
  
</ul>

    </div>
  </div>









  
  
  

  
  
  

  
  <section id="comments">
    <script src="https://utteranc.es/client.js"
        repo="pat-alt/pat-alt.github.io"
        issue-term="title"
        label="🔮"
        theme="github-light"
        crossorigin="anonymous"
        async>
</script>

  </section>
  








  
  
  <div class="article-widget content-widget-hr">
    <h3>Related</h3>
    <ul>
      
      <li><a href="/post/bayesian-logistic-regression/">Bayesian Logistic Regression</a></li>
      
      <li><a href="/post/2021-02-01-a-peek-inside-the-black-box-interpreting-neural-networks/">A peek inside the &#39;Black Box&#39; - interpreting neural networks</a></li>
      
    </ul>
  </div>
  





  </div>
</article>
  </div>

  <div class="page-footer">
    
    
    <div class="container">
      <footer class="site-footer">

  



  

  

  
  <p class="powered-by">
    © 2022 Patrick Altmeyer
  </p>
  

  
  






  <p class="powered-by">
    
    
    
      
      
      
      
      
      
      Published with <a href="https://wowchemy.com/?utm_campaign=poweredby" target="_blank" rel="noopener">Wowchemy</a> — the free, <a href="https://github.com/wowchemy/wowchemy-hugo-modules" target="_blank" rel="noopener">open source</a> website builder that empowers creators.
    
  </p>
</footer>

    </div>
    
  </div>

  
<div id="modal" class="modal fade" role="dialog">
  <div class="modal-dialog">
    <div class="modal-content">
      <div class="modal-header">
        <h5 class="modal-title">Cite</h5>
        <button type="button" class="close" data-dismiss="modal" aria-label="Close">
          <span aria-hidden="true">&times;</span>
        </button>
      </div>
      <div class="modal-body">
        <pre><code class="tex hljs"></code></pre>
      </div>
      <div class="modal-footer">
        <a class="btn btn-outline-primary my-1 js-copy-cite" href="#" target="_blank">
          <i class="fas fa-copy"></i> Copy
        </a>
        <a class="btn btn-outline-primary my-1 js-download-cite" href="#" target="_blank">
          <i class="fas fa-download"></i> Download
        </a>
        <div id="modal-error"></div>
      </div>
    </div>
  </div>
</div>

      

    
    <script src="/js/vendor-bundle.min.b73dfaac3b6499dc997741748a7c3fe2.js"></script>

    
    
    
      
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.imagesloaded/4.1.4/imagesloaded.pkgd.min.js" integrity="sha256-lqvxZrPLtfffUl2G/e7szqSvPBILGbwmsGE1MKlOi0Q=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/jquery.isotope/3.0.6/isotope.pkgd.min.js" integrity="sha256-CBrpuqrMhXwcLLUd5tvQ4euBHCdh7wGlDfNz8vbu/iI=" crossorigin="anonymous"></script>
      

      
      

      

      
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/highlight.min.js" integrity="sha512-TDKKr+IvoqZnPzc3l35hdjpHD0m+b2EC2SrLEgKDRWpxf2rFCxemkgvJ5kfU48ip+Y+m2XVKyOCD85ybtlZDmw==" crossorigin="anonymous"></script>
        
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/r.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/python.min.js" crossorigin="anonymous"></script>
        
        <script src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.2.0/languages/latex.min.js" crossorigin="anonymous"></script>
        
      

    

    
    
    
      <script src="https://cdnjs.cloudflare.com/ajax/libs/leaflet/1.7.1/leaflet.min.js" integrity="sha512-SeiQaaDh73yrb56sTW/RgVdi/mMqNeM2oBwubFHagc5BkixSpP1fvqF47mKzPGWYSSy4RwbBunrJBQ4Co8fRWA==" crossorigin="anonymous"></script>
    

    
    

    
    
    
      
      <script id="search-hit-fuse-template" type="text/x-template">
        <div class="search-hit" id="summary-{{key}}">
          <div class="search-hit-content">
            <div class="search-hit-name">
              <a href="{{relpermalink}}">{{title}}</a>
              <div class="article-metadata search-hit-type">{{type}}</div>
              <p class="search-hit-description">{{snippet}}</p>
            </div>
          </div>
        </div>
      </script>
      
        <script src="https://cdnjs.cloudflare.com/ajax/libs/fuse.js/3.2.1/fuse.min.js" integrity="sha256-VzgmKYmhsGNNN4Ph1kMW+BjoYJM2jV5i4IlFoeZA9XI=" crossorigin="anonymous"></script>
        <script src="https://cdnjs.cloudflare.com/ajax/libs/mark.js/8.11.1/jquery.mark.min.js" integrity="sha256-4HLtjeVgH0eIB3aZ9mLYF6E8oU5chNdjU6p6rrXpl9U=" crossorigin="anonymous"></script>
      
    

    
    

    
    
    
    

    
    
      
      
      
      
      
      
      
    

    
    
    
    
    
    
    
    
      
      
    
    
    <script src="/en/js/wowchemy.min.d68ecd57c0ec1f1f61d65fd568f1c3a0.js"></script>

    
  <script async defer src="https://buttons.github.io/buttons.js"></script>




</body>
</html>
