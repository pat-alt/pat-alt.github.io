[
  {
    "objectID": "content/recent.html",
    "href": "content/recent.html",
    "title": "Patrick Altmeyer",
    "section": "",
    "text": "Tweets by paltmey"
  },
  {
    "objectID": "content/publications/index.html",
    "href": "content/publications/index.html",
    "title": "Publications",
    "section": "",
    "text": "Complete list of publications: [pdf], [html], [bibtex]\n\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#overview",
    "href": "content/talks/posts/2022-juliacon/laplace.html#overview",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "Overview",
    "text": "Overview\n\n\nThe Case for Bayesian Deep Learning\nLaplace Redux in Julia üì¶\n\nFrom Bayesian Logistic Regression ‚Ä¶\n‚Ä¶ to Bayesian Neural Networks.\n\nGoals and Ambitions üéØ"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#bayesian-model-averaging",
    "href": "content/talks/posts/2022-juliacon/laplace.html#bayesian-model-averaging",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "Bayesian Model Averaging",
    "text": "Bayesian Model Averaging\n\nDon‚Äôt put all your ü•ö in one üß∫.\n\n\n\nIn Deep Learning we typically maximise highly non-convex functions full of local optima and saddle points.\nThere may be many \\(\\hat\\theta_1, ..., \\hat\\theta_m\\) that are slightly different, but yield similar performance.\n\n\n\n\n[‚Ä¶] parameters correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\n\n\n\\(\\theta\\) is a random variable. Shouldn‚Äôt we treat it that way?\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\qquad(1)\\]\n\nIntractable!\n\n\n\nIn practice we typically rely on a plugin approximation (Murphy 2022).\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta \\approx p(y|x,\\hat\\theta)\n\\qquad(2)\\]\n\nYes, ‚Äúplugin‚Äù is literal ‚Ä¶ can we do better?"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#enter-bayesian-deep-learning",
    "href": "content/talks/posts/2022-juliacon/laplace.html#enter-bayesian-deep-learning",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "Enter: Bayesian Deep Learning üîÆ",
    "text": "Enter: Bayesian Deep Learning üîÆ\n\nYes, we can!\n\n\n\nPopular approaches include ‚Ä¶\n\n\nMCMC (see Turing)\n\n\nVariational Inference (Blundell et al. 2015)\n\n\nMonte Carlo Dropout (Gal and Ghahramani 2016)\n\n\nDeep Ensembles (Lakshminarayanan, Pritzel, and Blundell 2016)\n\n\n\nLaplace Redux (Immer, Korzepa, and Bauer (2020),Daxberger et al. (2021))\n\n\n\n\n\nFigure¬†1: Pierre-Simon Laplace as chancellor of the Senate under the First French Empire. Source: Wikipedia\n\n\n\n\n\n\n\nFigure¬†2: Simulation of changing posteriour predictive distribution. Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#laplace-approximation",
    "href": "content/talks/posts/2022-juliacon/laplace.html#laplace-approximation",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\n\nWe first need to estimate the weight posterior \\(p(\\theta|\\mathcal{D})\\) ‚Ä¶\n\n\nIdea üí°: Taylor approximation at the mode.\n\n\nGoing through the maths we find that this yields a Gaussian posteriour centered around the MAP estimate \\(\\hat\\theta\\) (see pp.¬†148/149 in Murphy (2022)).\nCovariance corresponds to inverse Hessian at the mode (in practice we may have to rely on approximations).\n\n\n\n\n\n\n\nUnnormalized log-posterior and corresponding Laplace Approximation. Source: Murphy (2022).\n\n\n\nNow we can rely on MC or Probit Approximation to compute posterior predictive (classification)."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#laplaceredux.jl---a-small-package",
    "href": "content/talks/posts/2022-juliacon/laplace.html#laplaceredux.jl---a-small-package",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "LaplaceRedux.jl - a small package üì¶",
    "text": "LaplaceRedux.jl - a small package üì¶\n  \n\nWhat started out as my first coding project Julia ‚Ä¶\n\n\n\nBig fan of learning by coding so after reading the first chapters of Murphy (2022) I decided to code up Bayesian Logisitic Regression from scratch.\nI also wanted to learn Julia at the time, so tried to hit two birds with one stone.\nOutcome: 1. This blog post. 2. I have since been hooked on Julia.\n\n\n\n\n‚Ä¶ has turned into a small package üì¶ with great potential.\n\n\n\nWhen coming across the NeurIPS 2021 paper on Laplace Redux for deep learning (Daxberger et al. 2021), I figured I could step it up a notch.\nOutcome: LaplaceRedux.jl and another blog post.\n\n\n\n\nSo let‚Äôs add the package ‚Ä¶\nusing Pkg\nPkg.add(\"https://github.com/pat-alt/LaplaceRedux.jl\")\n‚Ä¶ and use it.\n\nusing LaplaceRedux"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#from-bayesian-logistic-regression",
    "href": "content/talks/posts/2022-juliacon/laplace.html#from-bayesian-logistic-regression",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "From Bayesian Logistic Regression ‚Ä¶",
    "text": "From Bayesian Logistic Regression ‚Ä¶\n\n\nFrom maths ‚Ä¶\n\nWe assume a Gaussian prior for our weights ‚Ä¶ \\[\np(\\theta) \\sim \\mathcal{N} \\left( \\theta | \\mathbf{0}, \\lambda^{-1} \\mathbf{I} \\right)=\\mathcal{N} \\left( \\theta | \\mathbf{0}, \\mathbf{H}_0^{-1} \\right)\n\\qquad(3)\\]\n\n\n‚Ä¶ which corresponds to logit binary crossentropy loss with weight decay:\n\\[\n\\ell(\\theta)= - \\sum_{n}^N [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\\\ \\frac{1}{2} (\\theta-\\theta_0)^T\\mathbf{H}_0(\\theta-\\theta_0)\n\\qquad(4)\\]\n\n\nFor Logistic Regression we have the Hessian in closed form (p.¬†338 in Murphy (2022)):\n\\[\n\\nabla_{\\theta}\\nabla_{\\theta}^\\mathsf{T}\\ell(\\theta) = \\frac{1}{N} \\sum_{n}^N(\\mu_n(1-\\mu_n)\\mathbf{x}_n)\\mathbf{x}_n^\\mathsf{T} + \\mathbf{H}_0\n\\qquad(5)\\]\n\n\n‚Ä¶ to code\n\n# Hessian:\nfunction ‚àá‚àáùìÅ(Œ∏,Œ∏_0,H_0,X,y)\n    N = length(y)\n    Œº = sigmoid(Œ∏,X)\n    H = ‚àë(Œº[n] * (1-Œº[n]) * X[n,:] * X[n,:]' for n=1:N)\n    return H + H_0\nend\n\nGotta love Julia ‚ù§Ô∏èüíúüíö\n\n\n\nLogistic Regression can be done in Flux ‚Ä¶\n\nusing Flux\n# Initializing weights as zeros only for illustrative purposes:\nnn = Chain(Dense(zeros(1,2),zeros(1))) \n\n\n\n‚Ä¶ but now we autograd! Leveraged in LaplaceRedux.\n\nla = Laplace(nn, Œª=Œª)\nfit!(la, data)\n\n\n\n\nFigure¬†3: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#to-bayesian-neural-networks",
    "href": "content/talks/posts/2022-juliacon/laplace.html#to-bayesian-neural-networks",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "‚Ä¶ to Bayesian Neural Networks",
    "text": "‚Ä¶ to Bayesian Neural Networks\n\n\nCode\n\nAn actual MLP ‚Ä¶\n\n# Build MLP:\nn_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(\n      randn(n_hidden,D)./10,\n      zeros(n_hidden), œÉ\n    ),\n    Dense(\n      randn(1,n_hidden)./10,\n      zeros(1)\n    )\n)  \n\n\n\n‚Ä¶ same API call:\n\nla = Laplace(\n  nn, Œª=Œª, \n  subset_of_weights=:last_layer\n)\nfit!(la, data)\n\n\n\nResults\n\n\n\n\nFigure¬†4: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#a-quick-note-on-the-prior",
    "href": "content/talks/posts/2022-juliacon/laplace.html#a-quick-note-on-the-prior",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "A quick note on the prior",
    "text": "A quick note on the prior\nLow prior uncertainty \\(\\rightarrow\\) posterior dominated by prior. High prior uncertainty \\(\\rightarrow\\) posterior approaches MLE.\nLogistic Regression\n\n\n\nFigure¬†5: Prior uncertainty increases from left to right (Logsitic Regression). Image by author.\n\n\nMLP\n\n\n\nFigure¬†6: Prior uncertainty increases from left to right (MLP). Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#a-crucial-detail-i-skipped",
    "href": "content/talks/posts/2022-juliacon/laplace.html#a-crucial-detail-i-skipped",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "A crucial detail I skipped",
    "text": "A crucial detail I skipped\n\nWe‚Äôre really been using linearized neural networks ‚Ä¶\n\n\n\n\nMC fails\n\n\nCould do Monte Carlo for true BNN predictive, but this performs poorly when using approximations for the Hessian.\nInstead we rely on linear expansion of predictive around mode (Immer, Korzepa, and Bauer 2020).\nIntuition: Hessian approximation involves linearization, then so should the predictive.\n\n\n\n\nApplying the GNN approximation [‚Ä¶] turns the underlying probabilistic model locally from a BNN into a GLM [‚Ä¶] Because we have effectively done inference in the GGN-linearized model, we should instead predict using these modified features. ‚Äî Immer, Korzepa, and Bauer (2020)\n\n\n\n\n\n\nFigure¬†7: MC samples from the Laplace posterior (Lawrence 2001)."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#juliacon-2022-and-beyond",
    "href": "content/talks/posts/2022-juliacon/laplace.html#juliacon-2022-and-beyond",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "JuliaCon 2022 and beyond",
    "text": "JuliaCon 2022 and beyond\n\n\n\n\nTo JuliaCon ‚Ä¶\n\nLearn about Laplace Redux by implementing it in Julia.\n\n\nTurn code into a small package.\n\n\nSubmit to JuliaCon 2022 and share the idea.\n\n\n‚Ä¶ and beyond\n\nPackage is bare-bones at this point and needs a lot of work.\n\n\nGoal: reach same level of maturity as Python counterpart. (Beautiful work btw!)\nProblem: limited capacity and fairly new to Julia.\nSolution: find contributors ü§ó.\n\n\n\n\n\n\n\nPhoto by Ivan Diaz on Unsplash"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#specific-goals",
    "href": "content/talks/posts/2022-juliacon/laplace.html#specific-goals",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "Specific Goals",
    "text": "Specific Goals\n\n\nEasy\n\nStill missing support for multi-class and regression.\nDue diligence: peer review and unit testing.\n\nHarder\n\nHessian approximations still quadratically large: use factorizations.\nHyperparameter tuning: what about that prior?\nScaling things up: subnetwork inference.\nEarly stopping: do we really end up at the mode?\n‚Ä¶\n\n\n\n\n\nSource: Giphy"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#more-resources",
    "href": "content/talks/posts/2022-juliacon/laplace.html#more-resources",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "More Resources üìö",
    "text": "More Resources üìö\n\n\n\nRead on ‚Ä¶\n\n\nBlog post (1) ‚Äì Bayesian Logisitic Regression: [TDS, homepage].\nBlog post (2) ‚Äì Bayesian Deep Learning: [TDS, homepage].\nDetailed slide pack generously shared by Professor Jos√© Miguel Hern√°ndez-Lobato: [pdf]\nPackage docs.\n\n\n‚Ä¶ or even better: get involved! ü§ó"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/laplace.html#references",
    "href": "content/talks/posts/2022-juliacon/laplace.html#references",
    "title": "Effortless Bayesian Deep Learning through Laplace Redux",
    "section": "References",
    "text": "References\n\n\n\nEffortless Bayesian Deep Learning through Laplace Redux ‚Äì JuliaCon 2022 ‚Äì Patrick Altmeyer\n\n\n\nBlundell, Charles, Julien Cornebise, Koray Kavukcuoglu, and Daan Wierstra. 2015. ‚ÄúWeight Uncertainty in Neural Network.‚Äù In International Conference on Machine Learning, 1613‚Äì22. PMLR.\n\n\nDaxberger, Erik, Agustinus Kristiadi, Alexander Immer, Runa Eschenhagen, Matthias Bauer, and Philipp Hennig. 2021. ‚ÄúLaplace Redux-Effortless Bayesian Deep Learning.‚Äù Advances in Neural Information Processing Systems 34.\n\n\nGal, Yarin, and Zoubin Ghahramani. 2016. ‚ÄúDropout as a Bayesian Approximation: Representing Model Uncertainty in Deep Learning.‚Äù In International Conference on Machine Learning, 1050‚Äì59. PMLR.\n\n\nImmer, Alexander, Maciej Korzepa, and Matthias Bauer. 2020. ‚ÄúImproving Predictions of Bayesian Neural Networks via Local Linearization.‚Äù https://arxiv.org/abs/2008.08400.\n\n\nLakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. ‚ÄúSimple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.‚Äù https://arxiv.org/abs/1612.01474.\n\n\nLawrence, Neil David. 2001. ‚ÄúVariational Inference in Probabilistic Models.‚Äù PhD thesis, University of Cambridge.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nWilson, Andrew Gordon. 2020. ‚ÄúThe Case for Bayesian Deep Learning.‚Äù https://arxiv.org/abs/2001.10995."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/index.html",
    "href": "content/talks/posts/2022-juliacon/index.html",
    "title": "JuliaCon 2022",
    "section": "",
    "text": "In July, 2022, I gave three different talks at JuliaCon 2022."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/index.html#explaining-black-box-models-through-counterfactuals",
    "href": "content/talks/posts/2022-juliacon/index.html#explaining-black-box-models-through-counterfactuals",
    "title": "JuliaCon 2022",
    "section": "Explaining Black-Box Models through Counterfactuals",
    "text": "Explaining Black-Box Models through Counterfactuals\nYou can watch the video below. See here for the slides."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/index.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "href": "content/talks/posts/2022-juliacon/index.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "title": "JuliaCon 2022",
    "section": "Effortless Bayesian Deep Learning through Laplace Redux",
    "text": "Effortless Bayesian Deep Learning through Laplace Redux\nYou can watch the video below. See here for the slides."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/index.html#julia-and-quarto-a-match-made-in-heaven",
    "href": "content/talks/posts/2022-juliacon/index.html#julia-and-quarto-a-match-made-in-heaven",
    "title": "JuliaCon 2022",
    "section": "Julia and Quarto: a Match Made in Heaven? üå§Ô∏è",
    "text": "Julia and Quarto: a Match Made in Heaven? üå§Ô∏è\nSee here for the slides."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#overview",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#overview",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Overview",
    "text": "Overview\n\n\nThe Problem with Black Boxes ‚¨õ\n\nWhat are black-box models? Why do we need explainability?\n\nEnter: Counterfactual Explanations üîÆ\n\nWhat are they? What are they not?\n\nCounterfactual Explanations in Julia (and beyond!) üì¶\n\nIntroducing: CounterfactualExplanations.jl\nPackage architecture\nUsage examples - what can it do?\n\nGoals and Ambitions üéØ\n\nFuture developments - where can it go?\nContributor‚Äôs guide"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#short-lists-pandas-and-gibbons",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#short-lists-pandas-and-gibbons",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Short Lists, Pandas and Gibbons",
    "text": "Short Lists, Pandas and Gibbons\n\nFrom human to data-driven decision-making ‚Ä¶\n\n\n\nBlack-box models like deep neural networks are being deployed virtually everywhere.\nIncludes safety-critical and public domains: health care, autonomous driving, finance, ‚Ä¶\nMore likely than not that your loan or employment application is handled by an algorithm.\n\n\n\n\n‚Ä¶ where black boxes are recipe for disaster.\n\n\n\nWe have no idea what exactly we‚Äôre cooking up ‚Ä¶\n\nHave you received an automated rejection email? Why didn‚Äôt you ‚ÄúmEet tHe sHoRtLisTiNg cRiTeRia‚Äù? üôÉ\n\n‚Ä¶ but we do know that some of it is junk.\n\n\n\n\n\n\n\nFigure¬†1: Adversarial attacks on deep neural networks. Source: Goodfellow, Shlens, and Szegedy (2014)"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#weapons-of-math-destruction",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#weapons-of-math-destruction",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "‚ÄúWeapons of Math Destruction‚Äù",
    "text": "‚ÄúWeapons of Math Destruction‚Äù\n\n\n\n‚ÄúYou cannot appeal to (algorithms). They do not listen. Nor do they bend.‚Äù\n‚Äî Cathy O‚ÄôNeil in Weapons of Math Destruction, 2016\n\n\n\n\nFigure¬†2: Cathy O‚ÄôNeil. Source: Cathy O‚ÄôNeil a.k.a. mathbabe.\n\n\n\n\n\nIf left unchallenged, these properties of black-box models can create undesirable dynamics in automated decision-making systems:\n\nHuman operators in charge of the system have to rely on it blindly.\nIndividuals subject to the decisions generally have no way to challenge their outcome."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-1",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-1",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nCurrent Standard in ML\nWe typically want to maximize the likelihood of observing \\(\\mathcal{D}_n\\) under given parameters (Murphy 2022):\n\\[\n\\theta^* = \\arg \\max_{\\theta} p(\\mathcal{D}_n|\\theta)\n\\qquad(1)\\]\nCompute an MLE (or MAP) point estimate \\(\\hat\\theta = \\mathbb{E} \\theta^*\\) and use plugin approximation for prediction:\n\\[\np(y|x,\\mathcal{D}_n) \\approx p(y|x,\\hat\\theta)\n\\qquad(2)\\]\n\nIn an ideal world we can just use parsimonious and interpretable models like GLM (Rudin 2019), for which in many cases we can rely on asymptotic properties of \\(\\theta\\) to quantify uncertainty.\nIn practice these models often have performance limitations.\nBlack-box models like deep neural networks are popular, but they are also the very opposite of parsimonious.\n\nObjective"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-2",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-2",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nObjective\n\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn this setting it is often crucial to treat models probabilistically!\n\\[\np(y|x,\\mathcal{D}_n) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D}_n)d\\theta\n\\qquad(3)\\]\n\n\n\nProbabilistic models covered briefly today. More in my other talk on Laplace Redux ‚Ä¶"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-3",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#towards-trustworthy-ai-3",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\n\nWe can now make predictions ‚Äì great! But do we know how the predictions are actually being made?\n\n\nObjective\nWith the model trained for its task, we are interested in understanding how its predictions change in response to input changes.\n\\[\n\\nabla_x p(y|x,\\mathcal{D}_n;\\hat\\theta)\n\\qquad(4)\\]\n\n\nCounterfactual reasoning (in this context) boils down to simple questions: what if \\(x\\) (factual) \\(\\Rightarrow\\) \\(x\\prime\\) (counterfactual)?\nBy strategically perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.\nCounterfactual Explanations always have full fidelity by construction (as opposed to surrogate explanations, for example).\n\n\n\n\nImportant to realize that we are keeping \\(\\hat\\theta\\) constant!"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#a-framework-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#a-framework-for-counterfactual-explanations",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "A Framework for Counterfactual Explanations",
    "text": "A Framework for Counterfactual Explanations\n\nEven though [‚Ä¶] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the ‚Äúblack box‚Äù. (Wachter, Mittelstadt, and Russell 2017)\n\n\n\nFramework\n\nObjective originally proposed by Wachter, Mittelstadt, and Russell (2017) is as follows\n\\[\n\\min_{x\\prime \\in \\mathcal{X}} h(x\\prime) \\ \\ \\ \\mbox{s. t.} \\ \\ \\ M(x\\prime) = t\n\\qquad(5)\\]\nwhere \\(h\\) relates to the complexity of the counterfactual and \\(M\\) denotes the classifier.\n\n\nTypically this is approximated through regularization:\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) + \\lambda h(x\\prime)\n\\qquad(6)\\]\n\n\nIntuition\n\n\n\n\nFigure¬†3: A cat performing gradient descent in the feature space √† la Wachter, Mittelstadt, and Russell (2017)."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#counterfactuals-as-in-adversarial-examples",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#counterfactuals-as-in-adversarial-examples",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Adversarial Examples?",
    "text": "Counterfactuals ‚Ä¶ as in Adversarial Examples?\n\n\nYes and no!\n\nWhile both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.\n\n\n\nEffective counterfactuals should meet certain criteria ‚úÖ\n\n\ncloseness: the average distance between factual and counterfactual features should be small (Wachter, Mittelstadt, and Russell (2017))\nactionability: the proposed feature perturbation should actually be actionable (Ustun, Spangher, and Liu (2019), Poyiadzi et al. (2020))\nplausibility: the counterfactual explanation should be plausible to a human (Joshi et al. (2019))\nunambiguity: a human should have no trouble assigning a label to the counterfactual (Schut et al. (2021))\nsparsity: the counterfactual explanation should involve as few individual feature changes as possible (Schut et al. (2021))\nrobustness: the counterfactual explanation should be robust to domain and model shifts (Upadhyay, Joshi, and Lakkaraju (2021))\ndiversity: ideally multiple diverse counterfactual explanations should be provided (Mothilal, Sharma, and Tan (2020))\ncausality: counterfactual explanations reflect the structural causal model underlying the data generating process (Karimi et al. (2020), Karimi, Sch√∂lkopf, and Valera (2021))"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#counterfactuals-as-in-causal-inference",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#counterfactuals-as-in-causal-inference",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Causal Inference?",
    "text": "Counterfactuals ‚Ä¶ as in Causal Inference?\n\nNO!\n\n\n\nCausal inference: counterfactuals are thought of as unobserved states of the world that we would like to observe in order to establish causality.\n\nThe only way to do this is by actually interfering with the state of the world: \\(p(y|do(x),\\theta)\\).\nIn practice we can only move some individuals to the counterfactual state of the world and compare their outcomes to a control group.\nProvided we have controlled for confounders, properly randomized, ‚Ä¶ we can estimate an average treatment effect: \\(\\hat\\theta\\).\n\nCounterfactual Explanations: involves perturbing features after some model has been trained.\n\nWe end up comparing modeled outcomes \\(p(y|x,\\hat\\phi)\\) and \\(p(y|x\\prime,\\hat\\phi)\\) for individuals.\nWe have not magically solved causality.\n\n\n\n\nThe number of ostensibly pro data scientists confusing themselves into believing that \"counterfactual explanations\" capture real-world causality is just staggeringü§¶‚Äç‚ôÄÔ∏è. Where do we go from here? How can a community that doesn't even understand what's already known make advances?\n\n‚Äî Zachary Lipton (@zacharylipton) June 20, 2022"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#probabilistic-methods-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#probabilistic-methods-for-counterfactual-explanations",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Probabilistic Methods for Counterfactual Explanations",
    "text": "Probabilistic Methods for Counterfactual Explanations\nWhen people say that counterfactuals should look realistic or plausible, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:\n\\[\nx\\prime \\sim p(x)\n\\]\nBut how do we estimate \\(p(x)\\)? Two probabilistic approaches ‚Ä¶\n\n\nAPPROACH 1: use the model itselfAPPROACH 2: use some generative model\n\n\n\n\nSchut et al. (2021) note that by maximizing predictive probabilities \\(\\sigma(M(x\\prime))\\) for probabilistic models \\(M\\in\\mathcal{\\widetilde{M}}\\) one implicitly minimizes epistemic and aleotoric uncertainty.\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) \\ \\ \\ , \\ \\ \\ M\\in\\mathcal{\\widetilde{M}}\n\\qquad(7)\\]\n\n\n\n\nFigure¬†4: A cat performing gradient descent in the feature space √† la Schut et al. (2021)\n\n\n\n\n\n\n\n\nInstead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model (Joshi et al. 2019).\n\\[\nz\\prime = \\arg \\min_{z\\prime}  \\ell(M(dec(z\\prime)),t) + \\lambda h(x\\prime)\n\\qquad(8)\\]\nand\n\\[x\\prime = dec(z\\prime)\\]\nwhere \\(dec(\\cdot)\\) is the decoder function.\n\n\n\n\nFigure¬†5: Counterfactual (yellow) generated through latent space search (right panel) following Joshi et al. (2019). The corresponding counterfactual path in the feature space is shown in the left panel."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#limited-software-availability",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#limited-software-availability",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Limited Software Availability",
    "text": "Limited Software Availability\n\nWork currently scattered across different GitHub repositories ‚Ä¶\n\n\n\n\n\nOnly one unifying Python library: CARLA (Pawelczyk et al. 2021).\n\nComprehensive and (somewhat) extensible.\nBut not language-agnostic and some desirable functionality not supported.\nAlso not composable: each generator is treated as different class/entity.\n\nBoth R and Julia lacking any kind of implementation.\n\n\n\n\n\n\nPhoto by Volodymyr Hryshchenko on Unsplash."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#enter-counterfactualexplanations.jl",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#enter-counterfactualexplanations.jl",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Enter: CounterfactualExplanations.jl üì¶",
    "text": "Enter: CounterfactualExplanations.jl üì¶\n   \n\n‚Ä¶ until now!\n\n\n\n\n\nA unifying framework for generating Counterfactual Explanations.\nBuilt in Julia, but essentially language agnostic:\n\nCurrently supporting explanations for differentiable models built in Julia (e.g.¬†Flux) and torch (R and Python).\n\nDesigned to be easily extensible through dispatch.\nDesigned to be composable allowing users and developers to combine different counterfactual generators.\n\n\n\n\n\n\nPhoto by Denise Jans on Unsplash.\n\n\n\n\n\n\nJulia has an edge with respect to Trustworthy AI: it‚Äôs open-source, uniquely transparent and interoperable üî¥üü¢üü£"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#overview-1",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#overview-1",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Overview",
    "text": "Overview\n\nFigure¬†6: Overview of package architecture. Modules are shown in red, structs in green and functions in blue."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#generators",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#generators",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Generators",
    "text": "Generators\n\nusing CounterfactualExplanations, Plots, GraphRecipes\nplt = plot(AbstractGenerator, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))\nsavefig(plt, joinpath(www_path,\"generators.png\"))\n\n\nFigure¬†7: Type tree for AbstractGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#models",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#models",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Models",
    "text": "Models\n\nplt = plot(AbstractFittedModel, method=:tree, fontsize=10, nodeshape=:rect, size=(1000,700))\nsavefig(plt, joinpath(www_path,\"models.png\"))\n\n\nFigure¬†8: Type tree for AbstractFittedModel."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#a-simple-example",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#a-simple-example",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "A simple example",
    "text": "A simple example\n\n\n\nLoad and prepare some toy data.\nSelect a random sample.\nGenerate counterfactuals using different approaches.\n\n\n# Data:\nusing Random\nRandom.seed!(123)\nN = 100\nusing CounterfactualExplanations\nxs, ys = toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Randomly selected factual:\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))\n\n\n\n\n\nFigure¬†9: Synthetic data."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#generic-generator",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#generic-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Generic Generator",
    "text": "Generic Generator\n\n\nCode\n\nWe begin by instantiating the fitted model ‚Ä¶\n\n# Model\nw = [1.0 1.0] # estimated coefficients\nb = 0 # estimated bias\nM = LogisticModel(w, [b])\n\n\n\n‚Ä¶ then based on its prediction for \\(x\\) we choose the opposite label as our target ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ and finally generate the counterfactual.\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\nOutput\n\n\n‚Ä¶ et voil√†!\n\n\n\n\nFigure¬†10: Counterfactual path (left) and predicted probability (right) for GenericGenerator. The contour (left) shows the predicted probabilities of the classifier (Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#greedy-generator",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#greedy-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Greedy Generator",
    "text": "Greedy Generator\n\n\nCode\n\nThis time we use a Bayesian classifier ‚Ä¶\n\nusing LinearAlgebra\nŒ£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\nŒº = hcat(b, w)\nM = BayesianLogisticModel(Œº, Œ£)\n\n\n\n‚Ä¶ and once again choose our target label as before ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ to then finally use greedy search to find a counterfactual.\n\n# Counterfactual search:\nparams = GreedyGeneratorParams(\n  Œ¥ = 0.5,\n  n = 10\n)\ngenerator = GreedyGenerator(;params=params)\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\nOutput\n\n\nIn this case the Bayesian approach yields a similar outcome.\n\n\n\n\nFigure¬†11: Counterfactual path (left) and predicted probability (right) for GreedyGenerator. The contour (left) shows the predicted probabilities of the classifier (Bayesian Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#revise-generator",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#revise-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "REVISE Generator",
    "text": "REVISE Generator\n\n\nCode\nUsing the same classifier as before we can either use the specific REVISEGenerator ‚Ä¶\n\n# Counterfactual search:\ngenerator = REVISEGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n‚Ä¶ or realize that that REVISE (Joshi et al. 2019) just boils down to generic search in a latent space:\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator,\n  latent_space=true\n)\n\n\n\nOutput\n\n\nWe have essentially combined latent search with a probabilistic classifier (as in Antor√°n et al. (2020)).\n\n\n\n\nFigure¬†12: Counterfactual path (left) and predicted probability (right) for REVISEGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#mnist---latent-space-search",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#mnist---latent-space-search",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "MNIST - Latent Space Search",
    "text": "MNIST - Latent Space Search\n\n\nGood VAE\n\nLoading pre-trained classifiers and VAE ‚Ä¶\n\nX, ys = mnist_data() \nmodel = mnist_model() # simple MLP\n\n\n\n‚Ä¶ instantiating model and attaching VAE.\n\nM = FluxModel(model, likelihood=:classification_multi)\ncounterfactual_data = CounterfactualData(X,ys)\nvae = mnist_vae()\ncounterfactual_data.generative_model = vae\n\n\n\n\nThe results in Figure¬†13 look great!\n\n\n\n\nFigure¬†13: Turning a nine (9) into a four (4) using REVISE. It appears that the VAE is well-specified in this case.\n\n\n\n\nBad VAE\n\n\nBut things can also go wrong ‚Ä¶\n\nThe VAE used to generate the counterfactual in Figure¬†14 is not expressive enough.\n\n\n\nFigure¬†14: Turning a seven (7) into a nine (9) using REVISE with a weak VAE.\n\n\n\n\n\nThe counterfactual in Figure¬†15 is also valid ‚Ä¶ what to do?\n\n\n\n\nFigure¬†15: Turning a seven (7) into a nine (9) using generic search."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-models---deep-ensemble",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-models---deep-ensemble",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Custom Models - Deep Ensemble",
    "text": "Custom Models - Deep Ensemble\n\nLoading the pre-trained deep ensemble ‚Ä¶\n\nensemble = mnist_ensemble() # deep ensemble\n\n\n\nStep 1: add composite type as subtype of AbstractFittedModel.\n\nstruct FittedEnsemble <: Models.AbstractFittedModel\n    ensemble::AbstractArray\nend\n\n\n\nStep 2: dispatch logits and probs methods for new model type.\n\nusing Statistics\nimport CounterfactualExplanations.Models: logits, probs\nlogits(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([nn(X) for nn in M.ensemble],3), dims=3)\nprobs(M::FittedEnsemble, X::AbstractArray) = mean(Flux.stack([softmax(nn(X)) for nn in M.ensemble],3),dims=3)\nM = FittedEnsemble(ensemble)\n\n\n\n\nResults for a simple deep ensemble also look convincing!\n\n\n\n\nFigure¬†16: Turning a nine (9) into a four (4) using generic (Wachter) and greedy search for MLP and deep ensemble."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-models---interoperability",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-models---interoperability",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Custom Models - Interoperability",
    "text": "Custom Models - Interoperability\nAdding support for torch models was easy! Here‚Äôs how I implemented it for torch classifiers trained in R.\n\n\n\nSource code\n\nStep 1: add composite type as subtype of AbstractFittedModel\n\nImplemented here.\n\nStep 2: dispatch logits and probs methods for new model type.\n\nImplemented here.\n\n\n\nStep 3: add gradient access.\n\nImplemented here.\n\n\n\nUnchanged API\n\n\nM = RTorchModel(model)\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n# Define generator:\ngenerator = GenericGenerator()\n# Generate recourse:\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\n\nFigure¬†17: Counterfactual path (left) and predicted probability (right) for GenericGenerator and RTorchModel."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-generators",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#custom-generators",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Custom Generators",
    "text": "Custom Generators\nIdea üí°: let‚Äôs implement a generic generator with dropout!\n\n\n\nDispatch\n\nStep 1: create a subtype of AbstractGradientBasedGenerator (adhering to some basic rules).\n\n# Constructor:\nabstract type AbstractDropoutGenerator <: AbstractGradientBasedGenerator end\nstruct DropoutGenerator <: AbstractDropoutGenerator\n    loss::Symbol # loss function\n    complexity::Function # complexity function\n    mutability::Union{Nothing,Vector{Symbol}} # mutibility constraints \n    Œª::AbstractFloat # strength of penalty\n    œµ::AbstractFloat # step size\n    œÑ::AbstractFloat # tolerance for convergence\n    p_dropout::AbstractFloat # dropout rate\nend\n\n\n\nStep 2: implement logic for generating perturbations.\n\nimport CounterfactualExplanations.Generators: generate_perturbations, ‚àá\nusing StatsBase\nfunction generate_perturbations(generator::AbstractDropoutGenerator, counterfactual_state::State)\n    ùê†‚Çú = ‚àá(generator, counterfactual_state.M, counterfactual_state) # gradient\n    # Dropout:\n    set_to_zero = sample(1:length(ùê†‚Çú),Int(round(generator.p_dropout*length(ùê†‚Çú))),replace=false)\n    ùê†‚Çú[set_to_zero] .= 0\n    Œîx‚Ä≤ = - (generator.œµ .* ùê†‚Çú) # gradient step\n    return Œîx‚Ä≤\nend\n\n\n\nUnchanged API\n\n\n# Instantiate:\nusing LinearAlgebra\ngenerator = DropoutGenerator(\n    :logitbinarycrossentropy,\n    norm,\n    nothing,\n    0.1,\n    0.1,\n    1e-5,\n    0.5\n)\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\n\nFigure¬†18: Counterfactual path (left) and predicted probability (right) for custom DropoutGenerator and RTorchModel."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#juliacon-2022-and-beyond",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#juliacon-2022-and-beyond",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "JuliaCon 2022 and beyond",
    "text": "JuliaCon 2022 and beyond\n\n\n\n\nTo JuliaCon ‚Ä¶\n\nDevelop package, register and submit to JuliaCon 2022.\n\n\nNative support for deep learning models (Flux, torch).\n\n\nAdd latent space search.\n\n\n‚Ä¶ and beyond\n\n\nAdd more generators:\n\nDiCE (Mothilal, Sharma, and Tan 2020)\nROAR (Upadhyay, Joshi, and Lakkaraju 2021)\nMINT (Karimi, Sch√∂lkopf, and Valera 2021)\n\n\n\n\n\nAdd support for more models:\n\nMLJ, GLM, ‚Ä¶\nNon-differentiable\n\n\n\n\n\nEnhance preprocessing functionality.\n\n\n\n\nExtend functionality to regression problems.\n\n\n\n\nUse Flux optimizers.\n\n\n\n\n‚Ä¶\n\n\n\n\n\n\nPhoto by Ivan Diaz on Unsplash"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#more-resources",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#more-resources",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "More Resources üìö",
    "text": "More Resources üìö\n\n\n\nRead on ‚Ä¶\n\n\nBlog post introducing CE: [TDS, homepage].\nBlog post introducing package: [TDS, homepage].\nPackage docs with lots of examples.\n\n\n‚Ä¶ or get involved! ü§ó\n\n\nContributor‚Äôs Guide"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/counterfactuals.html#references",
    "href": "content/talks/posts/2022-juliacon/counterfactuals.html#references",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "References",
    "text": "References\n\n\n\nExplaining Black-Box Models through Counterfactuals ‚Äì JuliaCon 2022 ‚Äì Patrick Altmeyer\n\n\n\nAntor√°n, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and Jos√© Miguel Hern√°ndez-Lobato. 2020. ‚ÄúGetting a Clue: A Method for Explaining Uncertainty Estimates.‚Äù https://arxiv.org/abs/2006.06848.\n\n\nGoodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. ‚ÄúExplaining and Harnessing Adversarial Examples.‚Äù https://arxiv.org/abs/1412.6572.\n\n\nJoshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. ‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù https://arxiv.org/abs/1907.09615.\n\n\nKarimi, Amir-Hossein, Bernhard Sch√∂lkopf, and Isabel Valera. 2021. ‚ÄúAlgorithmic Recourse: From Counterfactual Explanations to Interventions.‚Äù In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 353‚Äì62.\n\n\nKarimi, Amir-Hossein, Julius Von K√ºgelgen, Bernhard Sch√∂lkopf, and Isabel Valera. 2020. ‚ÄúAlgorithmic Recourse Under Imperfect Causal Knowledge: A Probabilistic Approach.‚Äù https://arxiv.org/abs/2006.06831.\n\n\nKaur, Harmanpreet, Harsha Nori, Samuel Jenkins, Rich Caruana, Hanna Wallach, and Jennifer Wortman Vaughan. 2020. ‚ÄúInterpreting Interpretability: Understanding Data Scientists‚Äô Use of Interpretability Tools for Machine Learning.‚Äù In Proceedings of the 2020 CHI Conference on Human Factors in Computing Systems, 1‚Äì14.\n\n\nMothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. ‚ÄúExplaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.‚Äù In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607‚Äì17.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nPawelczyk, Martin, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. 2021. ‚ÄúCarla: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms.‚Äù https://arxiv.org/abs/2108.00783.\n\n\nPoyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. ‚ÄúFACE: Feasible and Actionable Counterfactual Explanations.‚Äù In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344‚Äì50.\n\n\nRudin, Cynthia. 2019. ‚ÄúStop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.‚Äù Nature Machine Intelligence 1 (5): 206‚Äì15.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In International Conference on Artificial Intelligence and Statistics, 1756‚Äì64. PMLR.\n\n\nSlack, Dylan, Sophie Hilgard, Emily Jia, Sameer Singh, and Himabindu Lakkaraju. 2020. ‚ÄúFooling Lime and Shap: Adversarial Attacks on Post Hoc Explanation Methods.‚Äù In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 180‚Äì86.\n\n\nUpadhyay, Sohini, Shalmali Joshi, and Himabindu Lakkaraju. 2021. ‚ÄúTowards Robust and Reliable Algorithmic Recourse.‚Äù https://arxiv.org/abs/2102.13620.\n\n\nUstun, Berk, Alexander Spangher, and Yang Liu. 2019. ‚ÄúActionable Recourse in Linear Classification.‚Äù In Proceedings of the Conference on Fairness, Accountability, and Transparency, 10‚Äì19.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. ‚ÄúCounterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.‚Äù Harv. JL & Tech. 31: 841.\n\n\nWilson, Andrew Gordon. 2020. ‚ÄúThe Case for Bayesian Deep Learning.‚Äù https://arxiv.org/abs/2001.10995."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/quarto.html#quarto-a-new-old-way-to-publish-science",
    "href": "content/talks/posts/2022-juliacon/quarto.html#quarto-a-new-old-way-to-publish-science",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "Quarto ‚Äì A New (Old) Way to Publish Science",
    "text": "Quarto ‚Äì A New (Old) Way to Publish Science\n\nHave used R Markdown for many years for essentially anything work-related.\nGenerate multiple different output formats with ease:\n\nThe old school: LaTeX and PDF (including Beamer); MS Office\nThe brave new world: beautiful HTML content\n\nwebsites\ne-books\napps\n‚Ä¶\n\n\nAll of this starting from the same place ‚Ä¶\n\n\nA plain Markdown document blended with your favourite programming language of your choice and a YAML header defining your output."
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/quarto.html#julia-and-quarto-a-perfect-match",
    "href": "content/talks/posts/2022-juliacon/quarto.html#julia-and-quarto-a-perfect-match",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "Julia and Quarto: a perfect match üíôüíúüíö",
    "text": "Julia and Quarto: a perfect match üíôüíúüíö\n\n\n\n\nPreferred setup: VSCode, Quarto and Julia\n\nCan switch between Jupyter and .qmd with ease.\nWhen working with .qmd, code chunks connect to REPL.\n\n\n\n\nDocumenter.jl and Quarto\n\nGenerally play nicely with each other (both Markdown based).\n\nformat: \n  commonmark:\n    variant: -raw_html\n\nYou get some stuff for free, e.g.¬†citation management.\nUnfotunately lose support for cross-referencing ‚Ä¶\n\n\n\n\nSuggestion: Quarto for JuliaCon Proceedings\n\nQuarto supports LaTex templates/classes ‚Ä¶\n‚Ä¶ but why only publish proceedings in PDF form?\nQuarto opens gateway to more innovative forms of publishing!\n\n\n\n\n\nCode\nusing Javis, Animations, Colors\nwww_path = \"www\"\n\nsize = 600\nradius_factor = 0.33\n\nfunction ground(args...)\n    background(\"transparent\")\n    sethue(\"white\")\nend\n\nfunction rotate_anim(idx::Number, total::Number) \n    distance_circle = 0.875\n    steps = collect(range(distance_circle,1-distance_circle,length=total))\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [0, steps[idx]*2œÄ],\n        [sineio()],\n    )\nend\n\ntranslate_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(size*radius_factor, 0)],\n    [sineio()],\n)\n\ntranslate_back_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(-(size*radius_factor), 0)],\n    [sineio()],\n)\n\njulia_colours = Dict(\n    :blue => \"#4063D8\",\n    :green => \"#389826\",\n    :purple => \"#9558b2\",\n    :red => \"#CB3C33\"\n)\ncolour_order = [:red, :purple, :green, :blue]\nn_colours = length(julia_colours)\nfunction color_anim(start_colour::String, quarto_col::String=\"#4b95d0\")\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [Lab(color(start_colour)), Lab(color(quarto_col))],\n        [sineio()],\n    )\nend\n\nvideo = Video(size, size)\n\nframe_starts = 1:10:40\nn_total = 250\nn_frames = 150\nBackground(1:n_total, ground)\n\n# Blob:\nfunction element(; radius = 1)\n    circle(O, radius, :fill) # The 4 is to make the circle not so small\nend\n\n# Cross:\nfunction cross(color=\"black\";orientation=:horizontal)\n    sethue(color)\n    setline(10)\n    if orientation==:horizontal\n        out = line(Point(-size,0),Point(size,0), :stroke)\n    else\n        out = line(Point(0,-size),Point(0,size), :stroke)\n    end\n    return out\nend\n\nfor (i, frame_start) in enumerate(1:10:40)\n\n    # Julia circles:\n    blob = Object(frame_start:n_total, (args...;radius=1) -> element(;radius=radius))\n    act!(blob, Action(1:Int(round(n_frames*0.25)), change(:radius, 1 => 75))) # scale up\n    act!(blob, Action(n_frames:(n_frames+50), change(:radius, 75 => 250))) # scale up further\n    act!(blob, Action(1:30, translate_anim, translate()))\n    act!(blob, Action(31:120, rotate_anim(i, n_colours), rotate_around(Point(-(size*radius_factor), 0))))\n    act!(blob, Action(121:150, translate_back_anim, translate()))\n    act!(blob, Action(1:150, color_anim(julia_colours[colour_order[i]]), sethue()))\n\n    # Quarto cross:\n    cross_h = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:horizontal))\n    cross_v = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:vertical))\nend\n\nrender(\n    video;\n    pathname = joinpath(www_path, \"intro.gif\"),\n)\n\n\n\n\n\nFigure¬†1: Julia and Quarto: a perfect match. Image by author (heavily borrowing from Javis.jl tutorial)"
  },
  {
    "objectID": "content/talks/posts/2022-juliacon/quarto.html#more-resources",
    "href": "content/talks/posts/2022-juliacon/quarto.html#more-resources",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "More Resources üìö",
    "text": "More Resources üìö\n\n\n\nRead on ‚Ä¶\n\n\nRelated blog post (hosted on a blog that itself is built with Quarto and involves lots of Julia content).\nExamples of Julia package documentation I‚Äôve built with Quarto + Documenter.jl:\n\nCounterfactualExplanations.jl\nLaplaceRedux.jl\n\nQuarto‚Äôs own guide to using Quarto with Julia.\n\n\n\n\n\n                   \n\n\n\n\n\n\n\n\nJulia and Quarto: a match made in heaven? üå§ ‚Äì JuliaCon 2022 ‚Äì Patrick Altmeyer"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#blurb",
    "href": "content/talks/posts/2022-boe/presentation.html#blurb",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Blurb",
    "text": "Blurb\nCounterfactual Explanations explain how inputs into a model need to change for it to produce different outputs. Explanations that involve realistic and actionable changes can be used for the purpose of Algorithmic Recourse: they offer human stakeholders a principled approach to not only understand the model they are seeking to explain, but also react to it or adjust it.\nThe general setup lends itself naturally to Bank datasets that revolve around counterparty risk, for example. In this seminar I will introduce the topic and place it into the broader context of Explainable AI. Using my Julia package I will go through a worked example involving a publicly available credit data set. Finally, I will also briefly present some of our recent research that points to potential pitfalls of current state-of-the-art approaches and proposes mitigation strategies.\nDISCLAIMER: Views presented in this presentation are my own."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#quick-intro",
    "href": "content/talks/posts/2022-boe/presentation.html#quick-intro",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Quick Intro",
    "text": "Quick Intro\n\n\n\nCurrently 2nd year of PhD in Trustworthy Artificial Intelligence at Delft University of Technology.\nWorking on Counterfactual Explanations and Probabilistic Machine Learning with applications in Finance.\nPreviously, educational background in Economics and Finance and two years at the Bank of England (MPAT \\(\\subset\\) MIAD).\nEnthusiastic about free open-source software, in particular Julia and Quarto."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#the-problem-with-todays-ai",
    "href": "content/talks/posts/2022-boe/presentation.html#the-problem-with-todays-ai",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "The Problem with Today‚Äôs AI",
    "text": "The Problem with Today‚Äôs AI\n\nFrom human to data-driven decision-making ‚Ä¶\n\n\n\nBlack-box models like deep neural networks are being deployed virtually everywhere.\nIncludes safety-critical and public domains: health care, autonomous driving, finance, ‚Ä¶\nMore likely than not that your loan or employment application is handled by an algorithm.\n\n\n\n\n‚Ä¶ where black boxes are recipe for disaster.\n\n\n\nWe have no idea what exactly we‚Äôre cooking up ‚Ä¶\n\nHave you received an automated rejection email? Why didn‚Äôt you ‚ÄúmEet tHe sHoRtLisTiNg cRiTeRia‚Äù? üôÉ\n\n‚Ä¶ but we do know that some of it is junk.\n\n\n\n\n\n\n\nFigure¬†1: Adversarial attacks on deep neural networks. Source: Goodfellow, Shlens, and Szegedy (2014)"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai",
    "href": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-1",
    "href": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-1",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nCurrent Standard in ML\nWe typically want to maximize the likelihood of observing \\(\\mathcal{D}_n\\) under given parameters (Murphy 2022):\n\\[\n\\theta^* = \\arg \\max_{\\theta} p(\\mathcal{D}_n|\\theta)\n\\qquad(1)\\]\nCompute an MLE (or MAP) point estimate \\(\\hat\\theta = \\mathbb{E} \\theta^*\\) and use plugin approximation for prediction:\n\\[\np(y|x,\\mathcal{D}_n) \\approx p(y|x,\\hat\\theta)\n\\qquad(2)\\]\n\nIn an ideal world we can just use parsimonious and interpretable models like GLM (Rudin 2019), for which in many cases we can rely on asymptotic properties of \\(\\theta\\) to quantify uncertainty.\nIn practice these models often have performance limitations.\nBlack-box models like deep neural networks are popular, but they are also the very opposite of parsimonious.\n\nObjective"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-2",
    "href": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-2",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nObjective\n\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn this setting it is often crucial to treat models probabilistically!\n\\[\np(y|x,\\mathcal{D}_n) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D}_n)d\\theta\n\\qquad(3)\\]"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-3",
    "href": "content/talks/posts/2022-boe/presentation.html#towards-trustworthy-ai-3",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\n\nWe can now make predictions ‚Äì great! But do we know how the predictions are actually being made?\n\n\nObjective\nWith the model trained for its task, we are interested in understanding how its predictions change in response to input changes.\n\\[\n\\nabla_x p(y|x,\\mathcal{D}_n;\\hat\\theta)\n\\qquad(4)\\]\n\n\nCounterfactual reasoning (in this context) boils down to simple questions: what if \\(x\\) (factual) \\(\\Rightarrow\\) \\(x\\prime\\) (counterfactual)?\nBy strategically perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.\nCounterfactual Explanations always have full fidelity by construction (as opposed to surrogate explanations, for example).\n\n\n\n\nImportant to realize that we are keeping \\(\\hat\\theta\\) constant!"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#todays-talk",
    "href": "content/talks/posts/2022-boe/presentation.html#todays-talk",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Today‚Äôs talk",
    "text": "Today‚Äôs talk\n\nüîÆ Explaining Black-Box Models through Counterfactuals (\\(\\approx\\) 10min)\n\nWhat are they? What are they not?\nCounterfactual Explanations in the broader XAI landscape\nFrom Counterfactual Explanations to Algorithmic Recourse\n\nüõ†Ô∏è Hands-on examples ‚Äî CounterfactualExplanations.jl in Julia (\\(\\approx\\) 15min)\nüìä Endogenous Macrodynamics in Algorithmic Recourse (\\(\\approx\\) 10min)\n‚ùì Q&A (\\(\\approx\\) 10min)\nüöÄ Related Research Topics (\\(\\approx\\) 10min)\n\nPredictive Uncertainty Quantification"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#a-framework-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-boe/presentation.html#a-framework-for-counterfactual-explanations",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "A Framework for Counterfactual Explanations",
    "text": "A Framework for Counterfactual Explanations\n\nEven though [‚Ä¶] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the ‚Äúblack box‚Äù. (Wachter, Mittelstadt, and Russell 2017)\n\n\n\nFramework\n\nObjective originally proposed by Wachter, Mittelstadt, and Russell (2017) is as follows\n\\[\n\\min_{x\\prime \\in \\mathcal{X}} h(x\\prime) \\ \\ \\ \\mbox{s. t.} \\ \\ \\ M(x\\prime) = t\n\\qquad(5)\\]\nwhere \\(h\\) relates to the complexity of the counterfactual and \\(M\\) denotes the classifier.\n\n\nTypically this is approximated through regularization:\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) + \\lambda h(x\\prime)\n\\qquad(6)\\]\n\n\nIntuition\n\n\n\n\nFigure¬†2: A cat performing gradient descent in the feature space √† la Wachter, Mittelstadt, and Russell (2017)."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#counterfactuals-as-in-adversarial-examples",
    "href": "content/talks/posts/2022-boe/presentation.html#counterfactuals-as-in-adversarial-examples",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Adversarial Examples?",
    "text": "Counterfactuals ‚Ä¶ as in Adversarial Examples?\n\n\n\nYes and no!\n\nWhile both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.\n\n\nDesiderata\n\n\ncloseness: the average distance between factual and counterfactual features should be small (Wachter, Mittelstadt, and Russell (2017))\nactionability: the proposed feature perturbation should actually be actionable (Ustun, Spangher, and Liu (2019), Poyiadzi et al. (2020))\nplausibility: the counterfactual explanation should be plausible to a human (Joshi et al. (2019))\nunambiguity: a human should have no trouble assigning a label to the counterfactual (Schut et al. (2021))\nsparsity: the counterfactual explanation should involve as few individual feature changes as possible (Schut et al. (2021))\nrobustness: the counterfactual explanation should be robust to domain and model shifts (Upadhyay, Joshi, and Lakkaraju (2021))\ndiversity: ideally multiple diverse counterfactual explanations should be provided (Mothilal, Sharma, and Tan (2020))\ncausality: counterfactual explanations reflect the structural causal model underlying the data generating process (Karimi et al. (2020), Karimi, Sch√∂lkopf, and Valera (2021))"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#counterfactuals-as-in-causal-inference",
    "href": "content/talks/posts/2022-boe/presentation.html#counterfactuals-as-in-causal-inference",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Causal Inference?",
    "text": "Counterfactuals ‚Ä¶ as in Causal Inference?\n\nNO!\n\n\n\nCausal inference: counterfactuals are thought of as unobserved states of the world that we would like to observe in order to establish causality.\n\nThe only way to do this is by actually interfering with the state of the world: \\(p(y|do(x),\\theta)\\).\nIn practice we can only move some individuals to the counterfactual state of the world and compare their outcomes to a control group.\nProvided we have controlled for confounders, properly randomized, ‚Ä¶ we can estimate an average treatment effect: \\(\\hat\\theta\\).\n\nCounterfactual Explanations: involves perturbing features after some model has been trained.\n\nWe end up comparing modeled outcomes \\(p(y|x,\\hat\\phi)\\) and \\(p(y|x\\prime,\\hat\\phi)\\) for individuals.\nWe have not magically solved causality.\n\n\n\n\nThe number of ostensibly pro data scientists confusing themselves into believing that \"counterfactual explanations\" capture real-world causality is just staggeringü§¶‚Äç‚ôÄÔ∏è. Where do we go from here? How can a community that doesn't even understand what's already known make advances?\n\n‚Äî Zachary Lipton (@zacharylipton) June 20, 2022"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#the-xai-landscape",
    "href": "content/talks/posts/2022-boe/presentation.html#the-xai-landscape",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "The XAI Landscape",
    "text": "The XAI Landscape\n\nA (highly) simplified and incomplete overview ‚Ä¶\n\n\n\n\n\n\n\nflowchart LR\n  XAI([Explainable AI])\n  IAI([Interpretable AI]) \n  global([Global Methods])\n  local([Local Methods])\n  surrogate([Surrogate])\n  permute([Feature Permutation])\n  game([Game Theory])\n  global-surrogate[Global Surrogate]\n  permute-feature[Permutation Feature Importance]\n  lime[LIME]\n  shap[\"SHAP\"]\n  shapley[\"Shapley\"]\n  ce[Counterfactual Explanations]\n\n  XAI ==> global & local\n  global ==> global-surrogate & permute-feature\n  local ==> lime & shap & shapley & ce\n\n  global-surrogate & lime & shap --- surrogate\n  permute-feature & ce --- permute\n  shap & shapley --- game\n\n\n\n\n\nFigure¬†3: A (highly) simplified and incomplete overview of the XAI landscape loosly based on Molnar (2020)."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#from-counterfactual-explanations-to-algorithmic-recourse",
    "href": "content/talks/posts/2022-boe/presentation.html#from-counterfactual-explanations-to-algorithmic-recourse",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "From Counterfactual Explanations to Algorithmic Recourse",
    "text": "From Counterfactual Explanations to Algorithmic Recourse\n\n\n\n‚ÄúYou cannot appeal to (algorithms). They do not listen. Nor do they bend.‚Äù\n‚Äî Cathy O‚ÄôNeil in Weapons of Math Destruction, 2016\n\n\n\n\nFigure¬†4: Cathy O‚ÄôNeil. Source: Cathy O‚ÄôNeil a.k.a. mathbabe.\n\n\n\nAlgorithmic Recourse\n\n\nO‚ÄôNeil (2016) points to various real-world involving black-box models and affected individuals facing adverse outcomes.\n\n\n\n\nThese individuals generally have no way to challenge their outcome.\n\n\n\n\nCounterfactual Explanations that involve actionable and realistic feature perturbations can be used for the purpose of Algorithmic Recourse."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#counterfactualexplanations.jl",
    "href": "content/talks/posts/2022-boe/presentation.html#counterfactualexplanations.jl",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "CounterfactualExplanations.jl üì¶",
    "text": "CounterfactualExplanations.jl üì¶\n     \n\n\n\n\nA unifying framework for generating Counterfactual Explanations.\nFast, extensible and composable allowing users and developers to add and combine different counterfactual generators.\nImplements a number of SOTA generators.\nBuilt in Julia, but can be used to explain models built in R and Python (still experimental).\nStatus üîÅ: ready for research, not production. Thought/challenge/contributions welcome!\n\n\n\n\n\n\nPhoto by Denise Jans on Unsplash.\n\n\n\n\n\n\nJulia has an edge with respect to Trustworthy AI: it‚Äôs open-source, uniquely transparent and interoperable üî¥üü¢üü£"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#a-simple-example",
    "href": "content/talks/posts/2022-boe/presentation.html#a-simple-example",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "A simple example",
    "text": "A simple example\n\n\n\nLoad and prepare some toy data.\nSelect a random sample.\nGenerate counterfactuals using different approaches.\n\n\n# Data:\nusing Random\nRandom.seed!(2022)\nN = 100\nusing CounterfactualExplanations\nxs, ys = toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Randomly selected factual:\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#generic-generator",
    "href": "content/talks/posts/2022-boe/presentation.html#generic-generator",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Generic Generator",
    "text": "Generic Generator\n\n\nCode\n\nWe begin by instantiating the fitted model ‚Ä¶\n\n# Model\nw = [1.0 1.0] # estimated coefficients\nb = 0 # estimated bias\nM = LogisticModel(w, [b])\n\n\n\n‚Ä¶ then based on its prediction for \\(x\\) we choose the opposite label as our target ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ and finally generate the counterfactual.\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 36 steps.\n\n\n\n\nOutput\n\n\n‚Ä¶ et voil√†!\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=5)\n\n\n\nFigure¬†5: Counterfactual path (left) and predicted probability (right) for GenericGenerator. The contour (left) shows the predicted probabilities of the classifier (Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#probabilistic-methods-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-boe/presentation.html#probabilistic-methods-for-counterfactual-explanations",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Probabilistic Methods for Counterfactual Explanations",
    "text": "Probabilistic Methods for Counterfactual Explanations\nWhen people say that counterfactuals should look realistic or plausible, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:\n\\[\nx\\prime \\sim p(x)\n\\]\nBut how do we estimate \\(p(x)\\)? Two probabilistic approaches ‚Ä¶\n\n\nAPPROACH 1: use the model itselfAPPROACH 2: use some generative model\n\n\n\n\nSchut et al. (2021) note that by maximizing predictive probabilities \\(\\sigma(M(x\\prime))\\) for probabilistic models \\(M\\in\\mathcal{\\widetilde{M}}\\) one implicitly minimizes epistemic and aleotoric uncertainty.\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) \\ \\ \\ , \\ \\ \\ M\\in\\mathcal{\\widetilde{M}}\n\\qquad(7)\\]\n\n\n\n\nFigure¬†6: A cat performing gradient descent in the feature space √† la Schut et al. (2021)\n\n\n\n\n\n\n\n\nInstead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model (Joshi et al. 2019).\n\\[\nz\\prime = \\arg \\min_{z\\prime}  \\ell(M(dec(z\\prime)),t) + \\lambda h(x\\prime)\n\\qquad(8)\\]\nand\n\\[x\\prime = dec(z\\prime)\\]\nwhere \\(dec(\\cdot)\\) is the decoder function.\n\n\n\n\nFigure¬†7: Counterfactual (yellow) generated through latent space search (right panel) following Joshi et al. (2019). The corresponding counterfactual path in the feature space is shown in the left panel."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#greedy-generator",
    "href": "content/talks/posts/2022-boe/presentation.html#greedy-generator",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Greedy Generator",
    "text": "Greedy Generator\n\n\nCode\n\nThis time we use a Bayesian classifier ‚Ä¶\n\nusing LinearAlgebra\nŒ£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\nŒº = hcat(b, w)\nM = BayesianLogisticModel(Œº, Œ£)\n\n\n\n‚Ä¶ and once again choose our target label as before ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ to then finally use greedy search to find a counterfactual.\n\n# Counterfactual search:\ngenerator = GreedyGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 56 steps.\n\n\n\n\nOutput\n\n\nIn this case the Bayesian approach yields a similar outcome.\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=15)\n\n\n\nFigure¬†8: Counterfactual path (left) and predicted probability (right) for GreedyGenerator. The contour (left) shows the predicted probabilities of the classifier (Bayesian Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#latent-space-generator",
    "href": "content/talks/posts/2022-boe/presentation.html#latent-space-generator",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Latent Space Generator",
    "text": "Latent Space Generator\n\n\nCode\n\nUsing the same classifier as before we can either use the specific REVISEGenerator ‚Ä¶\n\n# Counterfactual search:\ngenerator = REVISEGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\n‚Ä¶ or realize that that REVISE (Joshi et al. 2019) just boils down to generic search in a latent space:\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator,\n  latent_space=true\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 6 steps.\n\n\n\n\nOutput\n\n\nWe have essentially combined latent search with a probabilistic classifier (as in Antor√°n et al. (2020)).\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=2)\n\n\n\nFigure¬†9: Counterfactual path (left) and predicted probability (right) for REVISEGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#diverse-counterfactuals",
    "href": "content/talks/posts/2022-boe/presentation.html#diverse-counterfactuals",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Diverse Counterfactuals",
    "text": "Diverse Counterfactuals\n\n\nCode\n\nWe can use the DiCEGenerator to produce multiple diverse counterfactuals:\n\n# Counterfactual search:\ngenerator = DiCEGenerator(Œª=[0.1, 5.0])\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator;\n  num_counterfactuals = 5\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 124 steps.\n\n\n\n\nOutput\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=20)\n\n\n\n\nFigure¬†10: Counterfactual path (left) and predicted probability (right) for DiCEGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#a-real-world-example---credit-default",
    "href": "content/talks/posts/2022-boe/presentation.html#a-real-world-example---credit-default",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "A Real-World Example - Credit Default",
    "text": "A Real-World Example - Credit Default\n\nThe Give Me Some Credit dataset is publicly available from Kaggle.\n\n\nImprove on the state of the art in credit scoring by predicting the probability that somebody will experience financial distress in the next two years.\n\n\nWe have \\(y \\in \\{0=\\text{no stress},1=\\text{stress}\\}\\) and a number of demographic and credit-related features \\(X\\)."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#ignoring-mutability",
    "href": "content/talks/posts/2022-boe/presentation.html#ignoring-mutability",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Ignoring Mutability",
    "text": "Ignoring Mutability\nUsing DiCE to generate counterfactuals for a single individual, ignoring actionability:"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#respecting-mutability",
    "href": "content/talks/posts/2022-boe/presentation.html#respecting-mutability",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Respecting Mutability",
    "text": "Respecting Mutability\nUsing the generic generator to generate counterfactuals for multiple individuals, respecting that age cannot be decreased (you might argue that age also cannot be easily increased ‚Ä¶):"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#motivation",
    "href": "content/talks/posts/2022-boe/presentation.html#motivation",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Motivation",
    "text": "Motivation\n\n\n\nTL;DR: We find that standard implementation of various SOTA approaches to AR can induce substantial domain and model shifts. We argue that these dynamics indicate that individual recourse generates hidden external costs and provide mitigation strategies.\n\nIn this work we investigate what happens if Algorithmic Recourse is actually implemented by a large number of individuals.\nFigure¬†11 illustrates what we mean by Endogenous Macrodynamics in Algorithmic Recourse:\n\nPanel (a): we have a simple linear classifier trained for binary classification where samples from the negative class (y=0) are marked in blue and samples of the positive class (y=1) are marked in orange\nPanel (b): the implementation of AR for a random subset of individuals leads to a noticable domain shift\nPanel (c): as the classifier is retrained we observe a corresponding model shift (Upadhyay, Joshi, and Lakkaraju 2021)\nPanel (d): as this process is repeated, the decision boundary moves away from the target class.\n\n\n\n\n\nFigure¬†11: Proof of concept: repeated implementation of AR leads to domain and model shifts.\n\n\n\nWe argue that these shifts should be considered as an expected external cost of individual recourse and call for a paradigm shift from individual to collective recourse in these types of situations."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#generalised-framework",
    "href": "content/talks/posts/2022-boe/presentation.html#generalised-framework",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Generalised Framework",
    "text": "Generalised Framework\nFrom individual recourse ‚Ä¶\nWe restate Equation¬†6 to encapsulate latent space search:\n\\[\n\\begin{aligned}\n\\mathbf{s}^\\prime &= \\arg \\min_{\\mathbf{s}^\\prime \\in \\mathcal{S}} \\left\\{  {\\text{yloss}(M(f(\\mathbf{s}^\\prime)),y^*)}+ \\lambda {\\text{cost}(f(\\mathbf{s}^\\prime)) }  \\right\\}\n\\end{aligned}\n\\qquad(9)\\]\n‚Ä¶ towards collective recourse\nWe borrow the notion of negative externalities from Economics, to formalise the idea that individual recourse fails to account for external costs:\n\\[\n\\begin{aligned}\n\\mathbf{s}^\\prime &= \\arg \\min_{\\mathbf{s}^\\prime \\in \\mathcal{S}} \\{ {\\text{yloss}(M(f(\\mathbf{s}^\\prime)),y^*)} \\\\ &+ \\lambda_1 {\\text{cost}(f(\\mathbf{s}^\\prime))} + \\lambda_2 {\\text{extcost}(f(\\mathbf{s}^\\prime))} \\}\n\\end{aligned}\n\\qquad(10)\\]"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#findings",
    "href": "content/talks/posts/2022-boe/presentation.html#findings",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Findings",
    "text": "Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-word data."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#mitigation-strategies",
    "href": "content/talks/posts/2022-boe/presentation.html#mitigation-strategies",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Mitigation Strategies",
    "text": "Mitigation Strategies\n\n\n\nChoose more conservative decision thresholds.\nClassifer Preserving ROAR (ClaPROAR): penalise classifier loss.\n\n\\[\n\\begin{aligned}\n\\text{extcost}(f(\\mathbf{s}^\\prime)) = l(M(f(\\mathbf{s}^\\prime)),y^\\prime)\n\\end{aligned}\n\\qquad(11)\\]\n\nGravitational Counterfactual Explanations: penalise distance to some sensible point in the target domain.\n\n\\[\n\\begin{aligned}\n\\text{extcost}(f(\\mathbf{s}^\\prime)) = \\text{dist}(f(\\mathbf{s}^\\prime),\\bar{x})  \n\\end{aligned}\n\\qquad(12)\\]\n\n\n\n\nFigure¬†12: Illustrative example demonstrating the properties of the various mitigation strategies. Samples from the negative class \\((y = 0)\\) are marked in blue while samples of the positive class \\((y = 1)\\) are marked in orange.\n\n\n\n\n\n\n\n\n\nMitigation strategies applied to synthetic data.\n\n\n\n\n\n\nMitigation strategies applied to real-world data."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "href": "content/talks/posts/2022-boe/presentation.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Effortless Bayesian Deep Learning through Laplace Redux",
    "text": "Effortless Bayesian Deep Learning through Laplace Redux\n   \n\n\nLaplaceRedux.jl (formerly BayesLaplace.jl) is a small package that can be used for effortless Bayesian Deep Learning and Logistic Regression trough Laplace Approximation. It is inspired by this Python library and its companion paper.\n\n\n\nPlugin Approximation (left) and Laplace Posterior (right) for simple artificial neural network.\n\n\n\n\n\n\nSimulation of changing posteriour predictive distribution. Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#conformalprediction.jl",
    "href": "content/talks/posts/2022-boe/presentation.html#conformalprediction.jl",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "ConformalPrediction.jl",
    "text": "ConformalPrediction.jl\n      \nConformalPrediction.jl is a package for Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in MLJ (Blaom et al. 2020). Conformal Prediction is distribution-free, easy-to-understand, easy-to-use and model-agnostic.\n\n\n\nConformal Prediction in action: Prediction sets for two different samples and changing coverage rates. As coverage grows, so does the size of the prediction sets."
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#more-resources",
    "href": "content/talks/posts/2022-boe/presentation.html#more-resources",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "More Resources üìö",
    "text": "More Resources üìö\n\n\n\nRead on ‚Ä¶\n\n\nBlog post introducing CE: [TDS], [blog].\nBlog post on Laplace Redux: [TDS], [blog].\nBlog post on Conformal Prediction: [TDS], [blog].\n\n\n‚Ä¶ or get involved! ü§ó\n\n\nContributor‚Äôs Guide for CounterfactualExplanations.jl\nContributor‚Äôs Guide for ConformalPrediction.jl"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#image-sources",
    "href": "content/talks/posts/2022-boe/presentation.html#image-sources",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "Image Sources",
    "text": "Image Sources\n\nCrystal ball on beach: Nicole Avagliano on Unsplash\nColour gradient: A.Z on Unsplash\nElephant herd: Sergi Ferrete on Unsplash\nBank of England logo: Bank of England here"
  },
  {
    "objectID": "content/talks/posts/2022-boe/presentation.html#references",
    "href": "content/talks/posts/2022-boe/presentation.html#references",
    "title": "Explaining Machine Learning Models through Counterfactuals",
    "section": "References",
    "text": "References\n\n\n\nExplaining Machine Learning Models through Counterfactuals ‚Äî Patrick Altmeyer ‚Äî CC BY-NC\n\n\n\nAntor√°n, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and Jos√© Miguel Hern√°ndez-Lobato. 2020. ‚ÄúGetting a Clue: A Method for Explaining Uncertainty Estimates.‚Äù https://arxiv.org/abs/2006.06848.\n\n\nBlaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. ‚ÄúMLJ: A Julia Package for Composable Machine Learning.‚Äù Journal of Open Source Software 5 (55): 2704. https://doi.org/10.21105/joss.02704.\n\n\nGoodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. ‚ÄúExplaining and Harnessing Adversarial Examples.‚Äù https://arxiv.org/abs/1412.6572.\n\n\nJoshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. ‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù https://arxiv.org/abs/1907.09615.\n\n\nKarimi, Amir-Hossein, Bernhard Sch√∂lkopf, and Isabel Valera. 2021. ‚ÄúAlgorithmic Recourse: From Counterfactual Explanations to Interventions.‚Äù In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 353‚Äì62.\n\n\nKarimi, Amir-Hossein, Julius Von K√ºgelgen, Bernhard Sch√∂lkopf, and Isabel Valera. 2020. ‚ÄúAlgorithmic Recourse Under Imperfect Causal Knowledge: A Probabilistic Approach.‚Äù https://arxiv.org/abs/2006.06831.\n\n\nMolnar, Christoph. 2020. Interpretable Machine Learning. Lulu. com.\n\n\nMothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. ‚ÄúExplaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.‚Äù In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607‚Äì17.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nO‚ÄôNeil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown.\n\n\nPoyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. ‚ÄúFACE: Feasible and Actionable Counterfactual Explanations.‚Äù In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344‚Äì50.\n\n\nRudin, Cynthia. 2019. ‚ÄúStop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.‚Äù Nature Machine Intelligence 1 (5): 206‚Äì15.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In International Conference on Artificial Intelligence and Statistics, 1756‚Äì64. PMLR.\n\n\nUpadhyay, Sohini, Shalmali Joshi, and Himabindu Lakkaraju. 2021. ‚ÄúTowards Robust and Reliable Algorithmic Recourse.‚Äù https://arxiv.org/abs/2102.13620.\n\n\nUstun, Berk, Alexander Spangher, and Yang Liu. 2019. ‚ÄúActionable Recourse in Linear Classification.‚Äù In Proceedings of the Conference on Fairness, Accountability, and Transparency, 10‚Äì19.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. ‚ÄúCounterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.‚Äù Harv. JL & Tech. 31: 841.\n\n\nWilson, Andrew Gordon. 2020. ‚ÄúThe Case for Bayesian Deep Learning.‚Äù https://arxiv.org/abs/2001.10995."
  },
  {
    "objectID": "content/talks/posts/2022-boe/index.html",
    "href": "content/talks/posts/2022-boe/index.html",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "",
    "text": "In November, 2022, I gave a New Methods Seminar at the Bank of England. You can find the slides here.\n\n\n\nBank of England Logo"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#house-rules",
    "href": "content/talks/posts/2022-dscc/presentation.html#house-rules",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "House Rules",
    "text": "House Rules\n\nDISCLAIMER: Views presented in this presentation are my own."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#quick-intro",
    "href": "content/talks/posts/2022-dscc/presentation.html#quick-intro",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Quick Intro",
    "text": "Quick Intro\n\n\n\nCurrently 2nd year of PhD in Trustworthy Artificial Intelligence.\nWorking on Counterfactual Explanations and Probabilistic Machine Learning.\nPreviously, educational background in Economics and Finance and two years in monetary policy at the Bank of England.\nEnthusiastic about free open-source software, in particular Julia and Quarto."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#the-problem-with-todays-ai",
    "href": "content/talks/posts/2022-dscc/presentation.html#the-problem-with-todays-ai",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "The Problem with Today‚Äôs AI",
    "text": "The Problem with Today‚Äôs AI\n\nFrom human to data-driven decision-making ‚Ä¶\n\n\n\nBlack-box models like deep neural networks are being deployed virtually everywhere.\nIncludes safety-critical and public domains: health care, autonomous driving, finance, ‚Ä¶\nMore likely than not that your loan or employment application is handled by an algorithm.\n\n\n\n\n‚Ä¶ where black boxes are recipe for disaster.\n\n\n\nWe have no idea what exactly we‚Äôre cooking up ‚Ä¶\n\nHave you received an automated rejection email? Why didn‚Äôt you ‚ÄúmEet tHe sHoRtLisTiNg cRiTeRia‚Äù? üôÉ\n\n‚Ä¶ but we do know that some of it is junk.\n\n\n\n\n\n\n\nFigure¬†1: Adversarial attacks on deep neural networks. Source: Goodfellow, Shlens, and Szegedy (2014)"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai",
    "href": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-1",
    "href": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-1",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nCurrent Standard in ML\nWe typically want to maximize the likelihood of observing \\(\\mathcal{D}_n\\) under given parameters (Murphy 2022):\n\\[\n\\theta^* = \\arg \\max_{\\theta} p(\\mathcal{D}_n|\\theta)\n\\qquad(1)\\]\nCompute an MLE (or MAP) point estimate \\(\\hat\\theta = \\mathbb{E} \\theta^*\\) and use plugin approximation for prediction:\n\\[\np(y|x,\\mathcal{D}_n) \\approx p(y|x,\\hat\\theta)\n\\qquad(2)\\]\n\nIn an ideal world we can just use parsimonious and interpretable models like GLM (Rudin 2019), for which in many cases we can rely on asymptotic properties of \\(\\theta\\) to quantify uncertainty.\nIn practice these models often have performance limitations.\nBlack-box models like deep neural networks are popular, but they are also the very opposite of parsimonious.\n\nObjective"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-2",
    "href": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-2",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\nObjective\n\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn this setting it is often crucial to treat models probabilistically!\n\\[\np(y|x,\\mathcal{D}_n) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D}_n)d\\theta\n\\qquad(3)\\]"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-3",
    "href": "content/talks/posts/2022-dscc/presentation.html#towards-trustworthy-ai-3",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Towards Trustworthy AI",
    "text": "Towards Trustworthy AI\n\n\nGround Truthing\n\n\nProbabilistic Models\n\n\nCounterfactual Reasoning\n\n\n\nWe can now make predictions ‚Äì great! But do we know how the predictions are actually being made?\n\n\nObjective\nWith the model trained for its task, we are interested in understanding how its predictions change in response to input changes.\n\\[\n\\nabla_x p(y|x,\\mathcal{D}_n;\\hat\\theta)\n\\qquad(4)\\]\n\n\nCounterfactual reasoning (in this context) boils down to simple questions: what if \\(x\\) (factual) \\(\\Rightarrow\\) \\(x\\prime\\) (counterfactual)?\nBy strategically perturbing features and checking the model output, we can (begin to) understand how the model makes its decisions.\nCounterfactual Explanations always have full fidelity by construction (as opposed to surrogate explanations, for example).\n\n\n\n\nImportant to realize that we are keeping \\(\\hat\\theta\\) constant!"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#todays-talk",
    "href": "content/talks/posts/2022-dscc/presentation.html#todays-talk",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Today‚Äôs talk",
    "text": "Today‚Äôs talk\n\nüîÆ Explaining Black-Box Models through Counterfactuals (\\(\\approx\\) 10min)\n\nWhat are they? What are they not?\nFrom Counterfactual Explanations to Algorithmic Recourse\n\nüõ†Ô∏è Hands-on examples ‚Äî CounterfactualExplanations.jl in Julia (\\(\\approx\\) 15min)\nüìä Endogenous Macrodynamics in Algorithmic Recourse (\\(\\approx\\) 10min)\nüöÄ The Road Ahead ‚Äî Related Research Topics (\\(\\approx\\) 10min)\n\nPredictive Uncertainty Quantification\n\n‚ùì Q&A (\\(\\approx\\) 10min)"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#a-framework-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-dscc/presentation.html#a-framework-for-counterfactual-explanations",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "A Framework for Counterfactual Explanations",
    "text": "A Framework for Counterfactual Explanations\n\nEven though [‚Ä¶] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the ‚Äúblack box‚Äù. (Wachter, Mittelstadt, and Russell 2017)\n\n\n\nFramework\n\nObjective originally proposed by Wachter, Mittelstadt, and Russell (2017) is as follows\n\\[\n\\min_{x\\prime \\in \\mathcal{X}} h(x\\prime) \\ \\ \\ \\mbox{s. t.} \\ \\ \\ M(x\\prime) = t\n\\qquad(5)\\]\nwhere \\(h\\) relates to the complexity of the counterfactual and \\(M\\) denotes the classifier.\n\n\nTypically this is approximated through regularization:\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) + \\lambda h(x\\prime)\n\\qquad(6)\\]\n\n\nIntuition\n\n\n\n\nFigure¬†2: A cat performing gradient descent in the feature space √† la Wachter, Mittelstadt, and Russell (2017)."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#counterfactuals-as-in-adversarial-examples",
    "href": "content/talks/posts/2022-dscc/presentation.html#counterfactuals-as-in-adversarial-examples",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Adversarial Examples?",
    "text": "Counterfactuals ‚Ä¶ as in Adversarial Examples?\n\n\n\nYes and no!\n\nWhile both are methodologically very similar, adversarial examples are meant to go undetected while CEs ought to be meaningful.\n\n\nDesiderata\n\n\ncloseness: the average distance between factual and counterfactual features should be small (Wachter, Mittelstadt, and Russell (2017))\nactionability: the proposed feature perturbation should actually be actionable (Ustun, Spangher, and Liu (2019), Poyiadzi et al. (2020))\nplausibility: the counterfactual explanation should be plausible to a human (Joshi et al. (2019))\nunambiguity: a human should have no trouble assigning a label to the counterfactual (Schut et al. (2021))\nsparsity: the counterfactual explanation should involve as few individual feature changes as possible (Schut et al. (2021))\nrobustness: the counterfactual explanation should be robust to domain and model shifts (Upadhyay, Joshi, and Lakkaraju (2021))\ndiversity: ideally multiple diverse counterfactual explanations should be provided (Mothilal, Sharma, and Tan (2020))\ncausality: counterfactual explanations reflect the structural causal model underlying the data generating process (Karimi et al. (2020), Karimi, Sch√∂lkopf, and Valera (2021))"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#counterfactuals-as-in-causal-inference",
    "href": "content/talks/posts/2022-dscc/presentation.html#counterfactuals-as-in-causal-inference",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Counterfactuals ‚Ä¶ as in Causal Inference?",
    "text": "Counterfactuals ‚Ä¶ as in Causal Inference?\n\nNO!\n\n\n\nCausal inference: counterfactuals are thought of as unobserved states of the world that we would like to observe in order to establish causality.\n\nThe only way to do this is by actually interfering with the state of the world: \\(p(y|do(x),\\theta)\\).\nIn practice we can only move some individuals to the counterfactual state of the world and compare their outcomes to a control group.\nProvided we have controlled for confounders, properly randomized, ‚Ä¶ we can estimate an average treatment effect: \\(\\hat\\theta\\).\n\nCounterfactual Explanations: involves perturbing features after some model has been trained.\n\nWe end up comparing modeled outcomes \\(p(y|x,\\hat\\phi)\\) and \\(p(y|x\\prime,\\hat\\phi)\\) for individuals.\nWe have not magically solved causality.\n\n\n\n\nThe number of ostensibly pro data scientists confusing themselves into believing that \"counterfactual explanations\" capture real-world causality is just staggeringü§¶‚Äç‚ôÄÔ∏è. Where do we go from here? How can a community that doesn't even understand what's already known make advances?\n\n‚Äî Zachary Lipton (@zacharylipton) June 20, 2022"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#from-counterfactual-explanations-to-algorithmic-recourse",
    "href": "content/talks/posts/2022-dscc/presentation.html#from-counterfactual-explanations-to-algorithmic-recourse",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "From Counterfactual Explanations to Algorithmic Recourse",
    "text": "From Counterfactual Explanations to Algorithmic Recourse\n\n\n\n‚ÄúYou cannot appeal to (algorithms). They do not listen. Nor do they bend.‚Äù\n‚Äî Cathy O‚ÄôNeil in Weapons of Math Destruction, 2016\n\n\n\n\nFigure¬†3: Cathy O‚ÄôNeil. Source: Cathy O‚ÄôNeil a.k.a. mathbabe.\n\n\n\nAlgorithmic Recourse\n\n\nO‚ÄôNeil (2016) points to various real-world involving black-box models and affected individuals facing adverse outcomes.\n\n\n\n\nThese individuals generally have no way to challenge their outcome.\n\n\n\n\nCounterfactual Explanations that involve actionable and realistic feature perturbations can be used for the purpose of Algorithmic Recourse."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#limited-software-availability",
    "href": "content/talks/posts/2022-dscc/presentation.html#limited-software-availability",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Limited Software Availability",
    "text": "Limited Software Availability\n\nWork currently scattered across different GitHub repositories ‚Ä¶\n\n\n\n\n\nOnly one unifying Python library: CARLA (Pawelczyk et al. 2021).\n\nComprehensive and (somewhat) extensible.\nNot composable: each generator is treated as different class/entity.\n\nBoth R and Julia lacking any kind of implementation.\nEnter: üëâ CounterfactualExplanations.jl Altmeyer (2022)\n\n\n\n\n\n\nPhoto by Volodymyr Hryshchenko on Unsplash."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#counterfactualexplanations.jl",
    "href": "content/talks/posts/2022-dscc/presentation.html#counterfactualexplanations.jl",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "CounterfactualExplanations.jl üì¶",
    "text": "CounterfactualExplanations.jl üì¶\n     \n\n\n\n\nA unifying framework for generating Counterfactual Explanations.\nFast, extensible and composable allowing users and developers to add and combine different counterfactual generators.\nImplements a number of SOTA generators.\nBuilt in Julia, but can be used to explain models built in R and Python (still experimental).\n\n\n\n\n\n\nPhoto by Denise Jans on Unsplash.\n\n\n\n\n\n\nJulia has an edge with respect to Trustworthy AI: it‚Äôs open-source, uniquely transparent and interoperable üî¥üü¢üü£"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#a-simple-example",
    "href": "content/talks/posts/2022-dscc/presentation.html#a-simple-example",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "A simple example",
    "text": "A simple example\n\n\n\nLoad and prepare some toy data.\nSelect a random sample.\nGenerate counterfactuals using different approaches.\n\n\n# Data:\nusing Random\nRandom.seed!(123)\nN = 100\nusing CounterfactualExplanations\nxs, ys = toy_data_linear(N)\nX = hcat(xs...)\ncounterfactual_data = CounterfactualData(X,ys')\n\n# Randomly selected factual:\nx = select_factual(counterfactual_data,rand(1:size(X)[2]))"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#generic-generator",
    "href": "content/talks/posts/2022-dscc/presentation.html#generic-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Generic Generator",
    "text": "Generic Generator\n\n\nCode\n\nWe begin by instantiating the fitted model ‚Ä¶\n\n# Model\nw = [1.0 1.0] # estimated coefficients\nb = 0 # estimated bias\nM = LogisticModel(w, [b])\n\n\n\n‚Ä¶ then based on its prediction for \\(x\\) we choose the opposite label as our target ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ and finally generate the counterfactual.\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 36 steps.\n\n\n\n\nOutput\n\n\n‚Ä¶ et voil√†!\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=5)\n\n\n\nFigure¬†4: Counterfactual path (left) and predicted probability (right) for GenericGenerator. The contour (left) shows the predicted probabilities of the classifier (Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#probabilistic-methods-for-counterfactual-explanations",
    "href": "content/talks/posts/2022-dscc/presentation.html#probabilistic-methods-for-counterfactual-explanations",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Probabilistic Methods for Counterfactual Explanations",
    "text": "Probabilistic Methods for Counterfactual Explanations\nWhen people say that counterfactuals should look realistic or plausible, they really mean that counterfactuals should be generated by the same Data Generating Process (DGP) as the factuals:\n\\[\nx\\prime \\sim p(x)\n\\]\nBut how do we estimate \\(p(x)\\)? Two probabilistic approaches ‚Ä¶\n\n\nAPPROACH 1: use the model itselfAPPROACH 2: use some generative model\n\n\n\n\nSchut et al. (2021) note that by maximizing predictive probabilities \\(\\sigma(M(x\\prime))\\) for probabilistic models \\(M\\in\\mathcal{\\widetilde{M}}\\) one implicitly minimizes epistemic and aleotoric uncertainty.\n\\[\nx\\prime = \\arg \\min_{x\\prime}  \\ell(M(x\\prime),t) \\ \\ \\ , \\ \\ \\ M\\in\\mathcal{\\widetilde{M}}\n\\qquad(7)\\]\n\n\n\n\nFigure¬†5: A cat performing gradient descent in the feature space √† la Schut et al. (2021)\n\n\n\n\n\n\n\n\nInstead of perturbing samples directly, some have proposed to instead traverse a lower-dimensional latent embedding learned through a generative model (Joshi et al. 2019).\n\\[\nz\\prime = \\arg \\min_{z\\prime}  \\ell(M(dec(z\\prime)),t) + \\lambda h(x\\prime)\n\\qquad(8)\\]\nand\n\\[x\\prime = dec(z\\prime)\\]\nwhere \\(dec(\\cdot)\\) is the decoder function.\n\n\n\n\nFigure¬†6: Counterfactual (yellow) generated through latent space search (right panel) following Joshi et al. (2019). The corresponding counterfactual path in the feature space is shown in the left panel."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#greedy-generator",
    "href": "content/talks/posts/2022-dscc/presentation.html#greedy-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Greedy Generator",
    "text": "Greedy Generator\n\n\nCode\n\nThis time we use a Bayesian classifier ‚Ä¶\n\nusing LinearAlgebra\nŒ£ = Symmetric(reshape(randn(9),3,3).*0.01 + UniformScaling(1)) # MAP covariance matrix\nŒº = hcat(b, w)\nM = BayesianLogisticModel(Œº, Œ£)\n\n\n\n‚Ä¶ and once again choose our target label as before ‚Ä¶\n\n# Select target class:\ny = round(probs(M, x)[1])\ntarget = ifelse(y==1.0,0.0,1.0) # opposite label as target\n\n\n\n‚Ä¶ to then finally use greedy search to find a counterfactual.\n\n# Counterfactual search:\ngenerator = GreedyGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 67 steps.\n\n\n\n\nOutput\n\n\nIn this case the Bayesian approach yields a similar outcome.\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=15)\n\n\n\nFigure¬†7: Counterfactual path (left) and predicted probability (right) for GreedyGenerator. The contour (left) shows the predicted probabilities of the classifier (Bayesian Logistic Regression)."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#latent-space-generator",
    "href": "content/talks/posts/2022-dscc/presentation.html#latent-space-generator",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Latent Space Generator",
    "text": "Latent Space Generator\n\n\nCode\n\nUsing the same classifier as before we can either use the specific REVISEGenerator ‚Ä¶\n\n# Counterfactual search:\ngenerator = REVISEGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator\n)\n\n\n\n‚Ä¶ or realize that that REVISE (Joshi et al. 2019) just boils down to generic search in a latent space:\n\n# Counterfactual search:\ngenerator = GenericGenerator()\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator,\n  latent_space=true\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 8 steps.\n\n\n\n\nOutput\n\n\nWe have essentially combined latent search with a probabilistic classifier (as in Antor√°n et al. (2020)).\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=2)\n\n\n\nFigure¬†8: Counterfactual path (left) and predicted probability (right) for REVISEGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#diverse-counterfactuals",
    "href": "content/talks/posts/2022-dscc/presentation.html#diverse-counterfactuals",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Diverse Counterfactuals",
    "text": "Diverse Counterfactuals\n\n\nCode\n\nWe can use the DiCEGenerator to produce multiple diverse counterfactuals:\n\n# Counterfactual search:\ngenerator = DiCEGenerator(Œª=[0.1, 5.0])\ncounterfactual = generate_counterfactual(\n  x, target, counterfactual_data, M, generator;\n  num_counterfactuals = 5\n)\n\n\nConvergence: ‚úÖ\n\n\n\n after 229 steps.\n\n\n\n\nOutput\n\n\nanim = animate_path(counterfactual; plot_proba=true, colorbar=false, size=(800,300), alpha_=0.7)\ngif(anim, fps=20)\n\n\n\nFigure¬†9: Counterfactual path (left) and predicted probability (right) for DiCEGenerator."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#motivation",
    "href": "content/talks/posts/2022-dscc/presentation.html#motivation",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Motivation",
    "text": "Motivation\n\n\n\nTL;DR: We find that standard implementation of various SOTA approaches to AR can induce substantial domain and model shifts. We argue that these dynamics indicate that individual recourse generates hidden external costs and provide mitigation strategies.\n\nIn this work we investigate what happens if Algorithmic Recourse is actually implemented by a large number of individuals.\nFigure¬†10 illustrates what we mean by Endogenous Macrodynamics in Algorithmic Recourse:\n\nPanel (a): we have a simple linear classifier trained for binary classification where samples from the negative class (y=0) are marked in blue and samples of the positive class (y=1) are marked in orange\nPanel (b): the implementation of AR for a random subset of individuals leads to a noticable domain shift\nPanel (c): as the classifier is retrained we observe a corresponding model shift (Upadhyay, Joshi, and Lakkaraju 2021)\nPanel (d): as this process is repeated, the decision boundary moves away from the target class.\n\n\n\n\n\nFigure¬†10: Proof of concept: repeated implementation of AR leads to domain and model shifts.\n\n\n\nWe argue that these shifts should be considered as an expected external cost of individual recourse and call for a paradigm shift from individual to collective recourse in these types of situations."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#findings",
    "href": "content/talks/posts/2022-dscc/presentation.html#findings",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Findings",
    "text": "Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-word data."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#mitigation-strategies---intuition",
    "href": "content/talks/posts/2022-dscc/presentation.html#mitigation-strategies---intuition",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Mitigation Strategies - Intuition",
    "text": "Mitigation Strategies - Intuition\n\n\n\nChoose more conservative decision thresholds.\nClassifer Preserving ROAR (ClaPROAR): penalise classifier loss.\nGravitational Counterfactual Explanations: penalise distance to some sensible point in the target domain.\n\n\n\n\n\nFigure¬†11: Illustrative example demonstrating the properties of the various mitigation strategies. Samples from the negative class \\((y = 0)\\) are marked in blue while samples of the positive class \\((y = 1)\\) are marked in orange."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#mitigation-strategies---findings",
    "href": "content/talks/posts/2022-dscc/presentation.html#mitigation-strategies---findings",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Mitigation Strategies - Findings",
    "text": "Mitigation Strategies - Findings\n\n\n\n\n\nMitigation strategies applied to synthetic data.\n\n\n\n\n\n\nMitigation strategies applied to real-world data."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "href": "content/talks/posts/2022-dscc/presentation.html#effortless-bayesian-deep-learning-through-laplace-redux",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Effortless Bayesian Deep Learning through Laplace Redux",
    "text": "Effortless Bayesian Deep Learning through Laplace Redux\n   \n\n\nLaplaceRedux.jl (formerly BayesLaplace.jl) is a small package that can be used for effortless Bayesian Deep Learning and Logistic Regression trough Laplace Approximation. It is inspired by this Python library and its companion paper.\n\n\n\nPlugin Approximation (left) and Laplace Posterior (right) for simple artificial neural network.\n\n\n\n\n\n\nSimulation of changing posteriour predictive distribution. Image by author."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#conformalprediction.jl",
    "href": "content/talks/posts/2022-dscc/presentation.html#conformalprediction.jl",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "ConformalPrediction.jl",
    "text": "ConformalPrediction.jl\n      \nConformalPrediction.jl is a package for Uncertainty Quantification (UQ) through Conformal Prediction (CP) in Julia. It is designed to work with supervised models trained in MLJ (Blaom et al. 2020). Conformal Prediction is distribution-free, easy-to-understand, easy-to-use and model-agnostic.\n\n\n\nConformal Prediction in action: Prediction sets for two different samples and changing coverage rates. As coverage grows, so does the size of the prediction sets."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#more-resources",
    "href": "content/talks/posts/2022-dscc/presentation.html#more-resources",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "More Resources üìö",
    "text": "More Resources üìö\n\n\n\nRead on ‚Ä¶\n\n\nBlog post introducing CE: [TDS], [blog].\nBlog post on Laplace Redux: [TDS], [blog].\nBlog post on Conformal Prediction: [TDS], [blog].\n\n\n‚Ä¶ or get involved! ü§ó\n\n\nContributor‚Äôs Guide for CounterfactualExplanations.jl\nContributor‚Äôs Guide for ConformalPrediction.jl"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#image-sources",
    "href": "content/talks/posts/2022-dscc/presentation.html#image-sources",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "Image Sources",
    "text": "Image Sources\n\nCrystal ball on beach: Nicole Avagliano on Unsplash\nColour gradient: A.Z on Unsplash\nElephant herd: Sergi Ferrete on Unsplash\nDSCC 2022 logo: ING"
  },
  {
    "objectID": "content/talks/posts/2022-dscc/presentation.html#references",
    "href": "content/talks/posts/2022-dscc/presentation.html#references",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "References",
    "text": "References\n\n\n\nExplaining Black-Box Models through Counterfactuals ‚Äî Patrick Altmeyer ‚Äî CC BY-NC\n\n\n\nAltmeyer, Patrick. 2022. ‚ÄúCounterfactualExplanations.Jl - a Julia Package for Counterfactual Explanations and Algorithmic Recourse.‚Äù https://github.com/pat-alt/CounterfactualExplanations.jl.\n\n\nAntor√°n, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and Jos√© Miguel Hern√°ndez-Lobato. 2020. ‚ÄúGetting a Clue: A Method for Explaining Uncertainty Estimates.‚Äù https://arxiv.org/abs/2006.06848.\n\n\nBlaom, Anthony D., Franz Kiraly, Thibaut Lienart, Yiannis Simillides, Diego Arenas, and Sebastian J. Vollmer. 2020. ‚ÄúMLJ: A Julia Package for Composable Machine Learning.‚Äù Journal of Open Source Software 5 (55): 2704. https://doi.org/10.21105/joss.02704.\n\n\nGoodfellow, Ian J, Jonathon Shlens, and Christian Szegedy. 2014. ‚ÄúExplaining and Harnessing Adversarial Examples.‚Äù https://arxiv.org/abs/1412.6572.\n\n\nJoshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. ‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù https://arxiv.org/abs/1907.09615.\n\n\nKarimi, Amir-Hossein, Bernhard Sch√∂lkopf, and Isabel Valera. 2021. ‚ÄúAlgorithmic Recourse: From Counterfactual Explanations to Interventions.‚Äù In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 353‚Äì62.\n\n\nKarimi, Amir-Hossein, Julius Von K√ºgelgen, Bernhard Sch√∂lkopf, and Isabel Valera. 2020. ‚ÄúAlgorithmic Recourse Under Imperfect Causal Knowledge: A Probabilistic Approach.‚Äù https://arxiv.org/abs/2006.06831.\n\n\nMothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. ‚ÄúExplaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.‚Äù In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607‚Äì17.\n\n\nMurphy, Kevin P. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nO‚ÄôNeil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown.\n\n\nPawelczyk, Martin, Sascha Bielawski, Johannes van den Heuvel, Tobias Richter, and Gjergji Kasneci. 2021. ‚ÄúCarla: A Python Library to Benchmark Algorithmic Recourse and Counterfactual Explanation Algorithms.‚Äù https://arxiv.org/abs/2108.00783.\n\n\nPoyiadzi, Rafael, Kacper Sokol, Raul Santos-Rodriguez, Tijl De Bie, and Peter Flach. 2020. ‚ÄúFACE: Feasible and Actionable Counterfactual Explanations.‚Äù In Proceedings of the AAAI/ACM Conference on AI, Ethics, and Society, 344‚Äì50.\n\n\nRudin, Cynthia. 2019. ‚ÄúStop Explaining Black Box Machine Learning Models for High Stakes Decisions and Use Interpretable Models Instead.‚Äù Nature Machine Intelligence 1 (5): 206‚Äì15.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In International Conference on Artificial Intelligence and Statistics, 1756‚Äì64. PMLR.\n\n\nUpadhyay, Sohini, Shalmali Joshi, and Himabindu Lakkaraju. 2021. ‚ÄúTowards Robust and Reliable Algorithmic Recourse.‚Äù https://arxiv.org/abs/2102.13620.\n\n\nUstun, Berk, Alexander Spangher, and Yang Liu. 2019. ‚ÄúActionable Recourse in Linear Classification.‚Äù In Proceedings of the Conference on Fairness, Accountability, and Transparency, 10‚Äì19.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. ‚ÄúCounterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.‚Äù Harv. JL & Tech. 31: 841.\n\n\nWilson, Andrew Gordon. 2020. ‚ÄúThe Case for Bayesian Deep Learning.‚Äù https://arxiv.org/abs/2001.10995."
  },
  {
    "objectID": "content/talks/posts/2022-dscc/index.html",
    "href": "content/talks/posts/2022-dscc/index.html",
    "title": "Explaining Black-Box Models through Counterfactuals",
    "section": "",
    "text": "In November, 2022, I gave a one-hour presentation about Counterfactual Explanations at the ING Data Science Community Conference (DSCC) 2022. You can find the slides here.\n\n\n\nDSCC 2022 Logo"
  },
  {
    "objectID": "content/talks/index.html",
    "href": "content/talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Explaining Black-Box Models through Counterfactuals\n\n\nA Gentle Introduction\n\n\nSlides for my presentation at the Bank of England in November, 2022.\n\n\n\n\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nExplaining Black-Box Models through Counterfactuals\n\n\nA Gentle Introduction\n\n\nSlides for my presentation at the ING Data Science Community Conference 2022.\n\n\n\n\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nJuliaCon 2022\n\n\nVarious Presentations\n\n\nI gave three different talks at JuliaCon 2022.\n\n\n\n\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/about/contact.html",
    "href": "content/about/contact.html",
    "title": "Patrick Altmeyer",
    "section": "",
    "text": "Contact\nYou can best reach me via email or set up a chat."
  },
  {
    "objectID": "content/about/index.html",
    "href": "content/about/index.html",
    "title": "Patrick Altmeyer",
    "section": "",
    "text": "Researching Trustworthy Artificial Intelligence (AI) for Finance and Economics. I am an economist by background with an interest in cross-disciplinary research on the intersection of Trustworthy AI and Financial Economics. For my PhD in Trustworthy AI, I currently focus on Counterfactual Explanations and Probabilisitic Machine Learning under supervision of Cynthia Liem at Delft University of Technology. I am also a member of the AI for Fintech Research Lab.\nPreviously, I worked as an economist for Bank of England where I was involved in research, monetary policy briefings and market intelligence. I hold two masters degrees from Barcelona School of Economics, one in Data Science and one in Finance. I also hold an undergraduate degree in Economics from the University of Edinburgh.\nDownloadable resume: [html].\n\nContact\nYou can best reach me via email or set up a chat."
  },
  {
    "objectID": "content/about/biography.html",
    "href": "content/about/biography.html",
    "title": "Patrick Altmeyer",
    "section": "",
    "text": "Researching Trustworthy Artificial Intelligence (AI) for Finance and Economics. I am an economist by background with an interest in cross-disciplinary research on the intersection of Trustworthy AI and Financial Economics. For my PhD in Trustworthy AI, I currently focus on Counterfactual Explanations and Probabilisitic Machine Learning under supervision of Cynthia Liem at Delft University of Technology. I am also a member of the AI for Fintech Research Lab.\nPreviously, I worked as an economist for Bank of England where I was involved in research, monetary policy briefings and market intelligence. I hold two masters degrees from Barcelona School of Economics, one in Data Science and one in Finance. I also hold an undergraduate degree in Economics from the University of Edinburgh.\nDownloadable resume: [html]."
  },
  {
    "objectID": "content/software.html",
    "href": "content/software.html",
    "title": "Software",
    "section": "",
    "text": "I code in Julia üî¥üü£üü¢ and write in Quarto. Occasionally I also use R, Python and C++. After years of consuming open-source software, I‚Äôve recently started contributing free open-source software myself:"
  },
  {
    "objectID": "content/software.html#activity",
    "href": "content/software.html#activity",
    "title": "Software",
    "section": "Activity",
    "text": "Activity"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Patrick Altmeyer",
    "section": "",
    "text": "Welcome.\nI‚Äôm a PhD Candidate in Trustworthy Artificial Intelligence at Delft University of Technology working on the intersection of Computer Science and Finance.\nMy current research revolves around Counterfactual Explanations and Probabilistic Machine Learning. Previously, I worked as and Economist for the Bank of England.\nI code in Julia üî¥üü£üü¢ and write in Quarto. Occasionally I also use R, Python and C++. As much as possible I contribute to open-source: Github.\nTweets by paltmey"
  },
  {
    "objectID": "blog/posts/a-peek-inside-the-black-box-interpreting-neural-networks/index.html",
    "href": "blog/posts/a-peek-inside-the-black-box-interpreting-neural-networks/index.html",
    "title": "A peek inside the ‚ÄòBlack Box‚Äô - interpreting neural networks",
    "section": "",
    "text": "Propelled by advancements in modern computer technology, deep learning has re-emerged as perhaps the most promising artificial intelligence (AI) technology of the last two decades. By treating problems as a nested, hierarchy of hidden layers deep artificial neural networks achieve the power and flexibility necessary for AI systems to navigate complex real-world environments. Unfortunately, their very nature has earned them a reputation as Black Box algorithms and their lack of interpretability remains a major impediment to their more wide-spread application.\nIn science, research questions usually demand not just answers but also explanations and variable selection is often as important as prediction (Ish-Horowicz et al. 2019). Economists, for example, recognise the undeniable potential of deep learning, but are rightly hesitant to employ novel tools that are not fully transparent and ultimately cannot be trusted. Similarly, real-world applications of AI have come under increasing scrutiny with regulators imposing that individuals influenced by algorithms should have the right to obtain explanations (Fan, Xiong, and Wang 2020). In high-risk decision-making fields such as AI systems that drive autonomous vehicles the need for explanations is self-evident (Ish-Horowicz et al. 2019).\nIn light of these challenges it is not surprising that research on explainable AI has recently gained considerable momentum (Arrieta et al. 2020). While in this short essay we will focus on deep learning in particular, it should be noted that this growing body of literature is concerned with a broader realm of machine learning models. The rest of this note is structured as follows: the first section provides a brief overview of recent advancements towards interpreting deep neural networks largely drawing on Fan, Xiong, and Wang (2020); the second section considers a novel entropy-based approach towards interpretability proposed by Crawford et al. (2019); finally, in the last section we will see how this approach can be applied to deep neural networks as proposed in Ish-Horowicz et al. (2019).\n\nInterpretable DL - a whistle-stop tour\nBefore delving further into how the intrinsics of deep neural networks can be disentangled we should first clarify what interpretability in the context of algorithms actually means. Fan, Xiong, and Wang (2020) describes model interpretability simply as the extent to which humans can ‚Äúunderstand and reason‚Äù the model. This may concern an understanding of both the ad-hoc workings of the algorithm as well as the post-hoc interpretability of its output. In the context of linear regression, for example, ad-hoc workings of the model are often described through the intuitive idea of linearly projecting the outcome variable \\(\\mathbf{y}\\) onto the column space of \\(\\mathbf{X}\\). Post-hoc interpretations usually center around variable importance ‚Äì the main focus of the following sections. Various recent advancements tackle interpretability of DNNs from different angles depending on whether the focus is on ad-hoc or post-hoc interpretability. Fan, Xiong, and Wang (2020) further asses that model interpretability hinges on three main aspects of simulatability, decomposability and algorithmic transparency, but for the purpose of this short note the ad-hoc vs.¬†post-hoc taxonomy provides a simpler more natural framework. 1\nUnderstanding the ad-hoc intrinsic mechanisms of a DNN is inherently difficult. While generally transparency may be preserved in the presence of nonlinearity (e.g.¬†decision trees), multiple hidden layers of networks (each of them) involving nonlinear operations are usually out of the realm of human comprehension (Fan, Xiong, and Wang 2020). Training also generally involves optimization of non-convex functions that involve an increasing number of saddle points as the dimensionality increases (Fan, Xiong, and Wang 2020). Methods to circumvent this problematic usually boil down to decreasing the overall complexity, either by regularizing the model or through proxy methods. Regularization ‚Äì while traditionally done to avoid overfitting ‚Äì has been found to be useful to create more interpretable representations. Monotonicity constraints, for example, impose that as the value of a specified covariate increases model predictions either monotonically decrease or increase. Proxy methods construct simpler representations of a learned DNN, such as a rule-based decision tree. This essentially involves repeatedly querying the trained network while varying the inputs and then deriving decision rules based on the model output.\nPost-hoc interpretability usually revolves around the understanding of feature importance. A greedy approach to this issue involves simply removing features one by one and checking how model predictions change. A more sophisticated approach along these lines is Shapley value, which draws on cooperative game theory. The Shapley value assigns varying payouts to players depending on their contribution to overall payout. In the context of neural networks input covariate \\(\\mathbf{X}_p\\) represents a player while overall payout is represented by the difference between average and individual outcome predictions.2 Exact computations of Shapley values are prohibitive as the dimensionality increases, though approximate methods have recently been developed (Fan, Xiong, and Wang 2020).\nThe remainder of this note focuses on a novel approach to feature extraction that measures entropy shifts in a learned probabilistic neural network in response to model inputs \\(\\mathbf{X_1},...,\\mathbf{X}_P\\). We will first introduce this methodology in the context of Gaussian Process regression in the following section before finally turning to its application to Bayesian neural networks.\n\n\nAn entropy-based approach to variable importance\nIsh-Horowicz et al. (2019) motivate their methodology for interpreting neural networks through Gaussian Process regression. Consider the following Bayesian regression model with Gaussian priors:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& f(\\mathbf{X}|\\mathbf{w})&=\\phi(\\mathbf{X})^T\\mathbf{w} + \\varepsilon, &&\\varepsilon \\sim \\mathcal{N}(0,\\mathbf{I}) \\\\\n&& \\mathbf{w}& \\sim \\mathcal{N}(0,{1\\over{\\lambda}} \\mathbf{I})\\\\\n\\end{aligned}\n\\end{equation}\n\\tag{1}\\]\nThis naturally gives rise to a particular example of a Gaussian Process (GP). In particular, since \\(\\mathbf{u}(\\mathbf{X})=\\Phi(\\mathbf{X})^T\\mathbf{w}\\) is just a linear combination fo Gaussian random variables it follows a Gaussian Process itself\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\mathbf{u}(\\mathbf{X})=\\Phi(\\mathbf{X})^T\\mathbf{w}& \\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{K}) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{2}\\]\nwhere \\(\\mathbf{K}\\) is the Kernel (or Gram) matrix and \\(K_{i,j}=k(\\mathbf{X_i,\\mathbf{X}_j})={1\\over{\\lambda}}\\phi(\\mathbf{X_i})^T\\phi(\\mathbf{X_m})\\) is the kernel function (Bishop 2006). In other words, the prior distribution over \\(\\mathbf{w}\\) induces a probability distribution over random functions \\(\\mathbf{u}(\\mathbf{X})\\). Similarly, the GP can be understood as a prior distribution over a an infinite-dimensional reproducible kernel Hilbert space (RKHS) (Crawford et al. 2019), which in a finite-dimensional setting becomes multivariate Gaussian.\nIn a standard linear regression model coefficients characterize the projection of the outcome variable \\(\\mathbf{y}\\) onto the column space of the regressors \\(\\mathbf{X}\\). In particular, with ordinary least square we define:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\beta&=(\\mathbf{X}^T\\mathbf{X})^{-1}\\mathbf{X}^T\\mathbf{y} \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{3}\\]\nThe primary focus here is to learn the mapping from input to output. The key differentiating feature between this approach and the non-parametric model in Equation¬†1 is the fact that in case of the latter we are interested in learning not only the mapping from inputs to outputs, but also the representation (\\(\\mathbf{u}(\\mathbf{X})\\)) of the inputs (see for example (Goodfellow, Bengio, and Courville 2016)). To be even more specific, treating the feature representation itself as random as in Equation¬†1 allows us to learn non-linear relationships between the covariates \\(\\mathbf{X}\\), since they are implicitly captured by the RKHS (Crawford et al. 2019). Neural networks share this architecture and hence it is worth dwelling on it a bit further: the fact that the learned model inherently incorporates variable interactions leads to the observation that an individual feature is rarely important on its own with respect to the mapping from \\(\\mathbf{X}\\) to \\(\\mathbf{y}\\) (Ish-Horowicz et al. 2019). Hence, in order to gain an understanding of individual variable importance, one should aim to understand what role feature \\(\\mathbf{X}_j\\) plays within the learned model, thereby taking into account its interactions with other covariates. Formally, Crawford et al. (2019) and define the effect size analogue as the equivalent of the familiar regression coefficient in the non-parametric setting\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\tilde\\beta&=\\mathbf{X}^+\\Phi^T\\mathbf{w}=\\mathbf{X}^+\\mathbf{u} \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{4}\\]\nwhere \\(\\mathbf{X}^+=\\lim_{\\alpha} (\\mathbf{X}^T\\mathbf{X}+\\alpha \\mathbf{I})^{-1}\\mathbf{X}^T\\) denotes the Moore-Penrose pseudo-inverse (see for example Goodfellow, Bengio, and Courville (2016)). Intuitively the effect size analogue can be thought of as the resulting coefficients from regressing the fitted values \\(\\hat{\\mathbf{u}}\\) from the learned probabilistic model on the covariates \\(\\mathbf{X}\\). It can be interpreted in the same way as linear regression coefficients, in the sense that \\(\\tilde\\beta_j\\) describes the marginal change in \\(\\mathbf{u}\\) given a unit increase in \\(\\mathbf{X}_j\\) holding all else constant. Note here the subtle, but crucial difference between Equation¬†3 ‚Äì a projection from the outcome variable onto the column space of \\(\\mathbf{X}\\) ‚Äì and Equation¬†4 ‚Äì a projection from the learned model to \\(\\mathbf{X}\\). In other words, looking at \\(\\tilde\\beta\\) can be thought of peeking directly into the Block Box. Unfortunately, as Crawford et al. (2019) point out, working with Equation¬†4 is usually not straight-forward. From a practitioner‚Äôs point of view, it may also not be obvious how to interpret a coefficient that describes marginal effects of input variables on a learned model. A more useful indicator in this context would provide a measure of how much individual variables contribute to the overall variation in the learned model. For this purpose Crawford et al. (2019) propose to work with a distributional centrality measure based on \\(\\tilde\\beta\\), which we shall turn to next.\nThe proposed methodology in Crawford et al. (2019) and Ish-Horowicz et al. (2019) depends on the availability of a posterior distribution over \\(\\tilde\\beta\\) in that it measures its entropic shifts in response to the introduction of covariates. The intuition is straight-forward: within the context of the learned probabilistic model is covariate \\(\\mathbf{X}_j\\) informative or not? More formally this boils down to determining if the posterior distribution of \\(p(\\tilde\\beta_{-j})\\) is dependent on the effect of \\(\\tilde\\beta_j\\). This can be quantified through the Kullback-Leibler divergence (KLD) between \\(p(\\tilde\\beta_{-j})\\) and the conditional posterior \\(p(\\tilde\\beta_{-j}|\\tilde\\beta_j)\\):\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\text{KLD}_j&=\\text{KL}\\left(p(\\tilde\\beta_{-j}) || p(\\tilde\\beta_{-j}|\\tilde\\beta_j)\\right) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{5}\\]\nCovariates that contribute significant information to the model will have \\(\\text{KLD}>0\\), while for insignificant covariates \\(\\text{KLD}\\approx0\\). The measure of induced entropy change gives rise to a ranking of the covariates in terms of their relative importance in the model. The RATE criterion of variable \\(\\mathbf{X}_j\\) is then simply defined as\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\gamma_j&=\\frac{\\text{KLD}_j}{\\sum_{p=1}^{P}\\text{KLD}_p}\\in[0,1] \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{6}\\]\nwhich in light of its bounds can naturally be interpreted as \\(\\mathbf{X}_j\\)`s percentage contribution to the learned model. It is worth noting that \\(p(\\tilde\\beta_{-j}|\\tilde\\beta_j)\\) of course depends on the value of the conditioning variable. A natural choice is \\(\\tilde\\beta_j=0\\) which usually corresponds to the null hypothesis.\n\n\nApplication to Bayesian neural networks\nIn order to use the RATE criterion in the context of deep learning we need to work in the Bayesian setting. Contrary to standard artificial neural networks which work under the assumption that weights have some true latent value, Bayesian neural networks place a prior distribution over network parameters and hence treat weights as random variables (Goan and Fookes 2020). Not only does it perhaps seem more natural to treat unobserved weights as random, but the Bayesian setting also naturally gives rise to reason about uncertainty in predictions, which can ultimately help us develop more trustworthy models (Goan and Fookes 2020). A drawback of BNNs is that exact computation of posteriors is computationally challenging and often intractable (a non-trivial issue that we will turn back to in a moment).\nWhen the prior placed over parameters is Gaussian, the output of the BNN approaches a Gaussian Process as the width of the network grows, in line with the discussion in the previous section. This is exactly the assumption that Ish-Horowicz et al. (2019) work with. They propose an architecture for a multi-layer perceptron (MLP) composed of (1) an input layer collecting covariates \\(\\mathbf{X}_1,...,\\mathbf{X}_p\\), (2) a single deterministic, hidden layer and (3) an outer layer producing predictions from a probabilistic model \\(\\mathbf{u}(\\mathbf{X})\\). Let \\(\\mathbf{X}\\) be a \\((N \\times P)\\) matrix of covariates. Then formally, we have\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\hat{\\mathbf{y}}&=\\sigma(\\mathbf{u}) \\\\\n&& \\mathbf{u}(\\mathbf{Z})&=\\mathbf{Z}(\\mathbf{X})\\mathbf{w}^{(L+1)}, && \\mathbf{w}^{(L+1)} \\sim \\mathcal{N}(\\mathbf{m}, \\mathbf{V}) \\\\\n&& \\mathbf{Z}(\\mathbf{X})&=f(\\mathbf{X}\\mathbf{w}^{(L)}) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{7}\\]\nwhere \\(\\sigma(.)\\) is a link function and \\(\\mathbf{u}(\\mathbf{X})\\) represents the probabilistic model learned in the outer layer with weights \\(\\mathbf{w}^{(L+1)}\\) assumed to be Gaussian random variables.3 Finally, \\(\\mathbf{Z}(\\mathbf{X})\\) denotes the inner (or more generally penultimate) layer, an \\((N \\times P)\\) matrix of neural activations through \\(f:(\\mathbf{X}\\mathbf{w}^{(L)})\\mapsto \\mathbf{Z}\\). Ish-Horowicz et al. (2019) work with a simple single-layer MLP, but it should be evident that this be extended to arbitrary depth and complexity, while still maintaining the high-level structure imposed by Equation¬†7. This flexibility allows RATE to be applied to a wide range of Bayesian network architectures, since all that is really required is the posterior distribution over weights \\(\\mathbf{w}^{(L+1)}\\), which arises from the probabilistic outer layer. The fact that only the outer layer needs to be probabilistic has the additional benefit of mitigating the computational burden that comes with Bayesian inference, which was mentioned earlier.\nHaving established this basic, flexible set-up the Ish-Horowicz et al. (2019) go on to derive closed-form expressions for RATE in this setting. The details are omitted here since the logic is largely analogous to what we learned above, but can be found in Ish-Horowicz et al. (2019).\n\n\nConclusion\nThe RATE criterion originally proposed by Crawford et al. (2019) and shown to be applicable to Bayesian neural networks in Ish-Horowicz et al. (2019) offers an intuitive way to measure variable importance in the context of deep learning. By defining variable importance as the contribution inputs make to a probabilistic model, it implicitly incorporates the interactions between covariates and nonlinearities that the model has learned. In other words, it allows researchers to peek directly into the Black Box. This opens up interesting avenues for future research, as the approach can be readily applied in academic disciplines and real-world applications that rely heavily on explainability of outcomes.\n\n\n\nReferences\n\n\nArrieta, Alejandro Barredo, Natalia Diaz-Rodriguez, Javier Del Ser, Adrien Bennetot, Siham Tabik, Alberto Barbado, Salvador Garcia, et al. 2020. ‚ÄúExplainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges Toward Responsible AI.‚Äù Information Fusion 58: 82‚Äì115.\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. springer.\n\n\nCrawford, Lorin, Seth R Flaxman, Daniel E Runcie, and Mike West. 2019. ‚ÄúVariable Prioritization in Nonlinear Black Box Methods: A Genetic Association Case Study.‚Äù The Annals of Applied Statistics 13 (2): 958.\n\n\nFan, Fenglei, Jinjun Xiong, and Ge Wang. 2020. ‚ÄúOn Interpretability of Artificial Neural Networks.‚Äù https://arxiv.org/abs/2001.02522.\n\n\nGoan, Ethan, and Clinton Fookes. 2020. ‚ÄúBayesian Neural Networks: An Introduction and Survey.‚Äù In Case Studies in Applied Bayesian Data Science, 45‚Äì87. Springer.\n\n\nGoodfellow, Ian, Yoshua Bengio, and Aaron Courville. 2016. Deep Learning. MIT Press.\n\n\nIsh-Horowicz, Jonathan, Dana Udwin, Seth Flaxman, Sarah Filippi, and Lorin Crawford. 2019. ‚ÄúInterpreting Deep Neural Networks Through Variable Importance.‚Äù https://arxiv.org/abs/1901.09839.\n\n\n\n\n\n\n\n\nFootnotes\n\n\nSimulatability describes the overall, high-level understandability of the mechanisms underlying the model ‚Äì put simply, the less complex the model, the higher its simulatability. Decomposability concerns the extent to which the model can be taken apart into smaller pieces ‚Äì neural networks by there very nature are compositions of multiple layers. Finally, algorithmic transparency refers to the extent to which the training of the algorithm is well-understood and to some extent observable ‚Äì since DNNs generally deal with optimization of non-convex functions and often lack unique solution they are inherently intransparent.‚Ü©Ô∏é\nFor more detail see for example here.‚Ü©Ô∏é\nFor simplicity I have omitted the deterministic bias term.‚Ü©Ô∏é\n\nCitationBibTeX citation:@online{altmeyer2021,\n  author = {Patrick Altmeyer},\n  title = {A Peek Inside the ‚Äú{Black} {Box}‚Äù - Interpreting Neural\n    Networks},\n  date = {2021-02-07},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrick Altmeyer. 2021. ‚ÄúA Peek Inside the ‚ÄòBlack\nBox‚Äô - Interpreting Neural Networks.‚Äù February 7, 2021."
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html",
    "title": "How I‚Äôm building this website in R",
    "section": "",
    "text": "Note\n\n\n\nUpdate on Feb 20, 2022\nThe post below was written when I still used blogdown in combination with Hugo to build this blog. I have recently migrated the blog (along pretty much everything else I do) to quarto.\n\nQuarto¬Æ is an open-source scientific and technical publishing system built on Pandoc.\n\nBased on my first few experiences I would go further and say that quarto is the only open-source scientific and technical publishing system you‚Äôll ever need. The project is supported by RStudio and (unsurprisingly) Yihui Xie is one of the contributors. Go check it out!"
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#getting-started",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#getting-started",
    "title": "How I‚Äôm building this website in R",
    "section": "Getting started",
    "text": "Getting started\nIt turns out building a static website in R is remarkably easy, as long as you know your way around R Markdown. Knowledge of HTML and CSS helps, but is not strictly necessary and can be acquired along the way. My package of choice for this website is blogdown by Yihui Xie who has had a major impact on the R community through his many package contributions (knitr, bookdown, pagedown, ‚Ä¶) and certainly made my life a lot easier on many occasions.\nTo get started just follow the instructions on blogdown‚Äôs GitHub repository or keep reading here for a high-level overview. Setting up a basic website in R requires exactly two steps:\n\nSet up a local directory for the website. Let‚Äôs suppose you create it here ~/Documents/myAwesomeWebsite.\nIn R, navigate to the directory and simply run blogdown::newsite().\n\nThis will set up a basic template which you can develop. Changing the theme and playing with the basic structure of the website is relatively straight-forward. Personally I have so far managed to work things out based on a working knowledge of HTML and CSS that I‚Äôve developed in the past through my work with R Shiny."
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#deploying-your-website",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#deploying-your-website",
    "title": "How I‚Äôm building this website in R",
    "section": "Deploying your website",
    "text": "Deploying your website\nThere are various ways to deploy your website, i.e.¬†make it accessible to the public. This website is deployed through GitHub pages. Detailed instructions on how to do this can be found here. Since I already had an existing local clone of my pat-alt.github.io repo, I just dropped it in the source directory of the website:\nsource/\n‚îÇ\n‚îú‚îÄ‚îÄ config.yaml\n‚îú‚îÄ‚îÄ content/\n‚îú‚îÄ‚îÄ themes/\n‚îî‚îÄ‚îÄ ...\n\npatalt.github.io/\n‚îÇ\n‚îú‚îÄ‚îÄ .git/\n‚îú‚îÄ‚îÄ .nojekyll\n‚îú‚îÄ‚îÄ index.html\n‚îú‚îÄ‚îÄ about/\n‚îî‚îÄ‚îÄ ...\nAfter adding publishDir: pat-alt.github.io to my config.yaml and then running blogdown::hugo_build() the website was built inside the clone. All that was left to do was to commit changes from the local clone to the pat-alt.github.io remote repo. A few moments later the website was already up and running."
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#why-all-the-trouble",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#why-all-the-trouble",
    "title": "How I‚Äôm building this website in R",
    "section": "Why all the trouble?",
    "text": "Why all the trouble?\nThere are certainly easier ways to build a website. But if like me you do pretty much all your work in R Markdown and want to share some of it, then you will love blogdown. The beauty of it is that once the basic infrastructure is set up, adding content is as simple as running the following wrapper function\n\nblogdown::new_post(\"Your new post\", ext = \".Rmd\")\n\nwhere the first argument is just the title of your post and the ext argument can be used to specify that you want to create an R Markdown document that can include code chucks. The wrapper function will automatically set up a directory for your post under /post/. R Studio will redirect you to the relevant .Rmd file that you can then fill with content. By default that folder will look roughly like this:\n‚îú‚îÄ‚îÄ index.Rmd\n‚îú‚îÄ‚îÄ index.html\n‚îî‚îÄ‚îÄ index_files\n    ‚îî‚îÄ‚îÄ header-attrs\n        ‚îî‚îÄ‚îÄ header-attrs.js"
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#a-simple-coding-example",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#a-simple-coding-example",
    "title": "How I‚Äôm building this website in R",
    "section": "A simple coding example",
    "text": "A simple coding example\nAs you can probably tell from the code chunks above this post was created just in the way I described. So I thought I might as well go ahead with a simple coding example to add some flavour. Suppose you have built some function that you think is worth sharing with the world or simply learned something new and interesting. As a case in point, I recently had a look at the Rcpp package and wrote a small program in C++ to be used in R. Since R Markdown supports Rcpp code chunks (along with Python, bash, SQL, ‚Ä¶) it is straight-forward to show-case that code on this website.\nThe program can be used to simulate data from a categorical distribution. This distribution describes the possible results of a random variable that can take on one of \\(K\\) possible categories with different probabilities. In base R we could use rmultinom(n=1000,1,p=c(0.5,0.1,0.4)) to simulate draws from one such distribution with three different categories. Alternatively, we could write the program in C++ as follows:\n\n#include <Rcpp.h>\nusing namespace Rcpp;\n\n// [[Rcpp::export]]\nNumericMatrix simCategorical(int n, NumericVector p) {\n  int k = p.size();\n  NumericMatrix mat(k, n);\n  // Normalise prob if necessary:\n  if (sum(p)!=1) {\n    p = p/sum(p);\n  }\n  NumericVector emp_cdf = cumsum(p);\n  NumericVector u = Rcpp::runif(n, 0, 1);\n  // Matrix for 1-hot-encoding:\n  for (int j = 0; j < n; j++) {\n    // Perform binary search:\n    int l = 0;\n    int r = k;\n    double target = u[j];\n    while (l < r) {\n      int m = floor((l+r)/2);\n      if (emp_cdf[m] > target) {\n        r = m;\n      } else {\n        l = m+1;\n      }\n    }\n    mat(r,j) = 1;\n  }\n  return mat;\n}\n\nIn terms of performance it turns out that the simple C++ program actually does somewhat better than the base R alternative:\n\nlibrary(microbenchmark)\nlibrary(ggplot2)\nn <- 1000\np <- c(0.5,0.1,0.4)\nmb <- microbenchmark(\n    \"rmultinom\" = {rmultinom(n, 1, p)},\n    \"Rcpp\" = {simCategorical(n, p)}\n)\nautoplot(mb)"
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#embedding-existing-work",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#embedding-existing-work",
    "title": "How I‚Äôm building this website in R",
    "section": "Embedding existing work",
    "text": "Embedding existing work\nIf you have some existing work that you would like to share you can just use it to overwrite the index.Rmd file. blogdown supports any kind of R Markdown documents so you can use all of your favourite markdown packages (bookdown, pagedown, ‚Ä¶). Just make sure to specify HTML output in the YAML header."
  },
  {
    "objectID": "blog/posts/how-i-m-building-this-website-in-r/index.html#resources",
    "href": "blog/posts/how-i-m-building-this-website-in-r/index.html#resources",
    "title": "How I‚Äôm building this website in R",
    "section": "Resources",
    "text": "Resources\nFor more information about blogdown see here. To inspect the code that builds this website check out my GitHub repository."
  },
  {
    "objectID": "blog/posts/individual-recourse-for-black-box-models/index.html",
    "href": "blog/posts/individual-recourse-for-black-box-models/index.html",
    "title": "Individual recourse for Black Box Models",
    "section": "",
    "text": "In her popular book Weapons of Math Destruction Cathy O‚ÄôNeil presents the example of public school teacher Sarah Wysocki, who lost her job after a teacher evaluation algorithm had rendered her redundant (O‚ÄôNeil 2016). Sarah was highly popular among her peers, supervisors and students.\nThis post looks at a novel algorithmic solution to the problem that individuals like Sarah, who are faced with an undesirable outcome, should be provided with means to revise that outcome. The literature commonly refers to this as individual recourse. One of the first approaches towards individual recourse was proposed by Ustun, Spangher, and Liu (2019). In a recent follow-up paper, Joshi et al. (2019) propose a methodology coined REVISE, which extends the earlier approach in at least three key ways:\nFor a detailed discussion of these points you may check out this slide deck or consult the paper directly (freely available on DeepAI). Here, we will abstract from some of these complications and instead look at an application of a slightly simplified version of REVISE. This should help us to first build a good intuition. Readers interested in the technicalities and code may find all of this in the annex below."
  },
  {
    "objectID": "blog/posts/individual-recourse-for-black-box-models/index.html#from-to",
    "href": "blog/posts/individual-recourse-for-black-box-models/index.html#from-to",
    "title": "Individual recourse for Black Box Models",
    "section": "From üê± to üê∂",
    "text": "From üê± to üê∂\n\n\n\nWe will explain REVISE through a short tale of cats and dogs. The protagonist of this tale is Kitty üê±, a young cat that identifies as a dog. Unfortunately, Kitty is not very tall and her tail, though short for a cat, is longer than that of the average dog (?@fig-density).\n\n\n\n\n\n\nMuch to her dismay, Kitty has been recognized as a cat by a linear classifier \\(g_n(X)\\) that we trained through stochastic gradient descent using the data on animals‚Äô height and tail length. Once again interested readers may find technical details and code in the annex below. ?@fig-class shows the resulting linear separation in the attribute space with the decision boundary in solid black and Kitty‚Äôs location indicated by a red circle. Can we provide individual recourse to Kitty?\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nLet‚Äôs see if and how we can apply REVISE to Kitty‚Äôs problem. The following summary should give you some flavour of how the algorithm works:\n\nInitialize \\(\\mathbf{x}_i'^{(0)}\\), that is the attributes that will be revised recursively. Kitty‚Äôs original attributes seem like a reasonable place to start.\nThrough gradient descent recursively revise \\(\\mathbf{x}_i'^{(t)}\\) until \\(g_n(\\mathbf{x}_i'^{(T)})=\\)üê∂. At this point \\(T\\) the descent terminates since for these revised attributes the classifier labels Kitty as a dog.\nReturn \\(\\delta_i=\\mathbf{x}_i'^{(T)}-\\mathbf{x}_i\\), that is the individual recourse for Kitty.\n\n?@fig-revise illustrates what happens when this approach is applied to Kitty‚Äôs problem. The different panels show the results for different values of a regularization parameter \\(\\lambda\\) that governs the trade-off between achieving the desired label switch and keeping the distance between the original (\\(\\mathbf{x}_i\\)) and revised (\\(\\mathbf{x}_i'\\)) attributes small. In all but one case, REVISE converges: a decrease in tail length along with an increase in height eventually allows Kitty to cross the decision boundary. In other words, we have successfully turned Kitty into a dog - at least in the eyes of the linear classifier!\nWe also observe that as we increase \\(\\lambda\\) for a fixed learning rate, REVISE takes longer to converge. This should come as no surprise, since higher values of \\(\\lambda\\) lead to greater regularization with respect to the penalty we place on the distance that Kitty has to travel. When we penalize too much (\\(\\lambda=10\\)), Kitty never reaches the decision boundary, because she is reluctant to change her characteristics beyond a certain point. While not visible to the naked eye, in this particular example \\(\\lambda=0.001\\) corresponds to the best choice among the candidate values."
  },
  {
    "objectID": "blog/posts/individual-recourse-for-black-box-models/index.html#discussion",
    "href": "blog/posts/individual-recourse-for-black-box-models/index.html#discussion",
    "title": "Individual recourse for Black Box Models",
    "section": "Discussion",
    "text": "Discussion\nWhile hopefully Kitty‚Äôs journey has provided you with some useful intuition, the story is of course very silly. Even if your cat ever seems to signal that she wants to be a dog, helping her cross that decision boundary will be tricky. Some attributes are simply immutable or very difficult to change, which Joshi et al. (2019) do not fail to account for in their framework. Their proposed methodology offers a simple and ingenious approach towards providing individual recourse. Instead of concerning ourselves with Black Box interpretability, why not simply provide remedy in case things go wrong?\nTo some extent that idea has its merit. As this post has hopefully shown, REVISE is straight-forward to understand and readily applicable. It could be a very useful tool to provide individual recourse in many real-world applications. As the implementation of our simplified version of REVISE demonstrates, researchers should also find it relatively easy to develop the methodology further and tailor it to specific use cases. The simpler version here, for example, may be useful in settings where the dimensionality is relatively small and one can reasonably model the distribution of attributes without the need for generative models.\nStill, you may be wondering: if the original classifier is based on poorly defined rules and proxies, then what good does REVISE really do? Going back to the example of high-school teacher Sarah Wysocki, one of the key attributes determining teachers‚Äô evaluations was their students‚Äô performance. Realizing this, some teachers took the shortest route to success by artificially inflating their students‚Äô test scores. That same course of action may well have been suggested by REVISE. As Joshi et al. (2019) demonstrate, this very property of REVISE may actually proof useful in detecting weaknesses of decision making systems before setting them loose (key contribution 3).\nNonetheless, the example above also demonstrates that approaches like REVISE, useful as they may be, tend to provide solutions for very particular problems. In reality data-driven decision making systems are often subject to many different problems and hence research on trustworthy AI will need to tackle the issue from various angles. A few places to start include the question of dealing with data that is inherently biased, improving ad-hoc and post-hoc model interpretability and continuing efforts around causality-inspired AI."
  },
  {
    "objectID": "blog/posts/individual-recourse-for-black-box-models/index.html#references",
    "href": "blog/posts/individual-recourse-for-black-box-models/index.html#references",
    "title": "Individual recourse for Black Box Models",
    "section": "References",
    "text": "References\n\n\nJoshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. ‚ÄúTowards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.‚Äù https://arxiv.org/abs/1907.09615.\n\n\nO‚ÄôNeil, Cathy. 2016. Weapons of Math Destruction: How Big Data Increases Inequality and Threatens Democracy. Crown.\n\n\nUstun, Berk, Alexander Spangher, and Yang Liu. 2019. ‚ÄúActionable Recourse in Linear Classification.‚Äù In Proceedings of the Conference on Fairness, Accountability, and Transparency, 10‚Äì19."
  },
  {
    "objectID": "blog/posts/individual-recourse-for-black-box-models/index.html#annex",
    "href": "blog/posts/individual-recourse-for-black-box-models/index.html#annex",
    "title": "Individual recourse for Black Box Models",
    "section": "Annex",
    "text": "Annex\nIn my blog posts I aim to implement interesting ideas from scratch even if that sometimes means that things need to undergo some sort of simplification. The benefit of this approach is that the experience is educationally rewarding - both for myself and hopefully also for readers. The first two sections of this annex show how REVISE and linear classification can be implemented in R. The final section just shows how the synthetic data was generated. To also inspect the code that generates the visualizations and everything else, you can find the source code of this file on GitHub.\n\nLinear classifier\nLinear classification is implemented through stochastic gradient descent (SGD) with Hinge loss\n\\[\n\\begin{aligned}\n&& \\ell(-\\mathbf{w}^T\\mathbf{x}_i y_i)&=(1-\\mathbf{w}^T\\mathbf{x}_i y_i)_+ \\\\\n\\end{aligned}\n\\]\nwhere \\(\\mathbf{w}\\) is a coefficient vector, \\(\\mathbf{x}_i\\) is the attribute vector of individual \\(i\\) and \\(y_i\\) is the individual‚Äôs outcome. Since we apply SGD in order to minimize the loss function \\(\\ell\\) by varying \\(\\mathbf{w}\\), we need an expression for its gradient with respect to \\(\\mathbf{w}\\), which is given by:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\nabla_{\\mathbf{W}} \\left( \\ell(-\\mathbf{w}^T\\mathbf{x}_i y_i) \\right) &= \\begin{cases} -\\mathbf{x}_i y_i & \\text{if} \\ \\ \\ \\mathbf{w}^T\\mathbf{x}_i y_i \\le 1\\\\ 0 & \\text{otherwise} \\end{cases} \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{1}\\]\nThe code below uses this analytical solution to perform SGD over \\(T\\) iterations or as long as updates yield feasible parameter values. As the final vector of coefficients the function returns \\(\\mathbf{\\bar{w}}= \\frac{1}{T} \\sum_{t=1}^{T} \\mathbf{w}_t\\). Denoting the optimal coefficient vector as \\(\\mathbf{w}^*\\), it can be shown that under certain conditions \\(\\ell(\\mathbf{\\bar{w}})\\rightarrow\\ell(\\mathbf{w}^*)\\) as \\(T\\rightarrow\\infty\\).\n\n#' Stochastic gradient descent\n#'\n#' @param X Feature matrix.\n#' @param y Vector containing training labels.\n#' @param eta Learning rate.\n#' @param n_iter Maximum number of iterations.\n#' @param w_init Initial parameter values.\n#' @param save_steps Boolean checking if coefficients should be saved at each step.\n#'\n#' @return\n#' @export\n#'\n#' @author Patrick Altmeyer\nlinear_classifier <- function(X,y,eta=0.001,n_iter=1000,w_init=NULL,save_steps=FALSE) {\n  # Initialization: ----\n  n <- nrow(X) # number of observations\n  d <- ncol(X) # number of dimensions\n  if (is.null(w_init)) {\n    w <- matrix(rep(0,d)) # initialize coefficients as zero...\n  } else {\n    w <- matrix(w_init) # ...unless initial values have been provided.\n  }\n  w_avg <- 1/n_iter * w # initialize average coefficients\n  iter <- 1 # iteration count\n  if (save_steps) {\n    steps <- data.table(iter=0, w=c(w), d=1:d) # if desired, save coefficient at each step\n  } else {\n    steps <- NA\n  }\n  feasible_w <- TRUE # to check if coefficients are finite, non-nan, ...\n  # Surrogate loss:\n  l <- function(X,y,w) {\n    x <- (-1) * crossprod(X,w) * y\n    pmax(0,1 + x) # Hinge loss\n  }\n  grad <- function(X,y,w) {\n    X %*% ifelse(crossprod(X,w) * y<=1,-y,0) # Gradient of Hinge loss\n  }\n  # Stochastic gradient descent: ----\n  while (feasible_w & iter<n_iter) {\n    t <- sample(1:n,1) # random draw\n    X_t <- matrix(X[t,])\n    y_t <- matrix(y[t])\n    v_t <- grad(X_t,y_t,w) # compute estimate of gradient\n    # Update:\n    w <- w - eta * v_t # update coefficient vector\n    feasible_w <- all(sapply(w, function(i) !is.na(i) & is.finite(i))) # check if feasible\n    if (feasible_w) {\n      w_avg <- w_avg + 1/n_iter * w # update average\n    }\n    if (save_steps) {\n      steps <- rbind(steps, data.table(iter=iter, w=c(w), d=1:d))\n    }\n    iter <- iter + 1 # increase counter\n  }\n  # Output: ----\n  output <- list(\n    X = X,\n    y = matrix(y),\n    coefficients = w_avg,\n    eta = eta,\n    n_iter = n_iter,\n    steps = steps\n  )\n  class(output) <- \"classifier\" # assign S3 class\n  return(output)\n}\n\n# Methods: ----\nprint.classifier <- function(classifier) {\n  print(\"Coefficients:\")\n  print(classifier$coefficients)\n}\nprint <- function(classifier) {\n  UseMethod(\"print\")\n}\n\npredict.classifier <- function(classifier, newdata=NULL, discrete=TRUE) {\n  if (!is.null(newdata)) {\n    fitted <- newdata %*% classifier$coefficients # out-of-sampple prediction\n  } else {\n    fitted <- classifier$X %*% classifier$coefficients # in-sample fit\n  }\n  if (discrete) {\n    fitted <- sign(fitted) # map to {-1,1}\n  }\n  return(fitted)\n}\npredict <- function(classifier, newdata=NULL, discrete=TRUE) {\n  UseMethod(\"predict\")\n}\n\n\n\nREVISE (simplified)\nAs flagged above, we are looking at a slightly simplified version of the algorithm presented in Joshi et al. (2019). In particular, the approach here does not incorporate the threshold on the likelihood nor does it account for immutable attributes.\nLet \\(y\\in\\{-1,1\\}\\) be a binary outcome variable, \\(X\\in\\mathbb{R}^d\\) a feature matrix containing individuals‚Äô attributes and \\(g_n(X)\\) a corresponding data-dependent classifier. Suppose \\(y_i=-1\\) (the negative outcome) for some individual characterized by attributes \\(\\mathbf{x}_i\\). Then we want to find \\(\\mathbf{x}_i'\\) closest to \\(\\mathbf{x}_i\\) such that the classifier assigns the positive outcome \\(g(\\mathbf{x}_i^{'})=1\\). In order to do so, we use gradient descent with Hinge loss \\(\\ell\\) to minimize the following function\n\\[\n\\begin{aligned}\n&& \\min_{\\mathbf{x}_i^{'}}& \\ \\ell(g_n(\\mathbf{x}_i^{'}),1) + \\lambda d(\\mathbf{x}_i^{'},\\mathbf{x}_i) \\\\\n\\end{aligned}\n\\]\nwhere \\(d=||\\mathbf{x}_i^{'}-\\mathbf{x}_i||\\) denotes the Euclidean distance. Note that this time we take the coefficient vector defining \\(g_n\\) as given and instead vary the attributes. In particular, we will perform gradient descent steps as follows\n\\[\n\\begin{aligned}\n&& {\\mathbf{x}_i^{'}}^t&\\leftarrow {\\mathbf{x}_i^{'}}^{t-1} + \\eta \\nabla_{{\\mathbf{x}_i^{'}}} \\left( \\ell(g_n(\\mathbf{x}_i^{'}),1) + \\lambda d(\\mathbf{x}_i^{'},\\mathbf{x}_i)  \\right)  \\\\\n\\end{aligned}\n\\]\nwhere \\(\\eta\\) is the learning rate. The descent step is almost equivalent to the one described in Joshi et al. (2019), but here we greatly simplify things by optimizing directly in the attribute space instead of a latent space. The gradient of the loss function looks very similar to Equation¬†1. With respect to the Euclidean distance partial derivatives are of the following form:\n\\[\n\\begin{aligned}\n&&  \\frac{\\partial ||\\mathbf{x}_i^{'}-\\mathbf{x}_i||}{\\partial {x_i'}^{(d)}}  &= \\frac{{x_i'}^{(d)}-{x_i}^{(d)}}{||\\mathbf{x}_i^{'}-\\mathbf{x}_i||} \\\\\n\\end{aligned}\n\\]\nThe code that implements this optimization follows below.\n\n#' REVISE algoritm - a simplified version\n#'\n#' @param classifier The fitted classifier.\n#' @param x_star Attributes of individual seeking individual recourse.\n#' @param eta Learning rate.\n#' @param lambda Regularization parameter.\n#' @param n_iter Maximum number of operations.\n#' @param save_steps Boolean indicating if intermediate steps should be saved.\n#'\n#' @return\n#' @export\n#'\n#' @author Patrick Altmeyer\nrevise.classifier <- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {\n  # Initialization: ----\n  d <- length(x_star) # number of dimensions\n  if (!is.null(names(x_star))) {\n    d_names <- names(x_star) # names of attributes, if provided\n  } else {\n    d_names <- sprintf(\"X%i\", 1:d)\n  }\n  w <- classifier$coefficients # coefficient vector\n  x <- x_star # initialization of revised attributes\n  distance <- 0 # initial distance from starting point\n  converged <- predict(classifier, newdata = x)[1,1]==1 # positive outcome?\n  iter <- 1 # counter\n  if (save_steps) {\n    steps <- data.table(iter=1, x=x, d=d_names) # save intermediate steps, if desired\n  } else {\n    steps <- NA\n  }\n  # Gradients:\n  grad <- function(x,y,w) {\n    w %*% ifelse(crossprod(x,w) * y<=1,-y,0) # gradient of Hinge loss with respect to X\n  }\n  grad_dist <- function(x,x_star) {\n    d <- length(x_star)\n    distance <- dist(matrix(cbind(x_star,x),nrow=d,byrow = T))\n    matrix((x-x_star) / distance) # gradient of Euclidean distance with respect to X\n  }\n  # Gradient descent: ----\n  while(!converged & iter<n_iter) {\n    if (distance!=0) {\n      x <- c(x - eta * (grad(x=matrix(x),y=1,w) + lambda * grad_dist(x,x_star))) # gradient descent step\n    } else {\n      x <- c(x - eta * grad(x=matrix(x),y=1,w)) # gradient with respect to distance not defined at zero\n    }\n    converged <- predict(classifier, newdata = x)[1,1]==1 # positive outcome?\n    iter <- iter + 1 # update counter\n    if (save_steps) {\n      steps <- rbind(steps, data.table(iter=iter, x=x, d=d_names))\n    }\n    distance <- dist(matrix(cbind(x_star,x),nrow=d,byrow = T)) # update distance\n  }\n  # Output: ----\n  if (converged) {\n    revise <- x - x_star\n  } else {\n    revise <- NA\n  }\n  output <- list(\n    x_star = x_star,\n    revise = revise,\n    classifier = classifier,\n    steps = steps,\n    lambda = lambda,\n    distance = distance,\n    mean_distance = mean(abs(revise))\n  )\n  return(output)\n}\n\nrevise <- function(classifier,x_star,eta=1,lambda=0.01,n_iter=1000,save_steps=FALSE) {\n  UseMethod(\"revise\")\n}\n\n\n\nSimulated data\nThe synthetic data describing cats and dogs was generated as follows:\n\nsim_data <- function(n=100,averages,noise=0.1) {\n  d <- ncol(averages)\n  y <- 2*(rbinom(n,1,0.5)-0.5) # generate binary outcome: 1=dog, -1=cat\n  X <- as.matrix(averages[(y+1)/2+1,]) # generate attributes conditional on y\n  dogs <- y==1 # boolean index for dogs\n  cats <- y==-1 # boolean index for cats\n  X[cats,] <- X[cats,] + \n    matrix(rnorm(sum(cats)*d),nrow=sum(cats)) %*% diag(noise*averages[2,]) # add noise for y=1 (cats)\n  X[dogs,] <- X[dogs,] + \n    matrix(rnorm(sum(dogs)*d),nrow=sum(dogs)) %*% diag(noise*averages[2,]) # add noise for y=1 (dogs)\n  return(list(X=X,y=y))\n}"
  },
  {
    "objectID": "blog/posts/conformal-prediction/index.html",
    "href": "blog/posts/conformal-prediction/index.html",
    "title": "Conformal Prediction in Julia üü£üî¥üü¢",
    "section": "",
    "text": "Prediction sets for two different samples  and changing coverage rates.  As coverage grows, so does the size of the  prediction sets.\nA first crucial step towards building trustworthy AI systems is to be transparent about predictive uncertainty. Model parameters are random variables and their values are estimated from noisy data. That inherent stochasticity feeds through to model predictions and should to be addressed, at the very least in order to avoid overconfidence in models.\nBeyond that obvious concern, it turns out that quantifying model uncertainty actually opens up a myriad of possibilities to improve up- and down-stream modeling tasks like active learning and robustness. In Bayesian Active Learning, for example, uncertainty estimates are used to guide the search for new input samples, which can make ground-truthing tasks more efficient (Houlsby et al. 2011). With respect to model performance in downstream tasks, uncertainty quantification can be used to improve model calibration and robustness (Lakshminarayanan, Pritzel, and Blundell 2016).\nIn previous posts we have looked at how uncertainty can be quantified in the Bayesian context (see here and here). Since in Bayesian modeling we are generally concerned with estimating posterior distributions, we get uncertainty estimates almost as a byproduct. This is great for all intends and purposes, but it hinges on assumptions about prior distributions. Personally, I have no quarrel with the idea of making prior distributional assumptions. On the contrary, I think the Bayesian framework formalizes the idea of integrating prior information in models and therefore provides a powerful toolkit for conducting science. Still, in some cases this requirement may be seen as too restrictive or we may simply lack prior information.\nEnter: Conformal Prediction (CP) ‚Äî a scalable frequentist approach to uncertainty quantification and coverage control. In this post we will go through the basic concepts underlying CP. A number of hands-on usage examples in Julia should hopefully help to convey some intuition and ideally attract people interested in contributing to a new and exciting open-source development."
  },
  {
    "objectID": "blog/posts/conformal-prediction/index.html#sec-background",
    "href": "blog/posts/conformal-prediction/index.html#sec-background",
    "title": "Conformal Prediction in Julia üü£üî¥üü¢",
    "section": "üìñ Background",
    "text": "üìñ Background\nConformal Prediction promises to be an easy-to-understand, distribution-free and model-agnostic way to generate statistically rigorous uncertainty estimates. That‚Äôs quite a mouthful, so let‚Äôs break it down: firstly, as I will hopefully manage to illustrate in this post, the underlying concepts truly are fairly straight-forward to understand; secondly, CP indeed relies on only minimal distributional assumptions; thirdly, common procedures to generate conformal predictions really do apply almost universally to all supervised models, therefore making the framework very intriguing to the ML community; and, finally, CP does in fact come with a frequentist coverage guarantee that ensures that conformal prediction sets contain the true value with a user-chosen probability. For a formal proof of this marginal coverage property and a detailed introduction to the topic, I recommend Angelopoulos and Bates (2021).\n\n\n\n\n\n\nNote\n\n\n\nIn what follows we will loosely treat the tutorial by Angelopoulos and Bates (2021) and the general framework it sets as a reference. You are not expected to have read the paper, but I also won‚Äôt reiterate any details here.\n\n\nCP can be used to generate prediction intervals for regression models and prediction sets for classification models (more on this later). There is also some recent work on conformal predictive distributions and probabilistic predictions. Interestingly, it can even be used to complement Bayesian methods. Angelopoulos and Bates (2021), for example, point out that prior information should be incorporated into prediction sets and demonstrate how Bayesian predictive distributions can be conformalized in order to comply with the frequentist notion of coverage. Relatedly, Hoff (2021) proposes a Bayes-optimal prediction procedure. And finally, Stanton, Maddox, and Wilson (2022) very recently proposed a way to introduce conformal prediction in Bayesian Optimization. I find this type of work that combines different schools of thought very promising, but I‚Äôm drifting off a little ‚Ä¶ So, without further ado, let us look at some code."
  },
  {
    "objectID": "blog/posts/conformal-prediction/index.html#sec-julia",
    "href": "blog/posts/conformal-prediction/index.html#sec-julia",
    "title": "Conformal Prediction in Julia üü£üî¥üü¢",
    "section": "üì¶ Conformal Prediction in Julia",
    "text": "üì¶ Conformal Prediction in Julia\nIn this section of this first short post on CP we will look at how conformal prediction can be implemented in Julia. In particular, we will look at an approach that is compatible with any of the many supervised machine learning models available in MLJ: a beautiful, comprehensive machine learning framework funded by the Alan Turing Institute and the New Zealand Strategic Science Investment Fund Blaom et al. (2020). We will go through some basic usage examples employing a new Julia package that I have been working on: ConformalPrediction.jl.\n\n\n\n\n\n\nConformalPrediction.jl\n\n\n\nConformalPrediction.jl is a package for uncertainty quantification through conformal prediction for machine learning models trained in MLJ. At the time of writing it is still in its early stages of development, but already implements a range of different approaches to CP. Contributions are very much welcome:\n\nDocumentation\nContributor‚Äôs Guide\n\n\n\n\nSplit Conformal Classification\nWe consider a simple binary classification problem. Let \\((X_i, Y_i), \\ i=1,...,n\\) denote our feature-label pairs and let \\(\\mu: \\mathcal{X} \\mapsto \\mathcal{Y}\\) denote the mapping from features to labels. For illustration purposes we will use the moons dataset üåô. Using MLJ.jl we first generate the data and split into into a training and test set:\n\nusing MLJ\nusing Random\nRandom.seed!(123)\n\n# Data:\nX, y = make_moons(500; noise=0.15)\ntrain, test = partition(eachindex(y), 0.8, shuffle=true)\n\nHere we will use a specific case of CP called split conformal prediction which can then be summarized as follows:1\n\nPartition the training into a proper training set and a separate calibration set: \\(\\mathcal{D}_n=\\mathcal{D}^{\\text{train}} \\cup \\mathcal{D}^{\\text{cali}}\\).\nTrain the machine learning model on the proper training set: \\(\\hat\\mu_{i \\in \\mathcal{D}^{\\text{train}}}(X_i,Y_i)\\).\nCompute nonconformity scores, \\(\\mathcal{S}\\), using the calibration data \\(\\mathcal{D}^{\\text{cali}}\\) and the fitted model \\(\\hat\\mu_{i \\in \\mathcal{D}^{\\text{train}}}\\).\nFor a user-specified desired coverage ratio \\((1-\\alpha)\\) compute the corresponding quantile, \\(\\hat{q}\\), of the empirical distribution of nonconformity scores, \\(\\mathcal{S}\\).\nFor the given quantile and test sample \\(X_{\\text{test}}\\), form the corresponding conformal prediction set:\n\n\\[\nC(X_{\\text{test}})=\\{y:s(X_{\\text{test}},y) \\le \\hat{q}\\}\n\\tag{1}\\]\nThis is the default procedure used for classification and regression in ConformalPrediction.jl.\nYou may want to take a look at the source code for the classification case here. As a first important step, we begin by defining a concrete type SimpleInductiveClassifier that wraps a supervised model from MLJ.jl and reserves additional fields for a few hyperparameters. As a second step, we define the training procedure, which includes the data-splitting and calibration step. Finally, as a third step we implement the procedure in Equation¬†1 to compute the conformal prediction set.\n\n\n\n\n\n\nDevelopment Status\n\n\n\nThe permalinks above take you to the version of the package that was up-to-date at the time of writing. Since the package is in its early stages of development, the code base and API can be expected to change.\n\n\nNow let‚Äôs take this to our üåô data. To illustrate the package functionality we will demonstrate the envisioned workflow. We first define our atomic machine learning model following standard MLJ.jl conventions. Using ConformalPrediction.jl we then wrap our atomic model in a conformal model using the standard API call conformal_model(model::Supervised; kwargs...). To train and predict from our conformal model we can then rely on the conventional MLJ.jl procedure again. In particular, we wrap our conformal model in data (turning it into a machine) and then fit it on the training set. Finally, we use our machine to predict the label for a new test sample Xtest:\n\n# Model:\nKNNClassifier = @load KNNClassifier pkg=NearestNeighborModels\nmodel = KNNClassifier(;K=50) \n\n# Training:\nusing ConformalPrediction\nconf_model = conformal_model(model; coverage=.9)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = selectrows(X, first(test))\nytest = y[first(test)]\npredict(mach, Xtest)[1]\n\nimport NearestNeighborModels ‚úî\n\n\n\n\n\n\n           UnivariateFinite{Multiclass{2}}      \n     ‚îå                                        ‚îê \n   0 ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.94   \n     ‚îî                                        ‚îò \n\n\n\nThe final predictions are set-valued. While the softmax output remains unchanged for the SimpleInductiveClassifier, the size of the prediction set depends on the chosen coverage rate, \\((1-\\alpha)\\).\n\n\nWhen specifying a coverage rate very close to one, the prediction set will typically include many (in some cases all) of the possible labels. Below, for example, both classes are included in the prediction set when setting the coverage rate equal to \\((1-\\alpha)\\)=1.0. This is intuitive, since high coverage quite literally requires that the true label is covered by the prediction set with high probability.\n\n\n\nconf_model = conformal_model(model; coverage=coverage)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\nXtest = (x1=[1],x2=[0])\npredict(mach, Xtest)[1]\n\n\n           UnivariateFinite{Multiclass{2}}      \n     ‚îå                                        ‚îê \n   0 ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.5   \n   1 ‚î§‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ†‚ñ† 0.5   \n     ‚îî                                        ‚îò \n\n\n\n\n\nConversely, for low coverage rates, prediction sets can also be empty. For a choice of \\((1-\\alpha)\\)=0.1, for example, the prediction set for our test sample is empty. This is a bit difficult to think about intuitively and I have not yet come across a satisfactory, intuitive interpretation.2 When the prediction set is empty, the predict call currently returns missing:\n\n\n\nconf_model = conformal_model(model; coverage=coverage)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\n\n# Conformal Prediction:\npredict(mach, Xtest)[1]\n\nmissing\n\n\nFigure¬†1 should provide some more intuition as to what exactly is happening here. It illustrates the effect of the chosen coverage rate on the predicted softmax output and the set size in the two-dimensional feature space. Contours are overlayed with the moon data points (including test data). The two samples highlighted in red, \\(X_1\\) and \\(X_2\\), have been manually added for illustration purposes. Let‚Äôs look at these one by one.\nFirstly, note that \\(X_1\\) (red cross) falls into a region of the domain that is characterized by high predictive uncertainty. It sits right at the bottom-right corner of our class-zero moon üåú (orange), a region that is almost entirely enveloped by our class-one moon üåõ (green). For low coverage rates the prediction set for \\(X_1\\) is empty: on the left-hand side this is indicated by the missing contour for the softmax probability; on the right-hand side we can observe that the corresponding set size is indeed zero. For high coverage rates the prediction set includes both \\(y=0\\) and \\(y=1\\), indicative of the fact that the conformal classifier is uncertain about the true label.\nWith respect to \\(X_2\\), we observe that while also sitting on the fringe of our class-zero moon, this sample populates a region that is not fully enveloped by data points from the opposite class. In this region, the underlying atomic classifier can be expected to be more certain about its predictions, but still not highly confident. How is this reflected by our corresponding conformal prediction sets?\n\n\nCode\nXtest_2 = (x1=[-0.5],x2=[0.25])\ncov_ = .9\nconf_model = conformal_model(model; coverage=cov_)\nmach = machine(conf_model, X, y)\nfit!(mach, rows=train)\npÃÇ_2 = pdf(predict(mach, Xtest_2)[1], 0)\n\n\n\n\nWell, for low coverage rates (roughly \\(<0.9\\)) the conformal prediction set does not include \\(y=0\\): the set size is zero (right panel). Only for higher coverage rates do we have \\(C(X_2)=\\{0\\}\\): the coverage rate is high enough to include \\(y=0\\), but the corresponding softmax probability is still fairly low. For example, for \\((1-\\alpha)=0.9\\) we have \\(\\hat{p}(y=0|X_2)=0.72.\\)\n\n\nThese two examples illustrate an interesting point: for regions characterised by high predictive uncertainty, conformal prediction sets are typically empty (for low coverage) or large (for high coverage). While set-valued predictions may be something to get used to, this notion is overall intuitive.\n\n\nCode\n# Setup\ncoverages = range(0.75,1.0,length=5)\nn = 100\nx1_range = range(extrema(X.x1)...,length=n)\nx2_range = range(extrema(X.x2)...,length=n)\n\nanim = @animate for coverage in coverages\n    conf_model = conformal_model(model; coverage=coverage)\n    mach = machine(conf_model, X, y)\n    fit!(mach, rows=train)\n    p1 = contourf_cp(mach, x1_range, x2_range; type=:proba, title=\"Softmax\", axis=nothing)\n    scatter!(p1, X.x1, X.x2, group=y, ms=2, msw=0, alpha=0.75)\n    scatter!(p1, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"X‚ÇÅ\", shape=:cross, msw=6)\n    scatter!(p1, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"X‚ÇÇ\", shape=:diamond, msw=6)\n    p2 = contourf_cp(mach, x1_range, x2_range; type=:set_size, title=\"Set size\", axis=nothing)\n    scatter!(p2, X.x1, X.x2, group=y, ms=2, msw=0, alpha=0.75)\n    scatter!(p2, Xtest.x1, Xtest.x2, ms=6, c=:red, label=\"X‚ÇÅ\", shape=:cross, msw=6)\n    scatter!(p2, Xtest_2.x1, Xtest_2.x2, ms=6, c=:red, label=\"X‚ÇÇ\", shape=:diamond, msw=6)\n    plot(p1, p2, plot_title=\"(1-Œ±)=$(round(coverage,digits=2))\", size=(800,300))\nend\n\ngif(anim, fps=0.5)\n\n\n\n\nFigure¬†1: The effect of the coverage rate on the conformal prediction set. Softmax probabilities are shown on the left. The size of the prediction set is shown on the right."
  },
  {
    "objectID": "blog/posts/conformal-prediction/index.html#conclusion",
    "href": "blog/posts/conformal-prediction/index.html#conclusion",
    "title": "Conformal Prediction in Julia üü£üî¥üü¢",
    "section": "üèÅ Conclusion",
    "text": "üèÅ Conclusion\nThis has really been a whistle-stop tour of Conformal Prediction: an active area of research that probably deserves much more attention. Hopefully, though, this post has helped to provide some color and, if anything, made you more curious about the topic. Let‚Äôs recap the TL;DR from above:\n\nConformal Prediction is an interesting frequentist approach to uncertainty quantification that can even be combined with Bayes (Section¬†1).\nIt is scalable and model-agnostic and therefore well applicable to machine learning (Section¬†1).\nConformalPrediction.jl implements CP in pure Julia and can be used with any supervised model available from MLJ.jl (Section¬†2).\nImplementing CP directly on top of an existing, powerful machine learning toolkit demonstrates the potential usefulness of this framework to the ML community (Section¬†2).\nStandard conformal classifiers produce set-valued predictions: for ambiguous samples these sets are typically large (for high coverage) or empty (for low coverage) (Section¬†2.1).\n\nBelow I will leave you with some further resources."
  },
  {
    "objectID": "blog/posts/conformal-prediction/index.html#further-resources",
    "href": "blog/posts/conformal-prediction/index.html#further-resources",
    "title": "Conformal Prediction in Julia üü£üî¥üü¢",
    "section": "üìö Further Resources",
    "text": "üìö Further Resources\nChances are that you have already come across the Awesome Conformal Prediction repo: Manokhin (n.d.) provides a comprehensive, up-to-date overview of resources related to the conformal prediction. Among the listed articles you will also find Angelopoulos and Bates (2021), which inspired much of this post. The repo also points to open-source implementations in other popular programming languages including Python and R."
  },
  {
    "objectID": "blog/posts/welcome/index.html",
    "href": "blog/posts/welcome/index.html",
    "title": "Welcome",
    "section": "",
    "text": "Welcome to my blog!\nHaving worked with R Markdown and some of Yihui Xie‚Äôs amazing packages for years, I have only now come across his blogdown package. For a while I have been thinking about a good way to share some of my work and actually started collecting snippets in a Gitbook through bookdown quite some time ago. While the book is a work-in-progress that I aim to finish eventually, I will use this website to regularly share content related to my work, research and other things.\n\n\n\n\n\n\nNote\n\n\n\nUpdate on Feb 20, 2022\nI have recently migrated this blog and pretty much everything else I do to quarto.\n\nQuarto¬Æ is an open-source scientific and technical publishing system built on Pandoc.\n\nBased on my first few experiences I would go further and say that quarto is the only open-source scientific and technical publishing system you‚Äôll ever need. The project is supported by RStudio and (unsurprisingly) Yihui Xie is one of the contributors. Go check it out!\n\n\n\n\n\nCitationBibTeX citation:@online{altmeyer2021,\n  author = {Patrick Altmeyer and Patrick Altmeyer},\n  title = {Welcome},\n  date = {2021-02-01},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrick Altmeyer, and Patrick Altmeyer. 2021. ‚ÄúWelcome.‚Äù\nFebruary 1, 2021."
  },
  {
    "objectID": "blog/posts/a-new-tool-for-explainable-ai/index.html",
    "href": "blog/posts/a-new-tool-for-explainable-ai/index.html",
    "title": "A new tool for explainable AI",
    "section": "",
    "text": "Turning a 9 (nine) into a 4 (four).\nCounterfactual explanations, which I introduced in one of my previous posts1, offer a simple and intuitive way to explain black-box models without opening them. Still, as of today there exists only one open-source library that provides a unifying approach to generate and benchmark counterfactual explanations for models built and trained in Python (Pawelczyk et al. 2021). This is great, but of limited use to users of other programming languages ü•≤.\nEnter CounterfactualExplanations.jl: a Julia package that can be used to explain machine learning algorithms developed and trained in Julia, Python and R. Counterfactual explanations fall into the broader category of explainable artificial intelligence (XAI).\nExplainable AI typically involves models that are not inherently interpretable but require additional tools to be explainable to humans. Examples of the latter include ensembles, support vector machines and deep neural networks. This is not to be confused with interpretable AI, which involves models that are inherently interpretable and transparent such as general additive models (GAM), decision trees and rule-based models.\nSome would argue that we best avoid explaining black-box models altogether (Rudin 2019) and instead focus solely on interpretable AI. While I agree that initial efforts should always be geared towards interpretable models, stopping there would entail missed opportunities and anyway is probably not very realistic in times of DALL\\(\\cdot\\)E and Co.\nThis post introduces the main functionality of the new Julia package. Following a motivating example using a model trained in Julia, we will see how easy the package can be adapted to work with models trained in Python and R. Since the motivation for this post is also to hopefully attract contributors, the final section outlines some of the exciting developments we have planned."
  },
  {
    "objectID": "blog/posts/a-new-tool-for-explainable-ai/index.html#counterfactuals-for-image-data",
    "href": "blog/posts/a-new-tool-for-explainable-ai/index.html#counterfactuals-for-image-data",
    "title": "A new tool for explainable AI",
    "section": "Counterfactuals for image data üñº",
    "text": "Counterfactuals for image data üñº\nTo introduce counterfactual explanations I used a simple binary classification problem in my previous post. It involved a linear classifier and a linearly separable, synthetic data set with just two features. This time we are going to step it up a notch: we will generate counterfactual explanations MNIST data. The MNIST dataset contains 60,000 training samples of handwritten digits in the form of 28x28 pixel grey-scale images (LeCun 1998). Each image is associated with a label indicating the digit (0-9) that the image represents.\nThe CounterfactualExplanations.jl package ships with two black-box models that were trained to predict labels for this data: firstly, a simple multi-layer perceptron (MLP) and, secondly, a corresponding deep ensemble. Originally proposed by Lakshminarayanan, Pritzel, and Blundell (2016), deep ensembles are really just ensembles of deep neural networks. They are still among the most popular approaches to Bayesian deep learning.2\n\nBlack-box models\nThe code below loads relevant packages along with the MNIST data and pre-trained models.\n\n# Load package, models and data:\nusing CounterfactualExplanations, Flux\nusing CounterfactualExplanations.Data: mnist_data, mnist_model, mnist_ensemble\ndata, X, ys = mnist_data()\nmodel = mnist_model()\nensemble = mnist_ensemble()\ncounterfactual_data = CounterfactualData(X,ys;domain=(0,1))\n\nWhile the package can currently handle a few simple classification models natively, it is designed to be easily extensible through users and contributors. Extending the package to deal with custom models typically involves only two simple steps:\n\nSubtyping: the custom model needs to be declared as a subtype of the package-internal type AbstractFittedModel.\nMultiple dispatch: the package-internal functions logits and probs need to be extended through custom methods for the new model type.\n\nThe following code implements these two steps first for the MLP and then for the deep ensemble.\n\nusing CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs\n# MLP:\n# Step 1)\nstruct NeuralNetwork <: Models.AbstractFittedModel\n    model::Any\nend\n# Step 2)\nlogits(M::NeuralNetwork, X::AbstractArray) = M.model(X)\nprobs(M::NeuralNetwork, X::AbstractArray)= softmax(logits(M, X))\nM = NeuralNetwork(model)\n\n# Deep ensemble:\nusing Flux: stack\n# Step 1)\nstruct FittedEnsemble <: Models.AbstractFittedModel\n    ensemble::AbstractArray\nend\n# Step 2)\nusing Statistics\nlogits(M::FittedEnsemble, X::AbstractArray) = mean(stack([m(X) for m in M.ensemble],3),dims=3)\nprobs(M::FittedEnsemble, X::AbstractArray) = mean(stack([softmax(m(X)) for m in M.ensemble],3),dims=3)\nM_ensemble = FittedEnsemble(ensemble)\n\n\n\nCounterfactual generators\nNext, we need to specify the counterfactual generators we want to use. The package currently ships with two default generators that both need gradient access: firstly, the generic generator introduced by Wachter, Mittelstadt, and Russell (2017) and, secondly, a greedy generator introduced by Schut et al. (2021).\nThe greedy generator is designed to be used with models that incorporate uncertainty in their predictions such as the deep ensemble introduced above. It works for probabilistic (Bayesian) models, because they only produce high-confidence predictions in regions of the feature domain that are populated by training samples. As long as the model is expressive enough and well-specified, counterfactuals in these regions will always be realistic and unambiguous since by construction they should look very similar to training samples. Other popular approaches to counterfactual explanations like REVISE (Joshi et al. 2019) and CLUE (Antor√°n et al. 2020) also play with this simple idea.\nThe following code instantiates the two generators for the problem at hand.\n\ngeneric = GenericGenerator(;loss=:logitcrossentropy)\ngreedy = GreedyGenerator(;loss=:logitcrossentropy)\n\n\n\nExplanations\nOnce the model and counterfactual generator are specified, running counterfactual search is very easy using the package. For a given factual (x), target class (target) and data set (counterfactual_data), simply running\n\ngenerate_counterfactual(x, target, counterfactual_data, M, generic)\n\nwill generate the results, in this case using the generic generator (generic) for the MLP (M). Since we have specified two different black-box models and two different counterfactual generators, we have four combinations of a model and a generator in total. For each of these combinations I have used the generate_counterfactual function to produce the results in Figure¬†1.\nIn every case the desired label switch is in fact achieved, but arguably from a human perspective only the counterfactuals for the deep ensemble look like a four. The generic generator produces mild perturbations in regions that seem irrelevant from a human perspective, but nonetheless yields a counterfactual that can pass as a four. The greedy approach clearly targets pixels at the top of the handwritten nine and yields the best result overall. For the non-Bayesian MLP, both the generic and the greedy approach generate counterfactuals that look much like adversarial examples: they perturb pixels in seemingly random regions on the image.\n\n\n\nFigure¬†1: Counterfactual explanations for MNIST: turning a nine (9) into a four (4)."
  },
  {
    "objectID": "blog/posts/a-new-tool-for-explainable-ai/index.html#language-interoperability",
    "href": "blog/posts/a-new-tool-for-explainable-ai/index.html#language-interoperability",
    "title": "A new tool for explainable AI",
    "section": "Language interoperability üë•",
    "text": "Language interoperability üë•\nThe Julia language offers unique support for programming language interoperability. For example, calling R or Python is made remarkably easy through RCall.jl and PyCall.jl, respectively. This functionality can be leveraged to use CounterfactualExplanations.jl to generate explanations for models that were developed in other programming languages. At this time there is no native support for foreign programming languages, but the following example involving a torch neural network trained in R demonstrates how versatile the package is.3\n\nExplaining a torch model\nWe will consider a simple MLP trained for a binary classification task. As before we first need to adapt this custom model for use with our package. The code below the two necessary steps - sub-typing and method extension. Logits are returned by the torch model and copied from the R environment into the Julia scope. Probabilities are then computed inside the Julia scope by passing the logits through the sigmoid function.\n\nusing Flux\nusing CounterfactualExplanations, CounterfactualExplanations.Models\nimport CounterfactualExplanations.Models: logits, probs # import functions in order to extend\n\n# Step 1)\nstruct TorchNetwork <: Models.AbstractFittedModel\n    nn::Any\nend\n\n# Step 2)\nfunction logits(M::TorchNetwork, X::AbstractArray)\n  nn = M.nn\n  y = rcopy(R\"as_array($nn(torch_tensor(t($X))))\")\n  y = isa(y, AbstractArray) ? y : [y]\n  return y'\nend\nfunction probs(M::TorchNetwork, X::AbstractArray)\n  return œÉ.(logits(M, X))\nend\nM = TorchNetwork(R\"model\")\n\nCompared to models trained in Julia, we need to do a little more work at this point. Since our counterfactual generators need gradient access, we essentially need to allow our package to communicate with the R torch library. While this may sound daunting, it turns out to be quite manageable: all we have to do is respecify the function that computes the gradient with respect to the counterfactual loss function so that it can deal with the TorchNetwork type we defined above. That is all the adjustment needed to use CounterfactualExplanations.jl for our custom R model. Figure¬†2 shows a counterfactual path for a randomly chosen sample with respect to the MLP trained in R.\n\n\n\n\n\n\nExperimental functionality\n\n\n\nYou may have stumbled across the term respecify above: does it really seem like a good idea to just replace an existing function from our package? Surely not! There are certainly better ways to go about this, which we will consider when adding native support for Python and R models in future package releases. Which brings us to our final section ‚Ä¶\n\n\n\nimport CounterfactualExplanations.Generators: ‚àÇ‚Ñì\nusing LinearAlgebra\n\n# Countefactual loss:\nfunction ‚àÇ‚Ñì(\n    generator::AbstractGradientBasedGenerator, \n    counterfactual_state::CounterfactualState) \n  M = counterfactual_state.M\n  nn = M.nn\n  x‚Ä≤ = counterfactual_state.x‚Ä≤\n  t = counterfactual_state.target_encoded\n  R\"\"\"\n  x <- torch_tensor($x‚Ä≤, requires_grad=TRUE)\n  output <- $nn(x)\n  loss_fun <- nnf_binary_cross_entropy_with_logits\n  obj_loss <- loss_fun(output,$t)\n  obj_loss$backward()\n  \"\"\"\n  grad = rcopy(R\"as_array(x$grad)\")\n  return grad\nend\n\n\n\n\nFigure¬†2: Counterfactual path using the generic counterfactual generator for a model trained in R."
  },
  {
    "objectID": "blog/posts/a-new-tool-for-explainable-ai/index.html#we-need-you",
    "href": "blog/posts/a-new-tool-for-explainable-ai/index.html#we-need-you",
    "title": "A new tool for explainable AI",
    "section": "We need you! ü´µ",
    "text": "We need you! ü´µ\nThe ambition for CounterfactualExplanations.jl is to provide a go-to place for counterfactual explanations to the Julia community and beyond. This is a grand ambition, especially for a package that has so far been built by a single developer who has little prior experience with Julia. We would therefore very much like to invite community contributions. If you have an interest in trustworthy AI, the open-source community and Julia, please do get involved! This package is still in its early stages of development, so any kind of contribution is welcome: advice on the core package architecture, pull requests, issues, discussions and even just comments below would be much appreciated.\nTo give you a flavor of what type of future developments we envision, here is a non-exhaustive list:\n\nNative support for additional counterfactual generators and predictive models including those built and trained in Python or R.\nAdditional datasets for testing, evaluation and benchmarking.\nImproved preprocessing including native support for categorical features.\nSupport for regression models.\n\nFinally, if you like this project but don‚Äôt have much time, then simply sharing this article or starring the repo on GitHub would also go a long way."
  },
  {
    "objectID": "blog/posts/a-new-tool-for-explainable-ai/index.html#further-reading",
    "href": "blog/posts/a-new-tool-for-explainable-ai/index.html#further-reading",
    "title": "A new tool for explainable AI",
    "section": "Further reading üìö",
    "text": "Further reading üìö\nIf you‚Äôre interested in learning more about this development, feel free to check out the following resources:\n\nPackage docs: [stable], [dev].\nContributor‚Äôs guide.\nGitHub repo."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#uncertainty",
    "href": "blog/posts/bayesian-logit/index.html#uncertainty",
    "title": "Bayesian Logistic Regression",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\n\n\nSimulation of changing parameter distribution.\n\n\n\nIf you‚Äôve ever searched for evaluation metrics to assess model accuracy, chances are that you found many different options to choose from (too many?). Accuracy is in some sense the holy grail of prediction so it‚Äôs not at all surprising that the machine learning community spends a lot time thinking about it. In a world where more and more high-stake decisions are being automated, model accuracy is in fact a very valid concern.\nBut does this recipe for model evaluation seem like a sound and complete approach to automated decision-making? Haven‚Äôt we forgot anything? Some would argue that we need to pay more attention to model uncertainty. No matter how many times you have cross-validated your model, the loss metric that it is being optimized against as well as its parameters and predictions remain inherently random variables. Focusing merely on prediction accuracy and ignoring uncertainty altogether can install a false level of confidence in automated decision-making systems. Any trustworthy approach to learning from data should therefore at the very least be transparent about its own uncertainty.\nHow can we estimate uncertainty around model parameters and predictions? Frequentist methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example here for the case of logistic regression). In Bayesian statistics and machine learning we are instead concerned with modelling the posterior distribution over model parameters. This approach to uncertainty quantification is known as Bayesian Inference because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on prior knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as unscientific. However, frequentist methods come with their own assumptions and pitfalls (see for example Murphy (2012)) for a discussion). Without diving further into this argument, let us now see how Bayesian Logistic Regression can be implemented from the bottom up."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-ground-truth",
    "href": "blog/posts/bayesian-logit/index.html#the-ground-truth",
    "title": "Bayesian Logistic Regression",
    "section": "The ground truth",
    "text": "The ground truth\n\n\n\nIn this post we will work with a synthetic toy data set \\(\\mathcal{D}\\) composed of \\(N\\) binary labels \\(y_n\\in\\{0,1\\}\\) and corresponding feature vectors \\(\\mathbf{x}_n\\in \\mathbb{R}^D\\). Working with synthetic data has the benefit that we have control over the ground truth that generates our data. In particular, we will assume that the binary labels \\(y_n\\) are generated by a logistic regression model\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y_n|\\mathbf{x}_n;\\mathbf{w})&\\sim\\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n)) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{1}\\]\nwhere \\(\\sigma(a)=1/(1+e^{-a})\\) is the sigmoid or logit function (Murphy 2022).1 Features are generated from a mixed Gaussian model.\nTo add a little bit of life to our example we will assume that the binary labels classify samples into cats and dogs, based on their height and tail length. ?@fig-ground shows the synthetic data in the two-dimensional feature domain. Following an introduction to Bayesian Logistic Regression in the next section we will use the synthetic data \\(\\mathcal{D}\\) to estimate our model."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-maths",
    "href": "blog/posts/bayesian-logit/index.html#the-maths",
    "title": "Bayesian Logistic Regression",
    "section": "The maths",
    "text": "The maths\nEstimation usually boils down to finding the vector of parameters \\(\\hat{\\mathbf{w}}\\) that maximizes the likelihood of observing \\(\\mathcal{D}\\) under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set \\(\\mathcal{D}=\\{x_m:m=1,...,M\\}\\).\n\nProblem setup\nThe starting point for Bayesian Logistic Regression is Bayes‚Äô Theorem:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w}) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{2}\\]\nFormally, this says that the posterior distribution of parameters \\(\\mathbf{w}\\) is proportional to the product of the likelihood of observing \\(\\mathcal{D}\\) given \\(\\mathbf{w}\\) and the prior density of \\(\\mathbf{w}\\). Applied to our context this can intuitively be understood as follows: our posterior beliefs around \\(\\mathbf{w}\\) are formed by both our prior beliefs and the evidence we observe. Yet another way to look at this is that maximising Equation¬†2 with respect to \\(\\mathbf{w}\\) corresponds to maximum likelihood estimation regularized by prior beliefs (we will come back to this).\nUnder the assumption that individual label-feature pairs are independently and identically distributed, their joint likelihood is simply the product over their individual densities. The prior beliefs around \\(\\mathbf{w}\\) are at our discretion. In practice they may be derived from previous experiments. Here we will use a zero-mean spherical Gaussian prior for reasons explained further below. To sum this up we have\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathcal{D}|\\mathbf{w})& \\sim \\prod_{n=1}^N p(y_n|\\mathbf{x}_n;\\mathbf{w})\\\\\n&& p(\\mathbf{w})& \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{3}\\]\nwith \\(\\mathbf{w}_0=\\mathbf{0}\\) and \\(\\Sigma_0=\\sigma^2\\mathbf{I}\\). Plugging this into Bayes‚Äô rule we finally have\n\\[\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto\\prod_{n=1}^N \\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n))\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n\\]\nUnlike with linear regression there are no closed-form analytical solutions to estimating or maximising this posterior, but fortunately accurate approximations do exist (Murphy 2022). One of the simplest approaches called Laplace Approximation is straight-forward to implement and computationally very efficient. It relies on the observation that under the assumption of a Gaussian prior, the posterior of logistic regression is also approximately Gaussian: in particular, this Gaussian distribution is centered around the maximum a posteriori (MAP) estimate \\(\\hat{\\mathbf{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) with a covariance matrix equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\hat{\\mathbf{w}}))^{-1}\\). With that in mind, finding \\(\\hat{\\mathbf{w}}\\) seems like a natural next step.\n\n\nSolving the problem\nIn practice we do not maximize the posterior \\(p(\\mathbf{w}|\\mathcal{D})\\) directly. Instead we minimize the negative log likelihood, which is an equivalent optimization problem and easier to implement. In Equation¬†4 below I have denoted the negative log likelihood as \\(\\ell(\\mathbf{w})\\) indicating that this is the loss function we aim to minimize. The following two lines in Equation¬†4 show the gradient and Hessian - so the first- and second-order derivatives of \\(\\ell\\) with respect to \\(\\mathbf{w}\\) - where \\(\\mathbf{H}_0=\\Sigma_0^{-1}\\) and \\(\\mu_n=\\sigma(\\mathbf{w}^T\\mathbf{x}_n)\\). To understand how exactly the gradient and Hessian are derived see for example chapter 10 in Murphy (2022).2.\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\ell(\\mathbf{w})&=- \\sum_{n=1}^{N} [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\mathbf{x}_n + \\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla^2_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\left( \\mu_n(1-\\mu_n) \\mathbf{x}_n \\mathbf{x}_n^T \\right) + \\mathbf{H}_0\\\\\n\\end{aligned}\n\\end{equation}\n\\tag{4}\\]\n\nSIDENOTE üí°\nNote how earlier I mentioned that maximising the posterior likelihood can be seen as regularized maximum likelihood estimation. We can now make that connection explicit: in Equation¬†4 let us assume that \\(\\mathbf{w}_0=\\mathbf{0}\\). Then since \\(\\mathbf{H}_0=\\lambda\\mathbf{I}\\) with \\(1/\\sigma^2\\) the second term in the first line is simply \\(\\lambda \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}=\\lambda \\frac{1}{2} ||\\mathbf{w}||_2^2\\). This is equivalent to running logistic regression with an \\(\\ell_2\\)-penalty (Bishop 2006).\n\n\nSince minimizing the loss function in Equation¬†4 is a convex optimization problem we have many efficient algorithms to choose from in order to solve this problem. With the Hessian at hand it seems natural to use a second-order method, because incorporating information about the curvature of the loss function generally leads to faster convergence. Here we will implement Newton‚Äôs method in line with the presentation in chapter 8 of Murphy (2022).\n\n\nPosterior predictive\nSuppose now that we have trained the Bayesian Logistic Regression model as our binary classifier \\(g_N(\\mathbf{x})\\) using our training data \\(\\mathcal{D}\\). A new unlabelled sample \\((\\mathbf{x}_{N+1},?)\\) arrives. As with any binary classifier we can predict the missing label by simply plugging the new sample into our classifier \\(\\hat{y}_{N+1}=g_N(\\mathbf{x}_{N+1})=\\sigma(\\hat{\\mathbf{w}}^T\\mathbf{x}_{N+1})\\), where \\(\\hat{\\mathbf{w}}\\) is the MAP estimate as before. If at training phase we have found \\(g_N(\\mathbf{x})\\) to achieve good accuracy, we may expect \\((\\mathbf{x}_{N+1},\\hat{y}_{N+1})\\) to be a reasonably good approximation of the true and unobserved pair \\((\\mathbf{x}_{N+1},y_{N+1})\\). But since we are still dealing with an expected value of a random variable, we would generally like to have an idea of how noisy this prediction is.\nFormally, we are interested in the posterior predictive distribution:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y=1|\\mathbf{x}, \\mathcal{D})&= \\int \\sigma(\\mathbf{w}^T \\mathbf{x})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w} \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{5}\\]\n\nSIDENOTE üí°\nThe approach that ignores uncertainty altogether corresponds to what is referred to as plugin approximation of the posterior predictive. Formally, it imposes \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\approx p(y=1|\\mathbf{x}, \\hat{\\mathbf{w}})\\).\n\n\nWith the posterior distribution over model parameters \\(p(\\mathbf{w}|\\mathcal{D})\\) at hand we have the necessary ingredients to estimate the posterior predictive distribution \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\).\nAn obvious, but computationally expensive way to estimate it is through Monte Carlo: draw \\(\\mathbf{w}_s\\) from \\(p(\\mathbf{w}|\\mathcal{D})\\) for \\(s=1:S\\) and compute fitted values \\(\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\) each. Then the posterior predictive distribution corresponds to the average over all fitted values, \\(p(y=1|\\mathbf{x}, \\mathcal{D})=1/S \\sum_{s=1}^{S}\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\). By the law of large numbers the Monte Carlo estimate is an accurate estimate of the true posterior predictive for large enough \\(S\\). Of course, ‚Äúlarge enough‚Äù is somewhat loosely defined here and depending on the problem can mean ‚Äúvery large‚Äù. Consequently, the computational costs involved essentially know no upper bound.\nFortunately, it turns out that we can trade off a little bit of accuracy in return for a convenient analytical solution. In particular, we have that \\(\\sigma(a) \\approx \\Phi(\\lambda a)\\) where \\(\\Phi(.)\\) is the standard Gaussian cdf and \\(\\lambda=\\pi/8\\) ensures that the two functions have the same slope at the origin (?@fig-probit). Without dwelling further on the details we can use this finding to approximate the integral in Equation¬†5 as a sigmoid function. This is called probit approximation and implemented below."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-code",
    "href": "blog/posts/bayesian-logit/index.html#the-code",
    "title": "Bayesian Logistic Regression",
    "section": "The code",
    "text": "The code\nWe now have all the necessary ingredients to code Bayesian Logistic Regression up from scratch. While in practice we would usually want to rely on existing packages that have been properly tested, I often find it very educative and rewarding to program algorithms from the bottom up. You will see that Julia‚Äôs syntax so closely resembles the mathematical formulas we have seen above, that going from maths to code is incredibly easy. Seeing those formulas and algorithms then actually doing their magic is quite fun! The code chunk below, for example, shows the implementation of the loss function and its derivatives from Equation¬†4 above. Take a moment to go through the code line-by-line and try to understand how it relates back to the equations in Equation¬†4. Isn‚Äôt it amazing how closely the code resembles the actual equations?\n\nAside from the optimization routine this is essentially all there is to coding up Bayesian Logistic Regression from scratch in Julia Language. If you are curious to see the full source code in detail you can check out this interactive notebook. Now let us finally turn back to our synthetic data and see how Bayesian Logistic Regression can help us understand the uncertainty around our model predictions.\n\nDISCLAIMER ‚ùóÔ∏è\nI should mention that this is the first time I program in Julia, so for any Julia pros out there: please bear with me! Happy to hear your suggestions/comments."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-estimates",
    "href": "blog/posts/bayesian-logit/index.html#the-estimates",
    "title": "Bayesian Logistic Regression",
    "section": "The estimates",
    "text": "The estimates\n?@fig-posterior below shows the resulting posterior distribution for \\(w_2\\) and \\(w_3\\) at varying degrees of prior uncertainty \\(\\sigma\\). The constant \\(w_1\\) is held constant at the mode (\\(\\hat{w}_1\\)). The red dot indicates the MLE. Note how for the choice of \\(\\sigma\\rightarrow 0\\) the posterior is equal to the prior. This is intuitive since we have imposed that we have no uncertainty around our prior beliefs and hence no amount of new evidence can move us in any direction. Conversely, for \\(\\sigma \\rightarrow \\infty\\) the posterior distribution is centered around the unconstrained MLE: prior knowledge is very uncertain and hence the posterior is dominated by the likelihood of the data.\n\n\n\nWhat about the posterior predictive? The story is similar: since for \\(\\sigma\\rightarrow 0\\) the posterior is completely dominated by the zero-mean prior we have \\(p(y=1|\\mathbf{x},\\hat{\\mathbf{w}})=0.5\\) everywhere (top left panel in ?@fig-predictive. As we gradually increase uncertainty around our prior the predictive posterior depends more and more on the data \\(\\mathcal{D}\\): uncertainty around predicted labels is high only in regions that are not populated by samples \\((y_n, \\mathbf{x}_n)\\). Not surprisingly, this effect is strongest for the MLE (\\(\\sigma\\rightarrow \\infty\\)) where we see some evidence of overfitting."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#wrapping-up",
    "href": "blog/posts/bayesian-logit/index.html#wrapping-up",
    "title": "Bayesian Logistic Regression",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this post we have seen how Bayesian Logistic Regression can be implemented from scratch in Julia language. The estimated posterior distribution over model parameters can be used to quantify uncertainty around coefficients and model predictions. I have argued that it is important to be transparent about model uncertainty to avoid being overly confident in estimates.\nThere are many more benefits associated with Bayesian (probabilistic) machine learning. Understanding where in the input domain our model exerts high uncertainty can for example be instrumental in labelling data: see for example Gal, Islam, and Ghahramani (2017) and follow-up works for an interesting application to active learning for image data. Similarly, there is a recent work that uses estimates of the posterior predictive in the context of algorithmic recourse (Schut et al. 2021). For a brief introduction to algorithmic recourse see one of my previous posts.\nAs a great reference for further reading about probabilistic machine learning I can highly recommend Murphy (2022). An electronic version of the book is currently freely available as a draft. Finally, remember that if you want to try yourself at the code, you can check out this interactive notebook."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#references",
    "href": "blog/posts/bayesian-logit/index.html#references",
    "title": "Bayesian Logistic Regression",
    "section": "References",
    "text": "References\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. springer.\n\n\nGal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. ‚ÄúDeep Bayesian Active Learning with Image Data.‚Äù In International Conference on Machine Learning, 1183‚Äì92. PMLR.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. MIT press.\n\n\n‚Äî‚Äî‚Äî. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. ‚ÄúGenerating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.‚Äù In International Conference on Artificial Intelligence and Statistics, 1756‚Äì64. PMLR."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html",
    "href": "blog/posts/effortsless-bayesian-dl/index.html",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "",
    "text": "A Bayesian Neural Network gradually learns.\nDeep learning has dominated AI research in recent years1 - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on Twitter. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slightly conscious - that‚Äôs ‚Äúmay‚Äù and ‚Äúslightly‚Äù and only if you just go deep enough? On the other side you have prominent skeptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent (Pearl and Mackenzie 2018)."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "The case for Bayesian Deep Learning",
    "text": "The case for Bayesian Deep Learning\nWhatever side of this entertaining twitter dispute you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization (Raghunathan et al. 2019) and does ultimately not guarantee stability (Bastounis, Hansen, and Vlaƒçiƒá 2021). With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness (Slack et al. 2020).\nExactly why are deep neural networks unstable and in-transparent? Let \\(\\mathcal{D}=\\{x,y\\}_{n=1}^N\\) denote our feature-label pairs and let \\(f(x;\\theta)=y\\) denote some deep neural network specified by its parameters \\(\\theta\\). Then the first thing to note is that the number of free parameters \\(\\theta\\) is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters relative to the size of \\(\\mathcal{D}\\) is generally huge:\n\n[‚Ä¶] deep neural networks are typically very underspecified by the available data, and [‚Ä¶] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such ‚Äúcompelling explanation‚Äù for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:\n\nHow compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?\n\nA scenario like this very much calls for treating predictions from deep learning models probabilistically [Wilson (2020)]23.\nFormally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\]\nThe integral implies that we essentially need many predictions from many different specifications of \\(\\theta\\). Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above in recent years: Gal and Ghahramani (2016) propose using dropout at test time while Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: Laplace approximation (LA).\nIf you have read my previous post on Bayesian Logistic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\nWhile LA was first proposed in the 18th century, it has so far not attracted serious attention from the deep learning community largely because it involves a possibly large Hessian computation. Daxberger et al. (2021) are on a mission to change the perception that LA has no use in DL: in their NeurIPS 2021 paper they demonstrate empirically that LA can be used to produce Bayesian model averages that are at least at par with existing approaches in terms of uncertainty quantification and out-of-distribution detection and significantly cheaper to compute. They show that recent advancements in autodifferentation can be leveraged to produce fast and accurate approximations of the Hessian and even provide a fully-fledged Python library that can be used with any pretrained Torch model. For this post, I have built a much less comprehensive, pure-play equivalent of their package in Julia - LaplaceRedux.jl can be used with deep learning models built in Flux.jl, which is Julia‚Äôs main DL library. As in the previous post on Bayesian logistic regression I will rely on Julia code snippits instead of equations to convey the underlying maths. If you‚Äôre curious about the maths, the NeurIPS 2021 paper provides all the detail you need.\n\nFrom Bayesian Logistic Regression ‚Ä¶\nLet‚Äôs recap: in the case of logistic regression we had a assumed a zero-mean Gaussian prior \\(p(\\mathbf{w}) \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)=\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\mathbf{H}_0^{-1} \\right)\\) for the weights that are used to compute logits \\(\\mu_n=\\mathbf{w}^T\\mathbf{x}_n\\), which in turn are fed to a sigmoid function to produce probabilities \\(p(y_n=1)=\\sigma(\\mu_n)\\). We saw that under this assumption solving the logistic regression problem corresponds to minimizing the following differentiable loss function:\n\\[\n\\ell(\\mathbf{w})= - \\sum_{n}^N [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\\\ \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0)\n\\]\nAs our first step towards Bayesian deep learning, we observe the following: the loss function above corresponds to the objective faced by a single-layer artificial neural network with sigmoid activation and weight decay4. In other words, regularized logistic regression is equivalent to a very simple neural network architecture and hence it is not surprising that underlying concepts can in theory be applied in much the same way.\nSo let‚Äôs quickly recap the next core concept: LA relies on the fact that the second-order Taylor expansion of our loss function \\(\\ell\\) evaluated at the maximum a posteriori (MAP) estimate \\(\\mathbf{\\hat{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) amounts to a multi-variate Gaussian distribution. In particular, that Gaussian is centered around the MAP estimate with covariance equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\mathbf{\\hat{w}}))^{-1}\\) (Murphy 2022).\nThat is basically all there is to the story: if we have a good estimate of \\(\\mathbf{H}(\\mathbf{\\hat{w}})\\) we have an analytical expression for an (approximate) posterior over parameters. So let‚Äôs go ahead and start by run Bayesian Logistic regression using Flux.jl. We begin by loading some required packages including LaplaceRedux.jl. It ships with a helper function toy_data_linear that creates a toy data set composed of linearly separable samples evenly balanced across the two classes.\n\n# Import libraries.\nusing Flux, Plots, Random, PlotThemes, Statistics, LaplaceRedux\ntheme(:wong)\n# Number of points to generate.\nxs, y = toy_data_linear(100)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);\n\nThen we proceed to prepare the single-layer neural network with weight decay. The term \\(\\lambda\\) determines the strength of the \\(\\ell2\\) penalty: we regularize parameters \\(\\theta\\) more heavily for higher values. Equivalently, we can say that from the Bayesian perspective it governs the strength of the prior \\(p(\\theta) \\sim \\mathcal{N} \\left( \\theta | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)= \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\lambda_0^{-2} \\mathbf{I} \\right)\\): a higher value of \\(\\lambda\\) indicates a higher conviction about our prior belief that \\(\\theta=\\mathbf{0}\\), which is of course equivalent to regularizing more heavily. The exact choice of \\(\\lambda=0.5\\) for this toy example is somewhat arbitrary (it made for good visualizations below). Note that I have used \\(\\theta\\) to denote our neural parameters to distinguish the case from Bayesian logistic regression, but we are in fact still solving the same problem.\n\nnn = Chain(Dense(2,1))\nŒª = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();\n\nBefore we apply Laplace approximation we train our model:\n\nusing Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 50\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nUp until this point we have just followed the standard recipe for training a regularized artificial neural network in Flux.jl for a simple binary classification task. To compute the Laplace approximation using LaplaceRedux.jl we need just two more lines of code:\n\nla = laplace(nn, Œª=Œª)\nfit!(la, data);\n\nUnder the hood the Hessian is approximated through the empirical Fisher, which can be computed using only the gradients of our loss function \\(\\nabla_{\\theta}\\ell(f(\\mathbf{x}_n;\\theta,y_n))\\) where \\(\\{\\mathbf{x}_n,y_n\\}\\) are training data (see NeurIPS 2021 paper for details). Finally, LaplaceRedux.jl ships with a function predict(ùë≥::LaplaceRedux, X::AbstractArray; link_approx=:probit) that computes the posterior predictive using a probit approximation, much like we saw in the previous post. That function is used under the hood of the plot_contour function below to create the right panel of Figure¬†1. It visualizes the posterior predictive distribution in the 2D feature space. For comparison I have added the corresponding plugin estimate as well. Note how for the Laplace approximation the predicted probabilities fan out indicating that confidence decreases in regions scarce of data.\n\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin);\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_logit.png\");\n\n\n\n\nFigure¬†1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\n\n\n‚Ä¶ to Bayesian Neural Networks\nNow let‚Äôs step it up a notch: we will repeat the exercise from above, but this time for data that is not linearly separable using a simple MLP instead of the single-layer neural network we used above. The code below is almost the same as above, so I will not go through the various steps again.\n\n# Number of points to generate:\nxs, y = toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y)\n\n# Build MLP:\nn_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(D, n_hidden, œÉ),\n    Dense(n_hidden, 1)\n)  \nŒª = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(Œª=Œª) = 1/2 * Œª^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()\n\n# Training:\nepochs = 200\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nFitting the Laplace approximation is also analogous, but note that this we have added an argument: subset_of_weights=:last_layer. This specifies that we only want to use the parameters of the last layer of our MLP. While we could have used all of them (subset_of_weights=:all), Daxberger et al. (2021) find that the last-layer Laplace approximation produces satisfying results, while be computationally cheaper. Figure¬†2 demonstrates that once again the Laplace approximation yields a posterior predictive distribution that is more conservative than the over-confident plugin estimate.\n\nla = laplace(nn, Œª=Œª, subset_of_weights=:last_layer)\nfit!(la, data);\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin)\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_mlp.png\");\n\n\n\n\nFigure¬†2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\nTo see why this is a desirable outcome consider the zoomed out version of Figure¬†2 below: the plugin estimator classifies with full confidence in regions completely scarce of any data. Arguably Laplace approximation produces a much more reasonable picture, even though it too could likely be improved by fine-tuning our choice of \\(\\lambda\\) and the neural network architecture.\n\nzoom=-50\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\np_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400));\nsavefig(plt, \"www/posterior_predictive_mlp_zoom.png\");\n\n\n\n\nFigure¬†3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Wrapping up",
    "text": "Wrapping up\nRecent state-of-the-art research on neural information processing suggests that Bayesian deep learning can be effortless: Laplace approximation for deep neural networks appears to work very well and it does so at minimal computational cost (Daxberger et al. 2021). This is great news, because the case for turning Bayesian is strong: society increasingly relies on complex automated decision-making systems that need to be trustworthy. More and more of these systems involve deep learning which in and of itself is not trustworthy. We have seen that typically there exist various viable parameterizations of deep neural networks each with their own distinct and compelling explanation for the data at hand. When faced with many viable options, don‚Äôt put all of your eggs in one basket. In other words, go Bayesian!"
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#resources",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#resources",
    "title": "Go deep, but also ‚Ä¶ go Bayesian!",
    "section": "Resources",
    "text": "Resources\nTo get started with Bayesian deep learning I have found many useful and free resources online, some of which are listed below:\n\nTuring.jl tutorial on Bayesian deep learning in Julia.\nVarious RStudio AI blog posts including this one and this one.\nTensorFlow blog post on regression with probabilistic layers.\nKevin Murphy‚Äôs draft text book, now also available as print."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "",
    "text": "Julia and Quarto: a perfect match.\nDoes your work involve research, coding, writing and publishing? If so, then chances are that you often find yourself bouncing back and forth between different open-source text editors, IDEs, programming languages and platforms depending on your current needs. Using a diverse set of tools is reasonable, because there typically is no single perfect approach that solves all our problems. For example, interactive notebooks like Jupyter are useful for working with code and communicating it to others, but they are probably not anyone‚Äôs first choice for producing a scientific article. Similarly, Beamer presentations can be useful for presenting science in a standardized fashion, but they are the very opposite of interactive and look incredibly boring.\nAs much as the great variety of free tools deserves being celebrated, all this bouncing back and forth can be really tiring. What if there was a single tool, an engine that can turn your work into all kinds of different outputs? I mean literally any output you can think of: Markdown, HTML, PDF, LateX, ePub, entire websites, presentations (yes, also Beamer if you have to), MS Word, OpenOffice, ‚Ä¶ the list goes on. All of that starting from the same place: a plain Markdown document blended with essentially any programming language of your choice and a YAML header defining your output. This tool now exists and it goes by the name Quarto.\nIn this short blog post I hope to convince you that Quarto is the only publishing engine you will ever need. What I am definitely not going to tell you is which IDE, text editor or programming language you should be using to actually produce your work. Quarto does not care about that. Quarto is here to make your life a bit easier (and by ‚Äòa bit‚Äô I mean a whole lot). Quarto is nothing less but a revolution for scientific publishing.\nTo put this all in some context (well, my context), I will now tell you a bit about what has led me to making such bold claims about yet another open-source tool.\nYes! But it‚Äôs worth noting that a lot of the benefits that Quarto brings have been available to R users for many years, thanks to the amazing work of many great open-source contributors like @xieyihui. Julia was the main reason for me to branch out of this comfortable R bubble as I describe below. That said, if you are a Julia user who really couldn‚Äôt care less about my previous experiences with R Markdown, this is a good time to skip straight ahead to Section¬†2. By the way, if you haven‚Äôt clicked on that link, here‚Äôs a small showcase demonstrating how it was generated. It shows easy it is to have everything well organised and connected with Quarto."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-bubble",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-bubble",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "A comfortable bubble üéà",
    "text": "A comfortable bubble üéà\nFor many years I have used R Markdown for essentially anything work-related. As an undergraduate economics student facing the unfortunate reality that people still teach Stata, I was drawn to R. This was partially because R has a great open-source community and also partially because Stata. Once I realised that I would be able to use R Markdown to write up all of my future homework assignments and even my thesis, I never looked back. MS Word was now officially dead to me. Overleaf was nothing more than a last resort if everyone else in my team insisted on using it for a group project. Being able to write my undergraduate dissertation in R Markdown was a first truly triumphant moment. Soon after that I would also try myself at Shiny, produce outputs in HTML and build entire websites through blogdown. And all of that from within R Studio involving R and Markdown and really not much else. During my first professional job at the Bank of England I was reluctant to use anything other than R Markdown to produce all of my output. Luckily for me, the Bank was very much heading in that same direction at the time and my reluctance was not perceived as stubbornness, but actually welcome (at least I hoped so).\n\nCracks in the bubble üß®\nSoon though, part of me felt a little boxed in. For any work that required me to look outside of the R bubble, I knew I might also have to give up a very, very comfortable work environment and my productivity would surely take a hit. During my master‚Äôs in Data Science, for example, the mantra was very much ‚ÄúPython + Jupyter or die‚Äù. Through reticulate and R Studio‚Äôs growing support for Python I managed to get by without having to leave my bubble too often. But reticulate always felt a little clunky (sorry!) and some professors were reluctant to accept anything other than Jupyter notebooks. Even if others had not perceived it that way in the past, I certainly started to feel that I might just be a little too attached the beautiful bubble that R Studio had created around me.\n\n\nEnter: Julia üí£\nThen there was Julia: elegant, fast, pure, scientific and - oh my REPL! - those beautiful colors and unicode symbols. The stuff of dreams, really! Geeky dreams, but dreams nonetheless. I had once before given Julia a shot when working with high-frequency trade data for a course in market microstructure. This was the first time R really revealed its limitations to me and my bubble nearly burst, but thanks to data.table and Rcpp I managed to escape with only minor bruises. Still, Julia kept popping up, teasing me whenever I would work on some Frakenstein-style C++ code snippets that would hopefully resolve my R bottlenecks. I actually enjoyed mixing some C++ into my R code like I did here, but the process was just a little painful and slow. But wouldn‚Äôt learning all of Julia take even more time and patience? And what about my dependence on R Markdown?\n\n\nJulia bursts my bubble üí•\nAs I started my PhD in September 2021, I eventually gave in. New beginnings - time to suck it up! If it meant that I‚Äôd have to use Jupyter notebooks with Julia, so be it! And so I was off to a somewhat bumpy start that would have me bouncing back and forth between trying to make Julia work in R Studio (meh), setting up Jupyter Lab (meeeh), just using the Julia REPL because ‚Äúthe REPL is all you need‚Äù (nope) and even struggling with Vim and Emacs. Then there was also Pluto.jl, of course, which admittedly looks amazing! But it also looks very much tailored to Julia and (I believe) the number of different output formats you can produce is still very limited. Eventually, I settled for VSCode in combination with Jupyter notebooks. As much as I dreaded the latter, Jupyter is popular, arguably versatile and supports both R and Julia. This setup worked well enough for me, but it still definitely fell short of the breeze that R Studio had always provided. One thing that really bugged me, for example, was the fact that the IJulia kernel was not accessible from the Julia REPL. Each notebook would have its own environment, which could only be accessed through the notebook. In R Studio the interaction between R Markdown and the console is seamless, as both have access to the same environment variables.\n\n\nEnter: Quarto ‚ù§Ô∏è‚Äçü©π\nAround the same time that I started using Julia, I read about Quarto for the first time. It looked ‚Ä¶ great! Like a timely little miracle really! But also ‚Ä¶ unfinished? Definitely experimental at the time. I loved the idea though and in a footnote somewhere on their website it said that the project was supported by R Studio which I took as a very good sign. So I decided to at least give it a quick try and built a small (tiny) website summarising some of the literature I had read for my PhD:\n\n\nJust had my first go #quarto and I absolutely love the concept! Open-source and language agnostic - truly amazing work from &#64rstudio https://t.co/veCg7ywQ8v\n\n‚Äî Patrick Altmeyer (&#64paltmey) October 29, 2021\n\n\nThis was a first very pleasant encounter with Quarto, arguable even smoother than building websites in blogdown. As for working with Julia though, I had made up my mind that VSCode was the way to go and at the time there was no Quarto extension (there is now). There was also little in terms of communication about the project by R Studio, probably because things were really still in the early development stages. I was hopeful that eventually Quarto would enable me to emulate the R Studio experience in VS Code, but for now things were not quite there yet.\n\n\nQuarto keeps growing ü§û\nSince I was now working with VSCode + Jupyter and since Quarto supports Jupyter as well as all of my old R Markdown work, my next little Quarto project involved turning my old blogdown-powered blog into a Quarto-powered blog. This was not strictly necessary, as I could always export my new Jupyter notebooks to HTML and let blogdown do the rest. But it did streamline things a little bit and the default Quarto blog theme - you are staring at it - is actually üî•. I also did not have to feel guilty towards @xieyihui about leaving blogdown, because unsurprisingly he is on the Quarto team. As I was working on this little project I started noticing that the Quarto website was updated regularly and responses to issues I opened like this one were answered very swiftly. Clearly, things were moving and they were moving fast. More recently, the news about Quarto has been spreading and it‚Äôs left some folks as confused and amazed as I was, when I first heard about it:\n\n\n#RStats can someone explain to me what's the difference between {Quarto} and {RMarkdown}? I saw a tweet about Quarto and now I'm all confused ‚Ä¶ What gap is it supposed to fill?\n\n‚Äî Erwin Lares (&#64lasrubieras) March 30, 2022\n\n\nThis is why finally I‚Äôve decided I should write a brief post about how and why I use Quarto. Since I have been working mostly with Julia for the past couple of months, I‚Äôve chosen to focus on the interaction between Quarto and Julia. Coincidentally, yesterday was also the first time I saw a guide dedicated to Julia on the Quarto website, so evidently I am not the only one interested in that marriage. This also means that there really is not too much left for me to talk about now, since Quarto‚Äôs documentation is state-of-the-art. But a few bits and pieces I mention below might hopefully still be useful or at least some food for thought."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-match",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-match",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "Quarto and Julia: a perfect match üíôüíúüíö",
    "text": "Quarto and Julia: a perfect match üíôüíúüíö\nWhile what follows may be relevant to other programming languages, my main goal for this last section is to flag Quarto to the Julia community. In any case, #rstats folks have been using R and Python in R Markdown documents for a while now and won‚Äôt need much of an introduction to Quarto. As for Python aficionados, I can only recommend to give Quarto a shot (you will still be able to use Jupyter notebooks).\n\nWorking with VSCode, Quarto and Julia\nThe very article you are reading right now was composed in a Quarto document. These documents feel and look very much like standard Julia Markdown documents, but you can do a lot more with them. You can find the source code for this and other documents presented in this blog here.\nTo get you started, here is my current setup combining VSCode, Quarto and Julia:\n\nVSCode extensions: in addition to the Julia extension you will need the Quarto extension. In addition, the YAML extension and some extension to preview Markdown docs would be helpful. I am not sure if Markdown Julia and Jupyter are strictly necessary, but it won‚Äôt hurt.\nI do most of my work in Quarto documents .qmd.\nIf you choose to also do that, make sure that the .qmd document has access to a Pkg.jl environment that has IJulia added.\n\nJulia code cells can be added anywhere along with your plain text Markdown. They look like this:\n```{julia}\nusing Pkg\nPkg.add(\"CounterfactualExplanations\")\n```\nContrary to Jupyter notebooks, executing this code cells will start a Julia REPL in VSCode. I find this very helpful, because it lets me fiddle with anything I have created inside the Quarto notebook without having to click into cells all the time. Quarto comes with great support for specifying code executing options. For example, for the code below I have specified #| echo: true in order for the code to be rendered. The code itself is the code I actually used to build the animation above (heavily borrowed from this Javis.jl tutorial).\n\nusing Javis, Animations, Colors\n\nsize = 600\nradius_factor = 0.33\n\nfunction ground(args...)\n    background(\"transparent\")\n    sethue(\"white\")\nend\n\nfunction rotate_anim(idx::Number, total::Number) \n    distance_circle = 0.875\n    steps = collect(range(distance_circle,1-distance_circle,length=total))\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [0, steps[idx]*2œÄ],\n        [sineio()],\n    )\nend\n\ntranslate_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(size*radius_factor, 0)],\n    [sineio()],\n)\n\ntranslate_back_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(-(size*radius_factor), 0)],\n    [sineio()],\n)\n\njulia_colours = Dict(\n    :blue => \"#4063D8\",\n    :green => \"#389826\",\n    :purple => \"#9558b2\",\n    :red => \"#CB3C33\"\n)\ncolour_order = [:red, :purple, :green, :blue]\nn_colours = length(julia_colours)\nfunction color_anim(start_colour::String, quarto_col::String=\"#4b95d0\")\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [Lab(color(start_colour)), Lab(color(quarto_col))],\n        [sineio()],\n    )\nend\n\nvideo = Video(size, size)\n\nframe_starts = 1:10:40\nn_total = 250\nn_frames = 150\nBackground(1:n_total, ground)\n\n# Blob:\nfunction element(; radius = 1)\n    circle(O, radius, :fill) # The 4 is to make the circle not so small\nend\n\n# Cross:\nfunction cross(color=\"black\";orientation=:horizontal)\n    sethue(color)\n    setline(10)\n    if orientation==:horizontal\n        out = line(Point(-size,0),Point(size,0), :stroke)\n    else\n        out = line(Point(0,-size),Point(0,size), :stroke)\n    end\n    return out\nend\n\nfor (i, frame_start) in enumerate(1:10:40)\n\n    # Julia circles:\n    blob = Object(frame_start:n_total, (args...;radius=1) -> element(;radius=radius))\n    act!(blob, Action(1:Int(round(n_frames*0.25)), change(:radius, 1 => 75))) # scale up\n    act!(blob, Action(n_frames:(n_frames+50), change(:radius, 75 => 250))) # scale up further\n    act!(blob, Action(1:30, translate_anim, translate()))\n    act!(blob, Action(31:120, rotate_anim(i, n_colours), rotate_around(Point(-(size*radius_factor), 0))))\n    act!(blob, Action(121:150, translate_back_anim, translate()))\n    act!(blob, Action(1:150, color_anim(julia_colours[colour_order[i]]), sethue()))\n\n    # Quarto cross:\n    cross_h = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:horizontal))\n    cross_v = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:vertical))\nend\n\nrender(\n    video;\n    pathname = joinpath(www_path, \"intro.gif\"),\n)\n\n\n\nWorking with Documenter.jl and Quarto\nAn interesting application of Quarto in the Julia ecosystem is package documentation. This is of course best done using Documenter.jl and fortunately the two play nicely with each other, since both share a common ground (Markdown). Their interaction is perhaps best demonstrated through this Julia library I recently developed: CounterfactualExplanatinos.jl. On there you will find lot of Julia scripts *.jl under src/ and test/, as well as many Markdown .md and Quarto documents .qmd under docs. I wrote the package documentation in the Quarto documents, rendered documents individually through quarto render [doc].qmd and then fed the resulting Markdown documents to Documenter.jl as always.\nBelow is my standard YAML header for those Quarto documents:\nformat: \n  commonmark:\n    variant: -raw_html\n    wrap: none\n    self-contained: true\ncrossref:\n  fig-prefix: Figure\n  tbl-prefix: Table\nbibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib\noutput: asis\nexecute: \n  echo: true\n  eval: false\njupyter: julia-1.7\nYou can see that it points to Bibtex file I host on another Github repository. This makes it very easy to generate citations and references for the rendered Markdown documents, that also show up in the docs (e.g.¬†here). Unfortunately, cross-referencing only partially works, because it relies on auto-generated HTML and Documenter.jl expects this to be passed in blocks. Choosing variant: -raw_html is only a workaround as I have discussed here. Ideally, Documenter.jl would just accept HTML documents rendered from Quarto, but currently only Markdown documents are accepted by make_docs. Still, if anything this workaround is a nice gimmick that extends the default Documenter.jl functionality, without any hassle involved. Hopefully, this can be improved in the future.\n\n\nUsing Quarto for JuliaCon Proceedings\nAnother very good use-case for Quarto involves actual scientific publications in journals such as JuliaCon Proceedings. The existing submission process is tailored towards reproducibility and actually involves reviews directly on GitHub, which is fantastic. But currently only submissions in TeX format are accepted, which is not so great. Using Quarto would not only streamline this process further, but also open the JuliaCon Proceedings Journal up to publishing content in different output formats. Quarto docs could be used to still render the traditional PDF. But those same documents could also be used to create interactive versions in HTML. Arguably, the entire journal could probably be built through Quarto."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#wrapping-up",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#wrapping-up",
    "title": "Julia and Quarto: a match made in heaven? üå§",
    "section": "Wrapping up üéó",
    "text": "Wrapping up üéó\nIn this post I wanted to demonstrate that Quarto might just be the next revolution in scientific publishing. In particular, I hope I have managed to demonstrate its appeal to the Julia community, which I am proud to be part of now that I have managed to branch out of my old R bubble. Please let me hear your thoughts and comments below!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Conformal Prediction in Julia üü£üî¥üü¢\n\n\nPart 1 - Introduction\n\n\nA (very) gentle introduction to Conformal Prediction in Julia using my new package ConformalPrediction.jl.\n\n\n\n\n\n\nOct 25, 2022\n\n\nPatrick Altmeyer, Patrick Altmeyer\n\n\n14 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nA new tool for explainable AI\n\n\nExplaining models through counterfactuals\n\n\nThis post introduces a new Julia package for generating counterfactual explanations. The package can be used to explain machine learning algorithms developed and trained in Julia as well as other popular programming languages like Python and R.\n\n\n\n\n\n\nApr 20, 2022\n\n\nPatrick Altmeyer\n\n\n11 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nJulia and Quarto: a match made in heaven? üå§\n\n\nA new way to publish science\n\n\nAn opinionated, practical review celebrating the open-source community. I discuss why Quarto is nothing short of revolutionary and how I‚Äôve been using it with Julia.\n\n\n\n\n\n\nApr 7, 2022\n\n\nPatrick Altmeyer\n\n\n15 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nGo deep, but also ‚Ä¶ go Bayesian!\n\n\nEffortless Bayesian Deep Learning in Julia\n\n\nAn introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia.\n\n\n\n\n\n\nFeb 18, 2022\n\n\nPatrick Altmeyer\n\n\n11 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Logistic Regression\n\n\nFrom scratch in Julia Language\n\n\nAn introduction to Bayesian Logistic Regression from the bottom up with examples in Julia language.\n\n\n\n\n\n\nNov 15, 2021\n\n\nPatrick Altmeyer\n\n\n13 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nIndividual recourse for Black Box Models\n\n\nExplained through a tale of üê±‚Äôs and üê∂‚Äôs\n\n\nAn introduction to algorithmic recourse and an implementation of a simplified version of REVISE (Joshi et al., 2019). This post was prepared as part of my PhD application.\n\n\n\n\n\n\nApr 27, 2021\n\n\nPatrick Altmeyer\n\n\n10 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n  \n\n\n\n\nA peek inside the ‚ÄòBlack Box‚Äô - interpreting neural networks\n\n\n\n\n\nResearch on explainable AI has recently gained considerable momentum. In this post I explore a Bayesian approach to ex-post explainability of Deep Neural Networks.a\n\n\n\n\n\n\nFeb 7, 2021\n\n\nPatrick Altmeyer\n\n\n10 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I‚Äôm building this website in R\n\n\n\n\n\nA small post on how I (used to!) build this website using blogdown.\n\n\n\n\n\n\nFeb 2, 2021\n\n\nPatrick Altmeyer\n\n\n5 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nThe obligatory welcome post and a shout-out to Yihui Xie and the folks behind quarto.\n\n\n\n\n\n\nFeb 1, 2021\n\n\nPatrick Altmeyer, Patrick Altmeyer\n\n\n0 min\n\n\n11/4/22, 4:59:51 PM\n\n\n\n\n\n\nNo matching items"
  }
]