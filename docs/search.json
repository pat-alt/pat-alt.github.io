[
  {
    "objectID": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html",
    "href": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html",
    "title": "A year of using Quarto with Julia",
    "section": "",
    "text": "A year of using Quarto with Julia.\nEarlier this year in July, I gave a short Experience Talk at JuliaCon. In a related blog post I explained how the introduction of Quarto made my transition from R to Julia painless: I would be able to start learning Julia without having to give up on all the benefits associated with R Markdown.\nIn November, 2022, I am presenting on this topic again at the 2nd JuliaLang Eindhoven meetup. In addition to the slides, I thought I’d share a small companion blog post that highlights some useful tips and tricks for anyone interested in using Quarto with Julia."
  },
  {
    "objectID": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#general-things",
    "href": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#general-things",
    "title": "A year of using Quarto with Julia",
    "section": "General things",
    "text": "General things\nWe will start in this section with a few general recommendations.\n\nSetup\nI continue to recommend using VSCode for any work with Quarto and Julia. The Quarto docs explain how to get started by installing the necessary Quarto and IJulia extensions. Since most Julia users will regularly want to update their Julia version, I would additionally recommend to add IJulia.jl to your ~/.julia/config/startup.jl file:1\n# Setup OhMyREPL, Revise and Term\nimport Pkg\nlet\n    pkgs = [\"Revise\", \"OhMyREPL\", \"Term\", \"IJulia\"]\n    for pkg in pkgs\n        if Base.find_package(pkg) === nothing\n            Pkg.add(pkg)\n        end\n    end\nend\nAdditionally, you only need to remember that …\n\n… if you install a new Julia binary […], you must update the IJulia installation […] by running Pkg.build(\"IJulia\")\n— Source: IJulia docs\n\nI guess this step can also be automated in ~/.julia/config/startup.jl, but haven’t tried that yet.\n\n\nUsing .ipynb vs .qmd\nI also continue to recommend working with Quarto notebooks as opposed to Jupyter notebooks (files ending in .qmd and .ipynb, respectively). This is partially just based on preference (from R Markdown I’m used to working with .Rmd files), but there is also a good reason to consider using .qmd, even if you’re used to working with Jupyter: the code chunks in your Quarto notebook automatically link to the Julia REPL in VSCode. In other words, you can run code chunks in your notebook and then access any variable that you may have created in the REPL. I find this quite useful, cause it allows me to quickly test code. Perhaps there’s a good way to do this with Jupyter notebooks as well, but when I last used them I would always have to insert new code cells to test stuff.\nEither way switching between Jupyter and Quarto notebooks is straight-forward: quarto convert notebook.qmd will convert any Quarto notebook into a Jupyter notebook and vice versa. One potential benefit of Jupyter notebooks is their connection to Google Colab: it is possible to store Jupyter notebooks on Github and make them available on Colab, allowing users to quickly interact with your code without the need to clone anything. If this is important to you, you can still work with .qmd documents and simply specify keep-ipynb: true in the YAML header.\n\n\nDynamic Content\n\nThe world and the data that describes it is not static 📈. Why should scientific outputs be?\n\nOne of the things I have always really loved about R Markdown was the ability to use inline code: the Knitr engine allows you to call and render any object x that you have created in preceding R chunks like this: r x. This is very powerful, because it enables us to bridge the gap between computations and output. In other words, it allows us to easily produce reproducible and dynamic content.\nUntil recently I had not been aware that this is also possible for Julia. Consider the following example. The code below depends on remote data that is continuously updated:\n\nusing MarketData\nsnp = yahoo(\"^GSPC\")\n\nusing Dates\nlast_trade_day = timestamp(snp[end])[1]\np_close = values(snp[end,:Close])[1]\nlast_trade_day_formatted = Dates.format(last_trade_day, \"U d, yyyy\")\n\nIt loads the most recent publicly available data on equity prices from Yahoo finance. In an ideal world, we’d like any updates to these inputs to be reflected in our output. That way you can just re-render the Quarto notebook to get an updated report. To render Julia code inline, we use Markdown.jl like so:\n\nusing Markdown\nMarkdown.parse(\"\"\"\nWhen the S&P 500 last traded, on $(last_trade_day_formatted), it closed at $(p_close). \n\"\"\")\n\nWhen the S&P 500 last traded, on November 18, 2022, it closed at 3965.340088.\n\n\nIn practice, one would of course set #| echo: false in this case. Whatever content you publish, this approach will keep it up-to-date. This practice of simply re-rendering the source notebook also ensures that any other output remains up-to-date (e.g. Figure 1)\n\n\n\n\n\nFigure 1: Price history of the S&P 500.\n\n\n\n\n\n\nCode Execution\nRelated to the previous point, I typically define the following execution options in my _quarto.yml or _metadata.yml. The freeze: auto option ensures that documents are only rerendered if the source changes. In cases where code should always be re-executed you whould want to set freeze: false, instead. I set output: false because typically I have a lot of code chunks that don’t generate any output that is of immediate interest to readers.\nexecute:\n  freeze: auto\n  eval: true\n  echo: true\n  output: false\n\n\nReproducibility\nTo ensure that your content can be repoduced easily, it may additionally be helpful to explicitly specify the Julia version you used (jupyter: julia-1.8) and set up a global or local Julia environments. Inserting the following at the beginning of your Quarto notebook\nusing Pkg; Pkg.activate(\"<path>\")\nensures that the desired environemnt that lives in <path> is actually activated and used."
  },
  {
    "objectID": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#package-documentation",
    "href": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#package-documentation",
    "title": "A year of using Quarto with Julia",
    "section": "Package Documentation",
    "text": "Package Documentation\nI have also continued to use Quarto in combination with Documenter.jl to document my Julia packages. This essentially boils down to writing up documentation using interactive .qmd notebooks and then rendering those to .md files as inputs for Documenter.jl. There are a few good reasons for this approach, especially if you’re used to working with Quarto anyway:\n\nRe-rendering any docs with eval: true provides an additional layer of quality assurance: if any of the code chunks throws an error, you know that your documentation is outdated (perhaps due to an API change). It also offers a straight-forward way to test package functions that produce non-testable (e.g. stochastic) output. In such cases, the use of jldoctest is not always straight-forward (see here).\nYou get some stuff for free, e.g. citation management. Unfortunately, as far as I’m aware there is still no support for cross-referencing.\nYou can use Quarto execution options like execute-dir: project and resources: www/ to globally specify the working directory and a directory for external resources like images.\n\nThere are also a few peculiarities to be aware of. To avoid any issues with Documenter.jl, I’ve found it useful to ensure that the rendered .md files do not contain any raw HTML and to preserve text wrapping:\nformat: \n  commonmark:\n    variant: -raw_html\n    wrap: preserve\nWhen working with .qmd files you also need to use a slightly different syntax for admonitions. The following syntax inside the .qmd\n| !!! note \\\"An optional title\\\"\n|     Here is something that you should pay attention to.   \nwill generate the desired output inside the rendered .md:2\n!!! note \"An optional title\"\n    Here is something that you should pay attention to.   \nAny of my package repos — CounterfactualExplanations.jl, LaplaceRedux.jl, ConformalPrediction.jl — should provide additional colour on this topic."
  },
  {
    "objectID": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#quarto-for-academic-journal-articles",
    "href": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#quarto-for-academic-journal-articles",
    "title": "A year of using Quarto with Julia",
    "section": "Quarto for Academic Journal Articles",
    "text": "Quarto for Academic Journal Articles\nQuarto supports \\(\\LaTeX\\) templates/classes, which has helped me with paper submissions in the past (e.g. my pending JuliaCon Proceedings submissions). I’ve found that rticles still has an edge here, but the list of out-of-the-box templates for journal articles is growing. Should I find some time in the future, I will try to add a template for JuliaCon Proceedings. The beauty of this is that it should enable publishers to not only use traditional forms of publication (PDF), but also include more dynamic formats with ease (think distill, but more than that.)"
  },
  {
    "objectID": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#wrapping-up",
    "href": "blog/posts/tips-and-tricks-for-using-quarto-with-julia/index.html#wrapping-up",
    "title": "A year of using Quarto with Julia",
    "section": "Wrapping up",
    "text": "Wrapping up\nThis short post has provided a bit of an update on using Quarto with Julia. From my own experience so far, things have been getting easier and better (thanks to the amazing work of Quarto dev team). I’m exicted to see things improve even further and still think that Quarto is a revolutionary new tool for scientific publishing. Let’s hope publishers eventually recognise this value 👀."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#uncertainty",
    "href": "blog/posts/bayesian-logit/index.html#uncertainty",
    "title": "Bayesian Logistic Regression",
    "section": "Uncertainty",
    "text": "Uncertainty\n\n\n\n\nSimulation of changing parameter distribution.\n\n\n\nIf you’ve ever searched for evaluation metrics to assess model accuracy, chances are that you found many different options to choose from (too many?). Accuracy is in some sense the holy grail of prediction so it’s not at all surprising that the machine learning community spends a lot time thinking about it. In a world where more and more high-stake decisions are being automated, model accuracy is in fact a very valid concern.\nBut does this recipe for model evaluation seem like a sound and complete approach to automated decision-making? Haven’t we forgot anything? Some would argue that we need to pay more attention to model uncertainty. No matter how many times you have cross-validated your model, the loss metric that it is being optimized against as well as its parameters and predictions remain inherently random variables. Focusing merely on prediction accuracy and ignoring uncertainty altogether can install a false level of confidence in automated decision-making systems. Any trustworthy approach to learning from data should therefore at the very least be transparent about its own uncertainty.\nHow can we estimate uncertainty around model parameters and predictions? Frequentist methods for uncertainty quantification generally involve either closed-form solutions based on asymptotic theory or bootstrapping (see for example here for the case of logistic regression). In Bayesian statistics and machine learning we are instead concerned with modelling the posterior distribution over model parameters. This approach to uncertainty quantification is known as Bayesian Inference because we treat model parameters in a Bayesian way: we make assumptions about their distribution based on prior knowledge or beliefs and update these beliefs in light of new evidence. The frequentist approach avoids the need for being explicit about prior beliefs, which in the past has sometimes been considered as unscientific. However, frequentist methods come with their own assumptions and pitfalls (see for example Murphy (2012)) for a discussion). Without diving further into this argument, let us now see how Bayesian Logistic Regression can be implemented from the bottom up."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-ground-truth",
    "href": "blog/posts/bayesian-logit/index.html#the-ground-truth",
    "title": "Bayesian Logistic Regression",
    "section": "The ground truth",
    "text": "The ground truth\n\n\n\nIn this post we will work with a synthetic toy data set \\(\\mathcal{D}\\) composed of \\(N\\) binary labels \\(y_n\\in\\{0,1\\}\\) and corresponding feature vectors \\(\\mathbf{x}_n\\in \\mathbb{R}^D\\). Working with synthetic data has the benefit that we have control over the ground truth that generates our data. In particular, we will assume that the binary labels \\(y_n\\) are generated by a logistic regression model\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y_n|\\mathbf{x}_n;\\mathbf{w})&\\sim\\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n)) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{1}\\]\nwhere \\(\\sigma(a)=1/(1+e^{-a})\\) is the sigmoid or logit function (Murphy 2022).1 Features are generated from a mixed Gaussian model.\nTo add a little bit of life to our example we will assume that the binary labels classify samples into cats and dogs, based on their height and tail length. Figure 1 shows the synthetic data in the two-dimensional feature domain. Following an introduction to Bayesian Logistic Regression in the next section we will use the synthetic data \\(\\mathcal{D}\\) to estimate our model.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 1: Ground truth labels."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-maths",
    "href": "blog/posts/bayesian-logit/index.html#the-maths",
    "title": "Bayesian Logistic Regression",
    "section": "The maths",
    "text": "The maths\nEstimation usually boils down to finding the vector of parameters \\(\\hat{\\mathbf{w}}\\) that maximizes the likelihood of observing \\(\\mathcal{D}\\) under the assumed model. That estimate can then be used to compute predictions for some new unlabelled data set \\(\\mathcal{D}=\\{x_m:m=1,...,M\\}\\).\n\nProblem setup\nThe starting point for Bayesian Logistic Regression is Bayes’ Theorem:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto p(\\mathcal{D}|\\mathbf{w})p(\\mathbf{w}) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{2}\\]\nFormally, this says that the posterior distribution of parameters \\(\\mathbf{w}\\) is proportional to the product of the likelihood of observing \\(\\mathcal{D}\\) given \\(\\mathbf{w}\\) and the prior density of \\(\\mathbf{w}\\). Applied to our context this can intuitively be understood as follows: our posterior beliefs around \\(\\mathbf{w}\\) are formed by both our prior beliefs and the evidence we observe. Yet another way to look at this is that maximising Equation 2 with respect to \\(\\mathbf{w}\\) corresponds to maximum likelihood estimation regularized by prior beliefs (we will come back to this).\nUnder the assumption that individual label-feature pairs are independently and identically distributed, their joint likelihood is simply the product over their individual densities. The prior beliefs around \\(\\mathbf{w}\\) are at our discretion. In practice they may be derived from previous experiments. Here we will use a zero-mean spherical Gaussian prior for reasons explained further below. To sum this up we have\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(\\mathcal{D}|\\mathbf{w})& \\sim \\prod_{n=1}^N p(y_n|\\mathbf{x}_n;\\mathbf{w})\\\\\n&& p(\\mathbf{w})& \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{3}\\]\nwith \\(\\mathbf{w}_0=\\mathbf{0}\\) and \\(\\Sigma_0=\\sigma^2\\mathbf{I}\\). Plugging this into Bayes’ rule we finally have\n\\[\n\\begin{aligned}\n&& p(\\mathbf{w}|\\mathcal{D})&\\propto\\prod_{n=1}^N \\text{Ber}(y_n|\\sigma(\\mathbf{w}^T\\mathbf{x}_n))\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{w}_0, \\Sigma_0 \\right) \\\\\n\\end{aligned}\n\\]\nUnlike with linear regression there are no closed-form analytical solutions to estimating or maximising this posterior, but fortunately accurate approximations do exist (Murphy 2022). One of the simplest approaches called Laplace Approximation is straight-forward to implement and computationally very efficient. It relies on the observation that under the assumption of a Gaussian prior, the posterior of logistic regression is also approximately Gaussian: in particular, this Gaussian distribution is centered around the maximum a posteriori (MAP) estimate \\(\\hat{\\mathbf{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) with a covariance matrix equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\hat{\\mathbf{w}}))^{-1}\\). With that in mind, finding \\(\\hat{\\mathbf{w}}\\) seems like a natural next step.\n\n\nSolving the problem\nIn practice we do not maximize the posterior \\(p(\\mathbf{w}|\\mathcal{D})\\) directly. Instead we minimize the negative log likelihood, which is an equivalent optimization problem and easier to implement. In Equation 4 below I have denoted the negative log likelihood as \\(\\ell(\\mathbf{w})\\) indicating that this is the loss function we aim to minimize. The following two lines in Equation 4 show the gradient and Hessian - so the first- and second-order derivatives of \\(\\ell\\) with respect to \\(\\mathbf{w}\\) - where \\(\\mathbf{H}_0=\\Sigma_0^{-1}\\) and \\(\\mu_n=\\sigma(\\mathbf{w}^T\\mathbf{x}_n)\\). To understand how exactly the gradient and Hessian are derived see for example chapter 10 in Murphy (2022).2.\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& \\ell(\\mathbf{w})&=- \\sum_{n=1}^{N} [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\mathbf{x}_n + \\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0) \\\\\n&& \\nabla^2_{\\mathbf{w}}\\ell(\\mathbf{w})&= \\sum_{n=1}^{N} (\\mu_n-y_n) \\left( \\mu_n(1-\\mu_n) \\mathbf{x}_n \\mathbf{x}_n^T \\right) + \\mathbf{H}_0\\\\\n\\end{aligned}\n\\end{equation}\n\\tag{4}\\]\n\nSIDENOTE 💡\nNote how earlier I mentioned that maximising the posterior likelihood can be seen as regularized maximum likelihood estimation. We can now make that connection explicit: in Equation 4 let us assume that \\(\\mathbf{w}_0=\\mathbf{0}\\). Then since \\(\\mathbf{H}_0=\\lambda\\mathbf{I}\\) with \\(1/\\sigma^2\\) the second term in the first line is simply \\(\\lambda \\frac{1}{2} \\mathbf{w}^T\\mathbf{w}=\\lambda \\frac{1}{2} ||\\mathbf{w}||_2^2\\). This is equivalent to running logistic regression with an \\(\\ell_2\\)-penalty (Bishop 2006).\n\n\nSince minimizing the loss function in Equation 4 is a convex optimization problem we have many efficient algorithms to choose from in order to solve this problem. With the Hessian at hand it seems natural to use a second-order method, because incorporating information about the curvature of the loss function generally leads to faster convergence. Here we will implement Newton’s method in line with the presentation in chapter 8 of Murphy (2022).\n\n\nPosterior predictive\nSuppose now that we have trained the Bayesian Logistic Regression model as our binary classifier \\(g_N(\\mathbf{x})\\) using our training data \\(\\mathcal{D}\\). A new unlabelled sample \\((\\mathbf{x}_{N+1},?)\\) arrives. As with any binary classifier we can predict the missing label by simply plugging the new sample into our classifier \\(\\hat{y}_{N+1}=g_N(\\mathbf{x}_{N+1})=\\sigma(\\hat{\\mathbf{w}}^T\\mathbf{x}_{N+1})\\), where \\(\\hat{\\mathbf{w}}\\) is the MAP estimate as before. If at training phase we have found \\(g_N(\\mathbf{x})\\) to achieve good accuracy, we may expect \\((\\mathbf{x}_{N+1},\\hat{y}_{N+1})\\) to be a reasonably good approximation of the true and unobserved pair \\((\\mathbf{x}_{N+1},y_{N+1})\\). But since we are still dealing with an expected value of a random variable, we would generally like to have an idea of how noisy this prediction is.\nFormally, we are interested in the posterior predictive distribution:\n\\[\n\\begin{equation}\n\\begin{aligned}\n&& p(y=1|\\mathbf{x}, \\mathcal{D})&= \\int \\sigma(\\mathbf{w}^T \\mathbf{x})p(\\mathbf{w}|\\mathcal{D})d\\mathbf{w} \\\\\n\\end{aligned}\n\\end{equation}\n\\tag{5}\\]\n\nSIDENOTE 💡\nThe approach that ignores uncertainty altogether corresponds to what is referred to as plugin approximation of the posterior predictive. Formally, it imposes \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\approx p(y=1|\\mathbf{x}, \\hat{\\mathbf{w}})\\).\n\n\nWith the posterior distribution over model parameters \\(p(\\mathbf{w}|\\mathcal{D})\\) at hand we have the necessary ingredients to estimate the posterior predictive distribution \\(p(y=1|\\mathbf{x}, \\mathcal{D})\\).\nAn obvious, but computationally expensive way to estimate it is through Monte Carlo: draw \\(\\mathbf{w}_s\\) from \\(p(\\mathbf{w}|\\mathcal{D})\\) for \\(s=1:S\\) and compute fitted values \\(\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\) each. Then the posterior predictive distribution corresponds to the average over all fitted values, \\(p(y=1|\\mathbf{x}, \\mathcal{D})=1/S \\sum_{s=1}^{S}\\sigma(\\mathbf{w_s}^T\\mathbf{x})\\). By the law of large numbers the Monte Carlo estimate is an accurate estimate of the true posterior predictive for large enough \\(S\\). Of course, “large enough” is somewhat loosely defined here and depending on the problem can mean “very large”. Consequently, the computational costs involved essentially know no upper bound.\nFortunately, it turns out that we can trade off a little bit of accuracy in return for a convenient analytical solution. In particular, we have that \\(\\sigma(a) \\approx \\Phi(\\lambda a)\\) where \\(\\Phi(.)\\) is the standard Gaussian cdf and \\(\\lambda=\\pi/8\\) ensures that the two functions have the same slope at the origin (Figure 2). Without dwelling further on the details we can use this finding to approximate the integral in Equation 5 as a sigmoid function. This is called probit approximation and implemented below.\n\n\n\n\n\nFigure 2: Demonstration of the probit approximation."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-code",
    "href": "blog/posts/bayesian-logit/index.html#the-code",
    "title": "Bayesian Logistic Regression",
    "section": "The code",
    "text": "The code\nWe now have all the necessary ingredients to code Bayesian Logistic Regression up from scratch. While in practice we would usually want to rely on existing packages that have been properly tested, I often find it very educative and rewarding to program algorithms from the bottom up. You will see that Julia’s syntax so closely resembles the mathematical formulas we have seen above, that going from maths to code is incredibly easy. Seeing those formulas and algorithms then actually doing their magic is quite fun! The code chunk below, for example, shows the implementation of the loss function and its derivatives from Equation 4 above. Take a moment to go through the code line-by-line and try to understand how it relates back to the equations in Equation 4. Isn’t it amazing how closely the code resembles the actual equations?\n\nAside from the optimization routine this is essentially all there is to coding up Bayesian Logistic Regression from scratch in Julia Language. If you are curious to see the full source code in detail you can check out this interactive notebook. Now let us finally turn back to our synthetic data and see how Bayesian Logistic Regression can help us understand the uncertainty around our model predictions.\n\nDISCLAIMER ❗️\nI should mention that this is the first time I program in Julia, so for any Julia pros out there: please bear with me! Happy to hear your suggestions/comments."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#the-estimates",
    "href": "blog/posts/bayesian-logit/index.html#the-estimates",
    "title": "Bayesian Logistic Regression",
    "section": "The estimates",
    "text": "The estimates\nFigure 3 below shows the resulting posterior distribution for \\(w_2\\) and \\(w_3\\) at varying degrees of prior uncertainty \\(\\sigma\\). The constant \\(w_1\\) is held constant at the mode (\\(\\hat{w}_1\\)). The red dot indicates the MLE. Note how for the choice of \\(\\sigma\\rightarrow 0\\) the posterior is equal to the prior. This is intuitive since we have imposed that we have no uncertainty around our prior beliefs and hence no amount of new evidence can move us in any direction. Conversely, for \\(\\sigma \\rightarrow \\infty\\) the posterior distribution is centered around the unconstrained MLE: prior knowledge is very uncertain and hence the posterior is dominated by the likelihood of the data.\n\n\n\nFigure 3: Posterior distribution for \\(w_2\\) and \\(w_3\\) at varying degrees of prior uncertainty \\(\\sigma\\).\n\n\nWhat about the posterior predictive? The story is similar: since for \\(\\sigma\\rightarrow 0\\) the posterior is completely dominated by the zero-mean prior we have \\(p(y=1|\\mathbf{x},\\hat{\\mathbf{w}})=0.5\\) everywhere (top left panel in Figure 4. As we gradually increase uncertainty around our prior the predictive posterior depends more and more on the data \\(\\mathcal{D}\\): uncertainty around predicted labels is high only in regions that are not populated by samples \\((y_n, \\mathbf{x}_n)\\). Not surprisingly, this effect is strongest for the MLE (\\(\\sigma\\rightarrow \\infty\\)) where we see some evidence of overfitting.\n\n\n\nFigure 4: Predictive posterior distribution at varying degrees of prior uncertainty \\(\\sigma\\)."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#wrapping-up",
    "href": "blog/posts/bayesian-logit/index.html#wrapping-up",
    "title": "Bayesian Logistic Regression",
    "section": "Wrapping up",
    "text": "Wrapping up\nIn this post we have seen how Bayesian Logistic Regression can be implemented from scratch in Julia language. The estimated posterior distribution over model parameters can be used to quantify uncertainty around coefficients and model predictions. I have argued that it is important to be transparent about model uncertainty to avoid being overly confident in estimates.\nThere are many more benefits associated with Bayesian (probabilistic) machine learning. Understanding where in the input domain our model exerts high uncertainty can for example be instrumental in labelling data: see for example Gal, Islam, and Ghahramani (2017) and follow-up works for an interesting application to active learning for image data. Similarly, there is a recent work that uses estimates of the posterior predictive in the context of algorithmic recourse (Schut et al. 2021). For a brief introduction to algorithmic recourse see one of my previous posts.\nAs a great reference for further reading about probabilistic machine learning I can highly recommend Murphy (2022). An electronic version of the book is currently freely available as a draft. Finally, remember that if you want to try yourself at the code, you can check out this interactive notebook."
  },
  {
    "objectID": "blog/posts/bayesian-logit/index.html#references",
    "href": "blog/posts/bayesian-logit/index.html#references",
    "title": "Bayesian Logistic Regression",
    "section": "References",
    "text": "References\n\n\nBishop, Christopher M. 2006. Pattern Recognition and Machine Learning. springer.\n\n\nGal, Yarin, Riashat Islam, and Zoubin Ghahramani. 2017. “Deep Bayesian Active Learning with Image Data.” In International Conference on Machine Learning, 1183–92. PMLR.\n\n\nMurphy, Kevin P. 2012. Machine Learning: A Probabilistic Perspective. MIT press.\n\n\n———. 2022. Probabilistic Machine Learning: An Introduction. MIT Press.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In International Conference on Artificial Intelligence and Statistics, 1756–64. PMLR."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html",
    "href": "blog/posts/effortsless-bayesian-dl/index.html",
    "title": "Go deep, but also … go Bayesian!",
    "section": "",
    "text": "A Bayesian Neural Network gradually learns.\nDeep learning has dominated AI research in recent years1 - but how much promise does it really hold? That is very much an ongoing and increasingly polarising debate that you can follow live on Twitter. On one side you have optimists like Ilya Sutskever, chief scientist of OpenAI, who believes that large deep neural networks may already be slightly conscious - that’s “may” and “slightly” and only if you just go deep enough? On the other side you have prominent skeptics like Judea Pearl who has long since argued that deep learning still boils down to curve fitting - purely associational and not even remotely intelligent (Pearl and Mackenzie 2018)."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#the-case-for-bayesian-deep-learning",
    "title": "Go deep, but also … go Bayesian!",
    "section": "The case for Bayesian Deep Learning",
    "text": "The case for Bayesian Deep Learning\nWhatever side of this entertaining twitter dispute you find yourself on, the reality is that deep-learning systems have already been deployed at large scale both in academia and industry. More pressing debates therefore revolve around the trustworthiness of these existing systems. How robust are they and in what way exactly do they arrive at decisions that affect each and every one of us? Robustifying deep neural networks generally involves some form of adversarial training, which is costly, can hurt generalization (Raghunathan et al. 2019) and does ultimately not guarantee stability (Bastounis, Hansen, and Vlačić 2021). With respect to interpretability, surrogate explainers like LIME and SHAP are among the most popular tools, but they too have been shown to lack robustness (Slack et al. 2020).\nExactly why are deep neural networks unstable and in-transparent? Let \\(\\mathcal{D}=\\{x,y\\}_{n=1}^N\\) denote our feature-label pairs and let \\(f(x;\\theta)=y\\) denote some deep neural network specified by its parameters \\(\\theta\\). Then the first thing to note is that the number of free parameters \\(\\theta\\) is typically huge (if you ask Mr Sutskever it really probably cannot be huge enough!). That alone makes it very hard to monitor and interpret the inner workings of deep-learning algorithms. Perhaps more importantly though, the number of parameters relative to the size of \\(\\mathcal{D}\\) is generally huge:\n\n[…] deep neural networks are typically very underspecified by the available data, and […] parameters [therefore] correspond to a diverse variety of compelling explanations for the data. (Wilson 2020)\n\nIn other words, training a single deep neural network may (and usually does) lead to one random parameter specification that fits the underlying data very well. But in all likelihood there are many other specifications that also fit the data very well. This is both a strength and vulnerability of deep learning: it is a strength because it typically allows us to find one such “compelling explanation” for the data with ease through stochastic optimization; it is a vulnerability because one has to wonder:\n\nHow compelling is an explanation really if it competes with many other equally compelling, but potentially very different explanations?\n\nA scenario like this very much calls for treating predictions from deep learning models probabilistically [Wilson (2020)]23.\nFormally, we are interested in estimating the posterior predictive distribution as the following Bayesian model average (BMA):\n\\[\np(y|x,\\mathcal{D}) = \\int p(y|x,\\theta)p(\\theta|\\mathcal{D})d\\theta\n\\]\nThe integral implies that we essentially need many predictions from many different specifications of \\(\\theta\\). Unfortunately, this means more work for us or rather our computers. Fortunately though, researchers have proposed many ingenious ways to approximate the equation above in recent years: Gal and Ghahramani (2016) propose using dropout at test time while Lakshminarayanan, Pritzel, and Blundell (2016) show that averaging over an ensemble of just five models seems to do the trick. Still, despite their simplicity and usefulness these approaches involve additional computational costs compared to training just a single network. As we shall see now though, another promising approach has recently entered the limelight: Laplace approximation (LA).\nIf you have read my previous post on Bayesian Logistic Regression, then the term Laplace should already sound familiar to you. As a matter of fact, we will see that all concepts covered in that previous post can be naturally extended to deep learning. While some of these concepts will be revisited below, I strongly recommend you check out the previous post before reading on here. Without further ado let us now see how LA can be used for truly effortless deep learning."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#laplace-approximation",
    "title": "Go deep, but also … go Bayesian!",
    "section": "Laplace Approximation",
    "text": "Laplace Approximation\nWhile LA was first proposed in the 18th century, it has so far not attracted serious attention from the deep learning community largely because it involves a possibly large Hessian computation. Daxberger et al. (2021) are on a mission to change the perception that LA has no use in DL: in their NeurIPS 2021 paper they demonstrate empirically that LA can be used to produce Bayesian model averages that are at least at par with existing approaches in terms of uncertainty quantification and out-of-distribution detection and significantly cheaper to compute. They show that recent advancements in autodifferentation can be leveraged to produce fast and accurate approximations of the Hessian and even provide a fully-fledged Python library that can be used with any pretrained Torch model. For this post, I have built a much less comprehensive, pure-play equivalent of their package in Julia - LaplaceRedux.jl can be used with deep learning models built in Flux.jl, which is Julia’s main DL library. As in the previous post on Bayesian logistic regression I will rely on Julia code snippits instead of equations to convey the underlying maths. If you’re curious about the maths, the NeurIPS 2021 paper provides all the detail you need.\n\nFrom Bayesian Logistic Regression …\nLet’s recap: in the case of logistic regression we had a assumed a zero-mean Gaussian prior \\(p(\\mathbf{w}) \\sim \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)=\\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\mathbf{H}_0^{-1} \\right)\\) for the weights that are used to compute logits \\(\\mu_n=\\mathbf{w}^T\\mathbf{x}_n\\), which in turn are fed to a sigmoid function to produce probabilities \\(p(y_n=1)=\\sigma(\\mu_n)\\). We saw that under this assumption solving the logistic regression problem corresponds to minimizing the following differentiable loss function:\n\\[\n\\ell(\\mathbf{w})= - \\sum_{n}^N [y_n \\log \\mu_n + (1-y_n)\\log (1-\\mu_n)] + \\\\ \\frac{1}{2} (\\mathbf{w}-\\mathbf{w}_0)^T\\mathbf{H}_0(\\mathbf{w}-\\mathbf{w}_0)\n\\]\nAs our first step towards Bayesian deep learning, we observe the following: the loss function above corresponds to the objective faced by a single-layer artificial neural network with sigmoid activation and weight decay4. In other words, regularized logistic regression is equivalent to a very simple neural network architecture and hence it is not surprising that underlying concepts can in theory be applied in much the same way.\nSo let’s quickly recap the next core concept: LA relies on the fact that the second-order Taylor expansion of our loss function \\(\\ell\\) evaluated at the maximum a posteriori (MAP) estimate \\(\\mathbf{\\hat{w}}=\\arg\\max_{\\mathbf{w}} p(\\mathbf{w}|\\mathcal{D})\\) amounts to a multi-variate Gaussian distribution. In particular, that Gaussian is centered around the MAP estimate with covariance equal to the inverse Hessian evaluated at the mode \\(\\hat{\\Sigma}=(\\mathbf{H}(\\mathbf{\\hat{w}}))^{-1}\\) (Murphy 2022).\nThat is basically all there is to the story: if we have a good estimate of \\(\\mathbf{H}(\\mathbf{\\hat{w}})\\) we have an analytical expression for an (approximate) posterior over parameters. So let’s go ahead and start by run Bayesian Logistic regression using Flux.jl. We begin by loading some required packages including LaplaceRedux.jl. It ships with a helper function toy_data_linear that creates a toy data set composed of linearly separable samples evenly balanced across the two classes.\n\n# Import libraries.\nusing Flux, Plots, Random, PlotThemes, Statistics, LaplaceRedux\ntheme(:wong)\n# Number of points to generate.\nxs, y = toy_data_linear(100)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y);\n\nThen we proceed to prepare the single-layer neural network with weight decay. The term \\(\\lambda\\) determines the strength of the \\(\\ell2\\) penalty: we regularize parameters \\(\\theta\\) more heavily for higher values. Equivalently, we can say that from the Bayesian perspective it governs the strength of the prior \\(p(\\theta) \\sim \\mathcal{N} \\left( \\theta | \\mathbf{0}, \\sigma_0^2 \\mathbf{I} \\right)= \\mathcal{N} \\left( \\mathbf{w} | \\mathbf{0}, \\lambda_0^{-2} \\mathbf{I} \\right)\\): a higher value of \\(\\lambda\\) indicates a higher conviction about our prior belief that \\(\\theta=\\mathbf{0}\\), which is of course equivalent to regularizing more heavily. The exact choice of \\(\\lambda=0.5\\) for this toy example is somewhat arbitrary (it made for good visualizations below). Note that I have used \\(\\theta\\) to denote our neural parameters to distinguish the case from Bayesian logistic regression, but we are in fact still solving the same problem.\n\nnn = Chain(Dense(2,1))\nλ = 0.5\nsqnorm(x) = sum(abs2, x)\nweight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization();\n\nBefore we apply Laplace approximation we train our model:\n\nusing Flux.Optimise: update!, ADAM\nopt = ADAM()\nepochs = 50\n\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nUp until this point we have just followed the standard recipe for training a regularized artificial neural network in Flux.jl for a simple binary classification task. To compute the Laplace approximation using LaplaceRedux.jl we need just two more lines of code:\n\nla = laplace(nn, λ=λ)\nfit!(la, data);\n\nUnder the hood the Hessian is approximated through the empirical Fisher, which can be computed using only the gradients of our loss function \\(\\nabla_{\\theta}\\ell(f(\\mathbf{x}_n;\\theta,y_n))\\) where \\(\\{\\mathbf{x}_n,y_n\\}\\) are training data (see NeurIPS 2021 paper for details). Finally, LaplaceRedux.jl ships with a function predict(𝑳::LaplaceRedux, X::AbstractArray; link_approx=:probit) that computes the posterior predictive using a probit approximation, much like we saw in the previous post. That function is used under the hood of the plot_contour function below to create the right panel of Figure 1. It visualizes the posterior predictive distribution in the 2D feature space. For comparison I have added the corresponding plugin estimate as well. Note how for the Laplace approximation the predicted probabilities fan out indicating that confidence decreases in regions scarce of data.\n\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin);\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_logit.png\");\n\n\n\n\nFigure 1: Posterior predictive distribution of Logistic regression in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\n\n\n… to Bayesian Neural Networks\nNow let’s step it up a notch: we will repeat the exercise from above, but this time for data that is not linearly separable using a simple MLP instead of the single-layer neural network we used above. The code below is almost the same as above, so I will not go through the various steps again.\n\n# Number of points to generate:\nxs, y = toy_data_non_linear(200)\nX = hcat(xs...); # bring into tabular format\ndata = zip(xs,y)\n\n# Build MLP:\nn_hidden = 32\nD = size(X)[1]\nnn = Chain(\n    Dense(D, n_hidden, σ),\n    Dense(n_hidden, 1)\n)  \nλ = 0.01\nsqnorm(x) = sum(abs2, x)\nweight_regularization(λ=λ) = 1/2 * λ^2 * sum(sqnorm, Flux.params(nn))\nloss(x, y) = Flux.Losses.logitbinarycrossentropy(nn(x), y) + weight_regularization()\n\n# Training:\nepochs = 200\nfor epoch = 1:epochs\n  for d in data\n    gs = gradient(params(nn)) do\n      l = loss(d...)\n    end\n    update!(opt, params(nn), gs)\n  end\nend\n\nFitting the Laplace approximation is also analogous, but note that this we have added an argument: subset_of_weights=:last_layer. This specifies that we only want to use the parameters of the last layer of our MLP. While we could have used all of them (subset_of_weights=:all), Daxberger et al. (2021) find that the last-layer Laplace approximation produces satisfying results, while be computationally cheaper. Figure 2 demonstrates that once again the Laplace approximation yields a posterior predictive distribution that is more conservative than the over-confident plugin estimate.\n\nla = laplace(nn, λ=λ, subset_of_weights=:last_layer)\nfit!(la, data);\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin)\np_laplace = plot_contour(X',y,la;title=\"Laplace\")\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400))\nsavefig(plt, \"www/posterior_predictive_mlp.png\");\n\n\n\n\nFigure 2: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right).\n\n\nTo see why this is a desirable outcome consider the zoomed out version of Figure 2 below: the plugin estimator classifies with full confidence in regions completely scarce of any data. Arguably Laplace approximation produces a much more reasonable picture, even though it too could likely be improved by fine-tuning our choice of \\(\\lambda\\) and the neural network architecture.\n\nzoom=-50\np_plugin = plot_contour(X',y,la;title=\"Plugin\",type=:plugin,zoom=zoom);\np_laplace = plot_contour(X',y,la;title=\"Laplace\",zoom=zoom);\n# Plot the posterior distribution with a contour plot.\nplt = plot(p_plugin, p_laplace, layout=(1,2), size=(1000,400));\nsavefig(plt, \"www/posterior_predictive_mlp_zoom.png\");\n\n\n\n\nFigure 3: Posterior predictive distribution of MLP in the 2D feature space using plugin estimator (left) and Laplace approximation (right). Zoomed out."
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#wrapping-up",
    "title": "Go deep, but also … go Bayesian!",
    "section": "Wrapping up",
    "text": "Wrapping up\nRecent state-of-the-art research on neural information processing suggests that Bayesian deep learning can be effortless: Laplace approximation for deep neural networks appears to work very well and it does so at minimal computational cost (Daxberger et al. 2021). This is great news, because the case for turning Bayesian is strong: society increasingly relies on complex automated decision-making systems that need to be trustworthy. More and more of these systems involve deep learning which in and of itself is not trustworthy. We have seen that typically there exist various viable parameterizations of deep neural networks each with their own distinct and compelling explanation for the data at hand. When faced with many viable options, don’t put all of your eggs in one basket. In other words, go Bayesian!"
  },
  {
    "objectID": "blog/posts/effortsless-bayesian-dl/index.html#resources",
    "href": "blog/posts/effortsless-bayesian-dl/index.html#resources",
    "title": "Go deep, but also … go Bayesian!",
    "section": "Resources",
    "text": "Resources\nTo get started with Bayesian deep learning I have found many useful and free resources online, some of which are listed below:\n\nTuring.jl tutorial on Bayesian deep learning in Julia.\nVarious RStudio AI blog posts including this one and this one.\nTensorFlow blog post on regression with probabilistic layers.\nKevin Murphy’s draft text book, now also available as print."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html",
    "title": "Julia and Quarto: a match made in heaven? 🌤",
    "section": "",
    "text": "Julia and Quarto: a perfect match.\nDoes your work involve research, coding, writing and publishing? If so, then chances are that you often find yourself bouncing back and forth between different open-source text editors, IDEs, programming languages and platforms depending on your current needs. Using a diverse set of tools is reasonable, because there typically is no single perfect approach that solves all our problems. For example, interactive notebooks like Jupyter are useful for working with code and communicating it to others, but they are probably not anyone’s first choice for producing a scientific article. Similarly, Beamer presentations can be useful for presenting science in a standardized fashion, but they are the very opposite of interactive and look incredibly boring.\nAs much as the great variety of free tools deserves being celebrated, all this bouncing back and forth can be really tiring. What if there was a single tool, an engine that can turn your work into all kinds of different outputs? I mean literally any output you can think of: Markdown, HTML, PDF, LateX, ePub, entire websites, presentations (yes, also Beamer if you have to), MS Word, OpenOffice, … the list goes on. All of that starting from the same place: a plain Markdown document blended with essentially any programming language of your choice and a YAML header defining your output. This tool now exists and it goes by the name Quarto.\nIn this short blog post I hope to convince you that Quarto is the only publishing engine you will ever need. What I am definitely not going to tell you is which IDE, text editor or programming language you should be using to actually produce your work. Quarto does not care about that. Quarto is here to make your life a bit easier (and by ‘a bit’ I mean a whole lot). Quarto is nothing less but a revolution for scientific publishing.\nTo put this all in some context (well, my context), I will now tell you a bit about what has led me to making such bold claims about yet another open-source tool.\nYes! But it’s worth noting that a lot of the benefits that Quarto brings have been available to R users for many years, thanks to the amazing work of many great open-source contributors like @xieyihui. Julia was the main reason for me to branch out of this comfortable R bubble as I describe below. That said, if you are a Julia user who really couldn’t care less about my previous experiences with R Markdown, this is a good time to skip straight ahead to Section 2. By the way, if you haven’t clicked on that link, here’s a small showcase demonstrating how it was generated. It shows easy it is to have everything well organised and connected with Quarto."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-bubble",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-bubble",
    "title": "Julia and Quarto: a match made in heaven? 🌤",
    "section": "A comfortable bubble 🎈",
    "text": "A comfortable bubble 🎈\nFor many years I have used R Markdown for essentially anything work-related. As an undergraduate economics student facing the unfortunate reality that people still teach Stata, I was drawn to R. This was partially because R has a great open-source community and also partially because Stata. Once I realised that I would be able to use R Markdown to write up all of my future homework assignments and even my thesis, I never looked back. MS Word was now officially dead to me. Overleaf was nothing more than a last resort if everyone else in my team insisted on using it for a group project. Being able to write my undergraduate dissertation in R Markdown was a first truly triumphant moment. Soon after that I would also try myself at Shiny, produce outputs in HTML and build entire websites through blogdown. And all of that from within R Studio involving R and Markdown and really not much else. During my first professional job at the Bank of England I was reluctant to use anything other than R Markdown to produce all of my output. Luckily for me, the Bank was very much heading in that same direction at the time and my reluctance was not perceived as stubbornness, but actually welcome (at least I hoped so).\n\nCracks in the bubble 🧨\nSoon though, part of me felt a little boxed in. For any work that required me to look outside of the R bubble, I knew I might also have to give up a very, very comfortable work environment and my productivity would surely take a hit. During my master’s in Data Science, for example, the mantra was very much “Python + Jupyter or die”. Through reticulate and R Studio’s growing support for Python I managed to get by without having to leave my bubble too often. But reticulate always felt a little clunky (sorry!) and some professors were reluctant to accept anything other than Jupyter notebooks. Even if others had not perceived it that way in the past, I certainly started to feel that I might just be a little too attached the beautiful bubble that R Studio had created around me.\n\n\nEnter: Julia 💣\nThen there was Julia: elegant, fast, pure, scientific and - oh my REPL! - those beautiful colors and unicode symbols. The stuff of dreams, really! Geeky dreams, but dreams nonetheless. I had once before given Julia a shot when working with high-frequency trade data for a course in market microstructure. This was the first time R really revealed its limitations to me and my bubble nearly burst, but thanks to data.table and Rcpp I managed to escape with only minor bruises. Still, Julia kept popping up, teasing me whenever I would work on some Frakenstein-style C++ code snippets that would hopefully resolve my R bottlenecks. I actually enjoyed mixing some C++ into my R code like I did here, but the process was just a little painful and slow. But wouldn’t learning all of Julia take even more time and patience? And what about my dependence on R Markdown?\n\n\nJulia bursts my bubble 💥\nAs I started my PhD in September 2021, I eventually gave in. New beginnings - time to suck it up! If it meant that I’d have to use Jupyter notebooks with Julia, so be it! And so I was off to a somewhat bumpy start that would have me bouncing back and forth between trying to make Julia work in R Studio (meh), setting up Jupyter Lab (meeeh), just using the Julia REPL because “the REPL is all you need” (nope) and even struggling with Vim and Emacs. Then there was also Pluto.jl, of course, which admittedly looks amazing! But it also looks very much tailored to Julia and (I believe) the number of different output formats you can produce is still very limited. Eventually, I settled for VSCode in combination with Jupyter notebooks. As much as I dreaded the latter, Jupyter is popular, arguably versatile and supports both R and Julia. This setup worked well enough for me, but it still definitely fell short of the breeze that R Studio had always provided. One thing that really bugged me, for example, was the fact that the IJulia kernel was not accessible from the Julia REPL. Each notebook would have its own environment, which could only be accessed through the notebook. In R Studio the interaction between R Markdown and the console is seamless, as both have access to the same environment variables.\n\n\nEnter: Quarto ❤️‍🩹\nAround the same time that I started using Julia, I read about Quarto for the first time. It looked … great! Like a timely little miracle really! But also … unfinished? Definitely experimental at the time. I loved the idea though and in a footnote somewhere on their website it said that the project was supported by R Studio which I took as a very good sign. So I decided to at least give it a quick try and built a small (tiny) website summarising some of the literature I had read for my PhD:\n\n\nJust had my first go #quarto and I absolutely love the concept! Open-source and language agnostic - truly amazing work from &#64rstudio https://t.co/veCg7ywQ8v\n\n— Patrick Altmeyer (&#64paltmey) October 29, 2021\n\n\nThis was a first very pleasant encounter with Quarto, arguable even smoother than building websites in blogdown. As for working with Julia though, I had made up my mind that VSCode was the way to go and at the time there was no Quarto extension (there is now). There was also little in terms of communication about the project by R Studio, probably because things were really still in the early development stages. I was hopeful that eventually Quarto would enable me to emulate the R Studio experience in VS Code, but for now things were not quite there yet.\n\n\nQuarto keeps growing 🤞\nSince I was now working with VSCode + Jupyter and since Quarto supports Jupyter as well as all of my old R Markdown work, my next little Quarto project involved turning my old blogdown-powered blog into a Quarto-powered blog. This was not strictly necessary, as I could always export my new Jupyter notebooks to HTML and let blogdown do the rest. But it did streamline things a little bit and the default Quarto blog theme - you are staring at it - is actually 🔥. I also did not have to feel guilty towards @xieyihui about leaving blogdown, because unsurprisingly he is on the Quarto team. As I was working on this little project I started noticing that the Quarto website was updated regularly and responses to issues I opened like this one were answered very swiftly. Clearly, things were moving and they were moving fast. More recently, the news about Quarto has been spreading and it’s left some folks as confused and amazed as I was, when I first heard about it:\n\n\n#RStats can someone explain to me what's the difference between {Quarto} and {RMarkdown}? I saw a tweet about Quarto and now I'm all confused … What gap is it supposed to fill?\n\n— Erwin Lares (&#64lasrubieras) March 30, 2022\n\n\nThis is why finally I’ve decided I should write a brief post about how and why I use Quarto. Since I have been working mostly with Julia for the past couple of months, I’ve chosen to focus on the interaction between Quarto and Julia. Coincidentally, yesterday was also the first time I saw a guide dedicated to Julia on the Quarto website, so evidently I am not the only one interested in that marriage. This also means that there really is not too much left for me to talk about now, since Quarto’s documentation is state-of-the-art. But a few bits and pieces I mention below might hopefully still be useful or at least some food for thought."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-match",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#sec-match",
    "title": "Julia and Quarto: a match made in heaven? 🌤",
    "section": "Quarto and Julia: a perfect match 💙💜💚",
    "text": "Quarto and Julia: a perfect match 💙💜💚\nWhile what follows may be relevant to other programming languages, my main goal for this last section is to flag Quarto to the Julia community. In any case, #rstats folks have been using R and Python in R Markdown documents for a while now and won’t need much of an introduction to Quarto. As for Python aficionados, I can only recommend to give Quarto a shot (you will still be able to use Jupyter notebooks).\n\nWorking with VSCode, Quarto and Julia\nThe very article you are reading right now was composed in a Quarto document. These documents feel and look very much like standard Julia Markdown documents, but you can do a lot more with them. You can find the source code for this and other documents presented in this blog here.\nTo get you started, here is my current setup combining VSCode, Quarto and Julia:\n\nVSCode extensions: in addition to the Julia extension you will need the Quarto extension. In addition, the YAML extension and some extension to preview Markdown docs would be helpful. I am not sure if Markdown Julia and Jupyter are strictly necessary, but it won’t hurt.\nI do most of my work in Quarto documents .qmd.\nIf you choose to also do that, make sure that the .qmd document has access to a Pkg.jl environment that has IJulia added.\n\nJulia code cells can be added anywhere along with your plain text Markdown. They look like this:\n```{julia}\nusing Pkg\nPkg.add(\"CounterfactualExplanations\")\n```\nContrary to Jupyter notebooks, executing this code cells will start a Julia REPL in VSCode. I find this very helpful, because it lets me fiddle with anything I have created inside the Quarto notebook without having to click into cells all the time. Quarto comes with great support for specifying code executing options. For example, for the code below I have specified #| echo: true in order for the code to be rendered. The code itself is the code I actually used to build the animation above (heavily borrowed from this Javis.jl tutorial).\n\nusing Javis, Animations, Colors\n\nsize = 600\nradius_factor = 0.33\n\nfunction ground(args...)\n    background(\"transparent\")\n    sethue(\"white\")\nend\n\nfunction rotate_anim(idx::Number, total::Number) \n    distance_circle = 0.875\n    steps = collect(range(distance_circle,1-distance_circle,length=total))\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [0, steps[idx]*2π],\n        [sineio()],\n    )\nend\n\ntranslate_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(size*radius_factor, 0)],\n    [sineio()],\n)\n\ntranslate_back_anim = Animation(\n    [0, 1], # must go from 0 to 1\n    [O, Point(-(size*radius_factor), 0)],\n    [sineio()],\n)\n\njulia_colours = Dict(\n    :blue => \"#4063D8\",\n    :green => \"#389826\",\n    :purple => \"#9558b2\",\n    :red => \"#CB3C33\"\n)\ncolour_order = [:red, :purple, :green, :blue]\nn_colours = length(julia_colours)\nfunction color_anim(start_colour::String, quarto_col::String=\"#4b95d0\")\n    Animation(\n        [0, 1], # must go from 0 to 1\n        [Lab(color(start_colour)), Lab(color(quarto_col))],\n        [sineio()],\n    )\nend\n\nvideo = Video(size, size)\n\nframe_starts = 1:10:40\nn_total = 250\nn_frames = 150\nBackground(1:n_total, ground)\n\n# Blob:\nfunction element(; radius = 1)\n    circle(O, radius, :fill) # The 4 is to make the circle not so small\nend\n\n# Cross:\nfunction cross(color=\"black\";orientation=:horizontal)\n    sethue(color)\n    setline(10)\n    if orientation==:horizontal\n        out = line(Point(-size,0),Point(size,0), :stroke)\n    else\n        out = line(Point(0,-size),Point(0,size), :stroke)\n    end\n    return out\nend\n\nfor (i, frame_start) in enumerate(1:10:40)\n\n    # Julia circles:\n    blob = Object(frame_start:n_total, (args...;radius=1) -> element(;radius=radius))\n    act!(blob, Action(1:Int(round(n_frames*0.25)), change(:radius, 1 => 75))) # scale up\n    act!(blob, Action(n_frames:(n_frames+50), change(:radius, 75 => 250))) # scale up further\n    act!(blob, Action(1:30, translate_anim, translate()))\n    act!(blob, Action(31:120, rotate_anim(i, n_colours), rotate_around(Point(-(size*radius_factor), 0))))\n    act!(blob, Action(121:150, translate_back_anim, translate()))\n    act!(blob, Action(1:150, color_anim(julia_colours[colour_order[i]]), sethue()))\n\n    # Quarto cross:\n    cross_h = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:horizontal))\n    cross_v = Object((n_frames+50):n_total, (args...) -> cross(;orientation=:vertical))\nend\n\nrender(\n    video;\n    pathname = joinpath(www_path, \"intro.gif\"),\n)\n\n\n\nWorking with Documenter.jl and Quarto\nAn interesting application of Quarto in the Julia ecosystem is package documentation. This is of course best done using Documenter.jl and fortunately the two play nicely with each other, since both share a common ground (Markdown). Their interaction is perhaps best demonstrated through this Julia library I recently developed: CounterfactualExplanatinos.jl. On there you will find lot of Julia scripts *.jl under src/ and test/, as well as many Markdown .md and Quarto documents .qmd under docs. I wrote the package documentation in the Quarto documents, rendered documents individually through quarto render [doc].qmd and then fed the resulting Markdown documents to Documenter.jl as always.\nBelow is my standard YAML header for those Quarto documents:\nformat: \n  commonmark:\n    variant: -raw_html\n    wrap: none\n    self-contained: true\ncrossref:\n  fig-prefix: Figure\n  tbl-prefix: Table\nbibliography: https://raw.githubusercontent.com/pat-alt/bib/main/bib.bib\noutput: asis\nexecute: \n  echo: true\n  eval: false\njupyter: julia-1.7\nYou can see that it points to Bibtex file I host on another Github repository. This makes it very easy to generate citations and references for the rendered Markdown documents, that also show up in the docs (e.g. here). Unfortunately, cross-referencing only partially works, because it relies on auto-generated HTML and Documenter.jl expects this to be passed in blocks. Choosing variant: -raw_html is only a workaround as I have discussed here. Ideally, Documenter.jl would just accept HTML documents rendered from Quarto, but currently only Markdown documents are accepted by make_docs. Still, if anything this workaround is a nice gimmick that extends the default Documenter.jl functionality, without any hassle involved. Hopefully, this can be improved in the future.\n\n\nUsing Quarto for JuliaCon Proceedings\nAnother very good use-case for Quarto involves actual scientific publications in journals such as JuliaCon Proceedings. The existing submission process is tailored towards reproducibility and actually involves reviews directly on GitHub, which is fantastic. But currently only submissions in TeX format are accepted, which is not so great. Using Quarto would not only streamline this process further, but also open the JuliaCon Proceedings Journal up to publishing content in different output formats. Quarto docs could be used to still render the traditional PDF. But those same documents could also be used to create interactive versions in HTML. Arguably, the entire journal could probably be built through Quarto."
  },
  {
    "objectID": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#wrapping-up",
    "href": "blog/posts/julia-and-quarto-a-match-made-in-heaven/index.html#wrapping-up",
    "title": "Julia and Quarto: a match made in heaven? 🌤",
    "section": "Wrapping up 🎗",
    "text": "Wrapping up 🎗\nIn this post I wanted to demonstrate that Quarto might just be the next revolution in scientific publishing. In particular, I hope I have managed to demonstrate its appeal to the Julia community, which I am proud to be part of now that I have managed to branch out of my old R bubble. Please let me hear your thoughts and comments below!"
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html",
    "href": "blog/posts/conformal-regression/index.html",
    "title": "Prediction Intervals for any Regression Model",
    "section": "",
    "text": "Conformal Prediction intervals for differentcoverage rates. As coverage grows, so doesthe width of the prediction interval.\nThis is the third (and for now final) part of a series of posts that introduce Conformal Prediction in Julia using ConformalPrediction.jl. The first post introduced Conformal Prediction for supervised classification tasks: we learned that conformal classifiers produce set-valued predictions that are guaranteed to include the true label of a new sample with a certain probability. In the second post we applied these ideas to a more hands-on example: we saw how easy it is to use ConformalPrediction.jl to conformalize a Deep Learning image classifier.\nIn this post, we will look at regression models instead, that is supervised learning tasks involving a continuous outcome variable. Regression tasks are as ubiquitous as classification tasks. For example, we might be interested in using a machine learning model to predict house prices or the inflation rate of the Euro or the parameter size of the next large language model. In fact, many readers may be more familiar with regression models than classification, in which case it may also be easier for you to understand Conformal Prediction (CP) in this context."
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#background",
    "href": "blog/posts/conformal-regression/index.html#background",
    "title": "Prediction Intervals for any Regression Model",
    "section": "📖 Background",
    "text": "📖 Background\nBefore we start, let’s briefly recap what CP is all about. Don’t worry, we’re not about to deep-dive into methodology. But just to give you a high-level description upfront:\n\nConformal prediction (a.k.a. conformal inference) is a user-friendly paradigm for creating statistically rigorous uncertainty sets/intervals for the predictions of such models. Critically, the sets are valid in a distribution-free sense: they possess explicit, non-asymptotic guarantees even without distributional assumptions or model assumptions.\n— Angelopoulos and Bates (2021) (arXiv)\n\nIntuitively, CP works under the premise of turning heuristic notions of uncertainty into rigorous uncertainty estimates through repeated sampling or the use of dedicated calibration data.\nIn what follows we will explore what CP can do by going through a standard machine learning workflow using MLJ.jl and ConformalPrediction.jl. There will be less focus on how exactly CP works, but references will point you to additional resources.\n\n\n\n\n\n\nInteractive Version\n\n\n\nThis post is also available as a fully interactive Pluto.jl 🎈 notebook hosted on binder: \nIn my own experience, this may take some time to load, certainly long enough to get yourself a hot beverage ☕ or first read on here. But I promise you that the wait is worth it!"
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#data",
    "href": "blog/posts/conformal-regression/index.html#data",
    "title": "Prediction Intervals for any Regression Model",
    "section": "📈 Data",
    "text": "📈 Data\nMost machine learning workflows start with data. For illustrative purposes we will work with synthetic data. The helper function below can be used to generate some regression data.\n\nfunction get_data(;N=1000, xmax=3.0, noise=0.5, fun::Function=fun(X) = X * sin(X))\n    # Inputs:\n    d = Distributions.Uniform(-xmax, xmax)\n    X = rand(d, N)\n    X = MLJBase.table(reshape(X, :, 1))\n\n    # Outputs:\n    ε = randn(N) .* noise\n    y = @.(fun(X.x1)) + ε\n    y = vec(y)\n    return X, y\nend\n\nFigure 1 illustrates our observations (dots) along with the ground-truth mapping from inputs to outputs (line). We have defined that mapping \\(f: \\mathcal{X} \\mapsto \\mathcal{Y}\\) as follows:\n\nf(X) = X * cos(X)\n\n\n\n\n\n\nFigure 1: Some synthetic regression data. Observations are shown as dots. The ground-truth mapping from inputs to outputs is shown as a dashed line."
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#model-training-using-mlj",
    "href": "blog/posts/conformal-regression/index.html#model-training-using-mlj",
    "title": "Prediction Intervals for any Regression Model",
    "section": "🏋️ Model Training using MLJ",
    "text": "🏋️ Model Training using MLJ\nConformalPrediction.jl is interfaced to MLJ.jl (Blaom et al. 2020): a comprehensive Machine Learning Framework for Julia. MLJ.jl provides a large and growing suite of popular machine learning models that can be used for supervised and unsupervised tasks. Conformal Prediction is a model-agnostic approach to uncertainty quantification, so it can be applied to any common supervised machine learning model.\nThe interface to MLJ.jl therefore seems natural: any (supervised) MLJ.jl model can now be conformalized using ConformalPrediction.jl. By leveraging existing MLJ.jl functionality for common tasks like training, prediction and model evaluation, this package is light-weight and scalable. Now let’s see how all of that works …\nTo start with, let’s split our data into a training and test set:\n\ntrain, test = partition(eachindex(y), 0.4, 0.4, shuffle=true)\n\nNow let’s define a model for our regression task:\n\nModel = @load KNNRegressor pkg = NearestNeighborModels\nmodel = Model()\n\n\n\n\n\n\n\nHave it your way!\n\n\n\nThink this dataset is too simple? Wondering why on earth I’m not using XGBoost for this task? In the interactive version of this post you have full control over the data and the model. Try it out!\n\n\nUsing standard MLJ.jl workflows let us now first train the unconformalized model. We first wrap our model in data:\n\nmach_raw = machine(model, X, y)\n\nThen we fit the machine to the training data:\n\nMLJBase.fit!(mach_raw, rows=train, verbosity=0)\n\nFigure 2 below shows the resulting point predictions for the test data set:\n\n\n\n\n\nFigure 2: Point predictions for our machine learning model.\n\n\n\n\nHow is our model doing? It’s never quite right, of course, since predictions are estimates and therefore uncertain. Let’s see how we can use Conformal Prediction to express that uncertainty."
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#conformalizing-the-model",
    "href": "blog/posts/conformal-regression/index.html#conformalizing-the-model",
    "title": "Prediction Intervals for any Regression Model",
    "section": "🔥 Conformalizing the Model",
    "text": "🔥 Conformalizing the Model\nWe can turn our model into a conformalized model in just one line of code:\n\nconf_model = conformal_model(model)\n\nBy default conformal_model creates an Inductive Conformal Regressor (more on this below) when called on a <:Deterministic model. This behaviour can be changed by using the optional method key argument.\nTo train our conformal model we can once again rely on standard MLJ.jl workflows. We first wrap our model in data:\n\nmach = machine(conf_model, X, y)\n\nThen we fit the machine to the data:\n\nMLJBase.fit!(mach, rows=train, verbosity=0)\n\nNow let us look at the predictions for our test data again. The chart below shows the results for our conformalized model. Predictions from conformal regressors are range-valued: for each new sample the model returns an interval \\((y_{\\text{lb}},y_{\\text{ub}})\\in\\mathcal{Y}\\) that covers the test sample with a user-specified probability \\((1-\\alpha)\\), where \\(\\alpha\\) is the expected error rate. This is known as the marginal coverage guarantee and it is proven to hold under the assumption that training and test data are exchangeable.\n\n\n\n\n\nFigure 3: Prediction intervals for our conformalized machine learning model.\n\n\n\n\nIntuitively, a higher coverage rate leads to larger prediction intervals: since a larger interval covers a larger subspace of \\(\\mathcal{Y}\\), it is more likely to cover the true value.\nI don’t expect you to believe me that the marginal coverage property really holds. In fact, I couldn’t believe it myself when I first learned about it. If you like mathematical proofs, you can find one in this tutorial, for example. If you like convincing yourself through empirical observations, read on below …"
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#evaluation",
    "href": "blog/posts/conformal-regression/index.html#evaluation",
    "title": "Prediction Intervals for any Regression Model",
    "section": "🧐 Evaluation",
    "text": "🧐 Evaluation\nTo verify the marginal coverage property empirically we can look at the empirical coverage rate of our conformal predictor (see Section 3 of the tutorial for details). To this end our package provides a custom performance measure emp_coverage that is compatible with MLJ.jl model evaluation workflows. In particular, we will call evaluate! on our conformal model using emp_coverage as our performance metric. The resulting empirical coverage rate should then be close to the desired level of coverage.\n\nmodel_evaluation =\n    evaluate!(_mach, operation=MLJBase.predict, measure=emp_coverage, verbosity=0)\nprintln(\"Empirical coverage: $(round(model_evaluation.measurement[1], digits=3))\")\nprintln(\"Coverage per fold: $(round.(model_evaluation.per_fold[1], digits=3))\")\n\nEmpirical coverage: 0.902\nCoverage per fold: [0.94, 0.904, 0.874, 0.874, 0.898, 0.922]\n\n\n\n\n\n✅ ✅ ✅ Great! We got an empirical coverage rate that is slightly higher than desired 😁 … but why isn’t it exactly the same?\n\nIn most cases it will be slightly higher than desired, since \\((1-\\alpha)\\) is a lower bound. But note that it can also be slightly lower than desired. That is because the coverage property is “marginal” in the sense that the probability is averaged over the randomness in the data. For most purposes a large enough calibration set size (\\(n>1000\\)) mitigates that randomness enough. Depending on your choices above, the calibration set may be quite small (set to 500), which can lead to coverage slack (see Section 3 in the tutorial).\n\n\n\nSo what’s happening under the hood?\nInductive Conformal Prediction (also referred to as Split Conformal Prediction) broadly speaking works as follows:\n\nPartition the training into a proper training set and a separate calibration set\nTrain the machine learning model on the proper training set.\nUsing some heuristic notion of uncertainty (e.g., absolute error in the regression case), compute nonconformity scores using the calibration data and the fitted model.\nFor the given coverage ratio compute the corresponding quantile of the empirical distribution of nonconformity scores.\nFor the given quantile and test sample \\(X_{\\text{test}}\\), form the corresponding conformal prediction set like so: \\(C(X_{\\text{test}})=\\{y:s(X_{\\text{test}},y) \\le \\hat{q}\\}\\)"
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#recap",
    "href": "blog/posts/conformal-regression/index.html#recap",
    "title": "Prediction Intervals for any Regression Model",
    "section": "🔃 Recap",
    "text": "🔃 Recap\nThis has been a super quick tour of ConformalPrediction.jl. We have seen how the package naturally integrates with MLJ.jl, allowing users to generate rigorous predictive uncertainty estimates for any supervised machine learning model.\n\nAre we done?\nQuite cool, right? Using a single API call we are able to generate rigorous prediction intervals for all kinds of different regression models. Have we just solved predictive uncertainty quantification once and for all? Do we even need to bother with anything else? Conformal Prediction is a very useful tool, but like so many other things, it is not the final answer to all our problems. In fact, let’s see if we can take CP to its limits.\nThe helper function to generate data from above takes an optional argument xmax. By increasing that value, we effectively expand the domain of our input. Let’s do that and see how our conformal model does on this new out-of-domain data.\n\n\n\n\n\nFigure 4: Prediction intervals for our conformalized machine learning model applied to out-of-domain data.\n\n\n\n\n\nWhooooops 🤕 … looks like we’re in trouble: in Figure 4 the prediction intervals do not cover out-of-domain test samples well. What happened here?\n\nBy expanding the domain of out inputs, we have violated the exchangeability assumption. When that assumption is violated, the marginal coverage property does not hold. But do not despair! There are ways to deal with this."
  },
  {
    "objectID": "blog/posts/conformal-regression/index.html#read-on",
    "href": "blog/posts/conformal-regression/index.html#read-on",
    "title": "Prediction Intervals for any Regression Model",
    "section": "📚 Read on",
    "text": "📚 Read on\nIf you are curious to find out more, be sure to read on in the docs. There are also a number of useful resources to learn more about Conformal Prediction, a few of which I have listed below:\n\nA Gentle Introduction to Conformal Prediction and Distribution-Free Uncertainty Quantification by Angelopoulos and Bates (2022).\nAwesome Conformal Prediction repository by Manokhin (2022)\nMAPIE: a comprehensive Python library for conformal prediction.\nMy previous two blog posts.\n\nEnjoy!"
  },
  {
    "objectID": "blog/index.html",
    "href": "blog/index.html",
    "title": "Posts",
    "section": "",
    "text": "Prediction Intervals for any Regression Model\n\n\nConformal Prediction in Julia — Part 3\n\n\nThis third post introduces conformal regression by going through a standard machine learning workflow using MLJ.jl and ConformalPrediction.jl.\n\n\n\n\n\n\nDec 12, 2022\n\n\nPatrick Altmeyer\n\n\n10 min\n\n\n1/3/23, 11:18:34 AM\n\n\n\n\n\n\n  \n\n\n\n\nHow to Conformalize a Deep Image Classifier\n\n\nConformal Prediction in Julia — Part 2\n\n\nA guide demonstrating how to use ConformalPrediction.jl to conformalize a deep image classifier in a few lines of code.\n\n\n\n\n\n\nDec 5, 2022\n\n\nPatrick Altmeyer\n\n\n8 min\n\n\n1/3/23, 11:18:34 AM\n\n\n\n\n\n\n  \n\n\n\n\nA year of using Quarto with Julia\n\n\nTips and tricks for Julia practitioners\n\n\nA short companion post to my presentation on using Quarto with Julia at the 2nd JuliaLang Eindhoven meetup in November, 2022.\n\n\n\n\n\n\nNov 21, 2022\n\n\nPatrick Altmeyer\n\n\n7 min\n\n\n12/5/22, 5:09:24 PM\n\n\n\n\n\n\n  \n\n\n\n\nConformal Prediction in Julia 🟣🔴🟢\n\n\nConformal Prediction in Julia — Part 1\n\n\nA (very) gentle introduction to Conformal Prediction in Julia using my new package ConformalPrediction.jl.\n\n\n\n\n\n\nOct 25, 2022\n\n\nPatrick Altmeyer\n\n\n14 min\n\n\n1/3/23, 11:18:34 AM\n\n\n\n\n\n\n  \n\n\n\n\nA new tool for explainable AI\n\n\nCounterfactual Explanations in Julia — Part I\n\n\nThis post introduces a new Julia package for generating counterfactual explanations. The package can be used to explain machine learning algorithms developed and trained in Julia as well as other popular programming languages like Python and R.\n\n\n\n\n\n\nApr 20, 2022\n\n\nPatrick Altmeyer\n\n\n11 min\n\n\n1/3/23, 11:18:47 AM\n\n\n\n\n\n\n  \n\n\n\n\nJulia and Quarto: a match made in heaven? 🌤\n\n\nA new way to publish science\n\n\nAn opinionated, practical review celebrating the open-source community. I discuss why Quarto is nothing short of revolutionary and how I’ve been using it with Julia.\n\n\n\n\n\n\nApr 7, 2022\n\n\nPatrick Altmeyer\n\n\n15 min\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n  \n\n\n\n\nGo deep, but also … go Bayesian!\n\n\nEffortless Bayesian Deep Learning in Julia — Part I\n\n\nAn introduction to effortless Bayesian deep learning through Laplace approximation coded from scratch in Julia.\n\n\n\n\n\n\nFeb 18, 2022\n\n\nPatrick Altmeyer\n\n\n11 min\n\n\n1/3/23, 11:18:47 AM\n\n\n\n\n\n\n  \n\n\n\n\nBayesian Logistic Regression\n\n\nFrom scratch in Julia Language\n\n\nAn introduction to Bayesian Logistic Regression from the bottom up with examples in Julia language.\n\n\n\n\n\n\nNov 15, 2021\n\n\nPatrick Altmeyer\n\n\n13 min\n\n\n1/3/23, 1:04:32 PM\n\n\n\n\n\n\n  \n\n\n\n\nIndividual recourse for Black Box Models\n\n\nExplained through a tale of 🐱’s and 🐶’s\n\n\nAn introduction to algorithmic recourse and an implementation of a simplified version of REVISE (Joshi et al., 2019). This post was prepared as part of my PhD application.\n\n\n\n\n\n\nApr 27, 2021\n\n\nPatrick Altmeyer\n\n\n10 min\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n  \n\n\n\n\nA peek inside the ‘Black Box’ - interpreting neural networks\n\n\n\n\n\nResearch on explainable AI has recently gained considerable momentum. In this post I explore a Bayesian approach to ex-post explainability of Deep Neural Networks.a\n\n\n\n\n\n\nFeb 7, 2021\n\n\nPatrick Altmeyer\n\n\n10 min\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow I’m building this website in R\n\n\n\n\n\nA small post on how I (used to!) build this website using blogdown.\n\n\n\n\n\n\nFeb 2, 2021\n\n\nPatrick Altmeyer\n\n\n5 min\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome\n\n\n\n\n\nThe obligatory welcome post and a shout-out to Yihui Xie and the folks behind quarto.\n\n\n\n\n\n\nFeb 1, 2021\n\n\nPatrick Altmeyer, Patrick Altmeyer\n\n\n0 min\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/index.html",
    "href": "content/talks/posts/2023-ieee-satml/index.html",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "",
    "text": "In Februarya 2023, I presented our paper “Endogenous Macrodynamics in Algorithmic Recourse” at the first IEEE SaTML conference. You can find the slides here.\n\n\n\nIEEE SaTML Logo\n\n\n\n\n\nCitationBibTeX citation:@online{altmeyer2023,\n  author = {Patrick Altmeyer},\n  title = {Endogenous {Macrodynamics} in {Algorithmic} {Recourse}},\n  date = {23-01-03},\n  url = {https://www.paltmeyer.com/blog//content/talks/posts/2023-ieee-satml},\n  langid = {en}\n}\nFor attribution, please cite this work as:\nPatrick Altmeyer. 23AD. “Endogenous Macrodynamics in Algorithmic\nRecourse.” January 3, 23AD. https://www.paltmeyer.com/blog//content/talks/posts/2023-ieee-satml."
  },
  {
    "objectID": "content/talks/index.html",
    "href": "content/talks/index.html",
    "title": "Talks",
    "section": "",
    "text": "Endogenous Macrodynamics in Algorithmic Recourse\n\n\nIEEE — Secure and Trustworthy Machine Learning\n\n\nSlides for my presentation at the first IEEE SaTML conference in February, 2023.\n\n\n\n\n\n\nJan 3, 2023\n\n\nPatrick Altmeyer\n\n\n1/16/23, 9:44:09 AM\n\n\n\n\n\n\n  \n\n\n\n\nA year of using Quarto with Julia\n\n\nTips and tricks for Julia practitioners\n\n\nSlides for my presentation at the Julia Eindhoven Meetup in November, 2022.\n\n\n\n\n\n\nNov 22, 2022\n\n\nPatrick Altmeyer\n\n\n12/5/22, 5:09:24 PM\n\n\n\n\n\n\n  \n\n\n\n\nExplaining Black-Box Models through Counterfactuals\n\n\nA Gentle Introduction\n\n\nSlides for my presentation at the Bank of England in November, 2022.\n\n\n\n\n\n\nNov 8, 2022\n\n\nPatrick Altmeyer\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n  \n\n\n\n\nExplaining Black-Box Models through Counterfactuals\n\n\nA Gentle Introduction\n\n\nSlides for my presentation at the ING Data Science Community Conference 2022.\n\n\n\n\n\n\nNov 1, 2022\n\n\nPatrick Altmeyer\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\n  \n\n\n\n\nJuliaCon 2022\n\n\nVarious Presentations\n\n\nI gave three different talks at JuliaCon 2022.\n\n\n\n\n\n\nAug 1, 2022\n\n\nPatrick Altmeyer\n\n\n11/14/22, 4:40:55 PM\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#quick-intro",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#quick-intro",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Quick Intro",
    "text": "Quick Intro\n\n\n\nCurrently 2nd year of PhD in Trustworthy Artificial Intelligence at Delft University of Technology.\nWorking on Counterfactual Explanations and Probabilistic Machine Learning with applications in Finance.\nPreviously, educational background in Economics and Finance and two years at the Bank of England.\nEnthusiastic about free open-source software, in particular Julia and Quarto."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#in-a-nutshell",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#in-a-nutshell",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "In a nutshell …",
    "text": "In a nutshell …\n\n[…] we run experiments that simulate the application of recourse in practice using various state-of-the-art counterfactual generators and find that all of them induce substantial domain and model shifts.\n\n\n\nCounterfactual Explanation (CE) explain how inputs into a model need to change for it to produce different outputs.\nCounterfactual Explanations that involve realistic and actionable changes can be used for the purpose of Algorithmic Recourse (AR) to help individuals who face adverse outcomes.\n\n\n\n\n\n\n🎯 Key Contributions\n\n\n\nWe find that the induced shifts are substantial enough to likely impede the applicability of Algorithmic Recourse in some situations.\nFortunately, we find various strategies to mitigate these concerns.\nOur simulation framework for studying recourse dynamics is fast and open-sourced."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#consumer-credit-example",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#consumer-credit-example",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Consumer Credit Example",
    "text": "Consumer Credit Example\n\n\nSuppose Figure 1 relates to an automated decision-making system used by a retail bank to evaluate credit applicants with respect to their creditworthiness.\nAssume that the two features are meaningful in the sense that creditworthiness increases in the south-east direction.\nThen we can think of the outcome in panel (d) as representing a situation where the bank supplies credit to more borrowers (orange), but these borrowers are on average less creditworthy and more of them can be expected to default on their loan.\nThis represents a cost to the retail bank."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#student-admission-example",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#student-admission-example",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Student Admission Example",
    "text": "Student Admission Example\n\n\nSuppose Figure Figure 1 relates to an automated decision-making system used by a university in its student admission process.\nAssume that the two features are meaningful in the sense that the likelihood of students completing their degree increases in the south-east direction.\nThen we can think of the outcome in panel (b) as representing a situation where more students are admitted to university (orange), but they are more likely to fail their degree than students that were admitted in previous years.\nThe university admission committee catches on to this and suspends its efforts to offer Algorithmic Recourse.\nThis represents an opportunity cost to future student applicants, that may have derived utility from being offered recourse."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#algorithmic-recourse",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#algorithmic-recourse",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Algorithmic Recourse",
    "text": "Algorithmic Recourse\n\nEven though […] interpretability is of great importance and should be pursued, explanations can, in principle, be offered without opening the “black box”. (Wachter, Mittelstadt, and Russell 2017)\n\n\n\nFramework\n\nObjective originally proposed by Wachter, Mittelstadt, and Russell (2017) is as follows\n\\[\n\\min_{x^\\prime \\in \\mathcal{X}} \\text{cost}(x^\\prime) \\ \\ \\ \\mbox{s. t.} \\ \\ \\ M(x^\\prime) = y^*\n\\qquad(1)\\]\nTypically this is approximated through regularization:\n\\[\nx^\\prime = \\arg \\min_{x^\\prime}  \\text{yloss}(M(x^\\prime),y^*) + \\lambda \\text{cost}(x^\\prime)\n\\qquad(2)\\]\n\n\nIntuition\n\n\n\n\nFigure 2: A cat performing gradient descent in the feature space à la Wachter, Mittelstadt, and Russell (2017)."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#method-general",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#method-general",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "From individual recourse …",
    "text": "From individual recourse …\n\nWe include the following generators in our simulation experiments below: REVISE (Joshi et al. 2019), CLUE (Antorán et al. 2020), DiCE (Mothilal, Sharma, and Tan 2020) and a Greedy approach that relies on probabilistic models (Schut et al. 2021).\n\n\n\nAll of them can be described by the following generalized form of Equation 2:\n\n\\[\n\\begin{aligned}\n\\mathbf{s}^\\prime &= \\arg \\min_{\\mathbf{s}^\\prime \\in \\mathcal{S}} \\left\\{  {\\text{yloss}(M(f(\\mathbf{s}^\\prime)),y^*)}+ \\lambda {\\text{cost}(f(\\mathbf{s}^\\prime)) }  \\right\\}\n\\end{aligned}\n\\qquad(3)\\]\n\n\n\nHere \\(\\mathbf{s}^\\prime=\\left\\{s_k^\\prime\\right\\}_K\\) is a \\(K\\)-dimensional array of counterfactual states and \\(f: \\mathcal{S} \\mapsto \\mathcal{X}\\) maps from the counterfactual state space to the feature space.\n\n\n\n\n\n\nFigure 3: Feature space (left) and counterfactual state space (right)."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#method-collective",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#method-collective",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "… towards collective recourse",
    "text": "… towards collective recourse\n\nAll of the different approaches introduced above tackle the problem of Algorithmic Recourse from the perspective of one single individual.\n\n\n\nWe propose to extend Equation Equation 3 as follows:\n\n\\[\n\\begin{aligned}\n\\mathbf{s}^\\prime &= \\arg \\min_{\\mathbf{s}^\\prime \\in \\mathcal{S}} \\{ {\\text{yloss}(M(f(\\mathbf{s}^\\prime)),y^*)} \\\\ &+ \\lambda_1 {\\text{cost}(f(\\mathbf{s}^\\prime))} + \\lambda_2 {\\text{extcost}(f(\\mathbf{s}^\\prime))} \\}  \n\\end{aligned}\n\\qquad(4)\\]\n\n\n\nHere \\(\\text{cost}(f(\\mathbf{s}^\\prime))\\) denotes the proxy for private costs faced by the individual; the newly introduced term \\(\\text{extcost}(f(\\mathbf{s}^\\prime))\\) is meant to capture external costs generated by changes to \\(\\mathbf{s}^\\prime\\).\n\n\n\n\n\n\nNegative Externalities\n\n\nThe underlying concept of private and external costs is borrowed from Economics and well-established in that field: when the decisions or actions by some individual market participant generate external costs, then the market is said to suffer from negative externalities and is considered inefficient (Pindyck and Rubinfeld 2014)."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#research-questions",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#research-questions",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Research Questions",
    "text": "Research Questions\nPrincipal Concerns\n\nRQ 1 (Endogenous Shifts) Does the repeated implementation of recourse provided by state-of-the-art generators lead to shifts in the domain and model?\n\n\nRQ 2 (Costs) If so, are these dynamics substantial enough to be considered costly to stakeholders involved in real-world automated decision-making processes?\n\n\nRQ 3 (Heterogeneity) Do different counterfactual generators yield significantly different outcomes in this context? Furthermore, is there any heterogeneity concerning the chosen classifier and dataset?\n\n\nRQ 4 (Drivers) What are the drivers of endogenous dynamics in Algorithmic Recourse?\n\nSecondary Concerns\n\nRQ 5 (Mitigation Strategies) What are potential mitigation strategies with respect to endogenous macrodynamics in AR?"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#method-2-experiment",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#method-2-experiment",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Simulations",
    "text": "Simulations"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#method-2-metrics",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#method-2-metrics",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Evaluation Metrics",
    "text": "Evaluation Metrics"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-classifiers",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-classifiers",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Classifiers and Generative Models",
    "text": "Classifiers and Generative Models\nClassifiers\n\nSimple linear classifier—Logistic Regression.\nMultilayer perceptron—MLP.\nDeep Ensemble composed of five MLPs following Lakshminarayanan, Pritzel, and Blundell (2016).\n\nGenerative Models\nDifferent specifications of a plain-vanilla Variational Autoencoder (VAE)\n\n\n\n\nTable 1: Model parameters.\n\n\nModel\nData\nHidden Dim.\nLatent Dim.\nHidden Layers\nBatch\nDropout\nEpochs\n\n\n\n\nMLP\nSynthetic\n32\n-\n1\n-\n-\n100\n\n\nMLP\nReal-World\n64\n-\n2\n500\n0.1\n100\n\n\nVAE\nSynthetic\n32\n2\n1\n-\n-\n100\n\n\nVAE\nReal-World\n32\n8\n1\n-\n-\n250"
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-data",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-data",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Data",
    "text": "Data\nSynthetic Data\nWe use four synthetic binary classification datasets consisting of 1000 samples each: Overlapping, Linearly Separable, Circles and Moons Figure 4.\n\nFigure 4: Synthetic classification datasets used in our experiments. Samples from the negative class (\\(y=0\\)) are marked in blue while samples of the positive class (\\(y=1\\)) are marked in orange.Real-World Data\nWe use three different real-world datasets from the Finance and Economics domain, all of which are tabular and can be used for binary classification.\n\nThe Give Me Some Credit dataset: predict whether a borrower is likely to experience financial difficulties in the next two years (Kaggle 2011).\nThe UCI defaultCredit dataset (Yeh and Lien 2009): a benchmark dataset that can be used to train binary classifiers to predict the whether credit card clients default on their payment.\nThe California Housing dataset Pace and Barry (1997): continuous outcome variable binarized as \\(\\tilde{y}=\\mathbb{I}_{y>\\text{median}(Y)}\\) indicating if the median house price of a given district is above the median of all districts."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#ggenerators",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#ggenerators",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "\\(G\\)—Generators",
    "text": "\\(G\\)—Generators\n\nAll generators introduced earlier are included in the experiments: Wachter (Wachter, Mittelstadt, and Russell 2017), REVISE (Joshi et al. 2019), CLUE (Antorán et al. 2020), DiCE (Mothilal, Sharma, and Tan 2020) and Greedy (Schut et al. 2021).\nIn addition, we introduce two new generators in Section @ref(mitigate) that directly address the issue of endogenous domain and model shifts. We also test to what extent it may be beneficial to combine ideas underlying the various generators."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#more-resources",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#more-resources",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "More Resources 📚",
    "text": "More Resources 📚\n\n\n\nRead on …\n\n\nGranular results for all of our experiments can be found in this online companion: https://www.paltmeyer.com/endogenous-macrodynamics-in-algorithmic-recourse/.\nBlog post introducing Counterfactual Explanations: [TDS, homepage].\n\n\n… or get busy 🖥️\n\n\nCounterfactualExplanations.jl (Altmeyer 2022) provides an extensible, fast and language-agnostic implementation in Julia.\nWe have built a framework that extends the functionality from static benchmarks to simulation experiments: AlgorithmicRecourseDynamics.jl.\nThe Github repository containing all the code used to produce the results in this paper can be found here."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#image-sources",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#image-sources",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Image Sources",
    "text": "Image Sources\n\nCopyright for stock images belongs to TU Delft.\nAll other images, graphics or animations were created by us."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#references",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#references",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "References",
    "text": "References\n\n\nIEEE Conference on Secure and Trustworthy Machine Learning ’23 — Patrick Altmeyer — CC BY-NC\n\n\n\nAltmeyer, Patrick. 2022. “CounterfactualExplanations.jl - a Julia Package for Counterfactual Explanations and Algorithmic Recourse.” https://github.com/pat-alt/CounterfactualExplanations.jl.\n\n\nAntorán, Javier, Umang Bhatt, Tameem Adel, Adrian Weller, and José Miguel Hernández-Lobato. 2020. “Getting a Clue: A Method for Explaining Uncertainty Estimates.” https://arxiv.org/abs/2006.06848.\n\n\nBorisov, Vadim, Tobias Leemann, Kathrin Seßler, Johannes Haug, Martin Pawelczyk, and Gjergji Kasneci. 2021. “Deep Neural Networks and Tabular Data: A Survey.” https://arxiv.org/abs/2110.01889.\n\n\nGrinsztajn, Léo, Edouard Oyallon, and Gaël Varoquaux. 2022. “Why Do Tree-Based Models Still Outperform Deep Learning on Tabular Data?” https://arxiv.org/abs/2207.08815.\n\n\nHanneke, Steve. 2007. “A Bound on the Label Complexity of Agnostic Active Learning.” In Proceedings of the 24th International Conference on Machine Learning, 353–60.\n\n\nJoshi, Shalmali, Oluwasanmi Koyejo, Warut Vijitbenjaronk, Been Kim, and Joydeep Ghosh. 2019. “Towards Realistic Individual Recourse and Actionable Explanations in Black-Box Decision Making Systems.” https://arxiv.org/abs/1907.09615.\n\n\nKaggle. 2011. “Give Me Some Credit, Improve on the State of the Art in Credit Scoring by Predicting the Probability That Somebody Will Experience Financial Distress in the Next Two Years.” Kaggle. https://www.kaggle.com/c/GiveMeSomeCredit.\n\n\nKarimi, Amir-Hossein, Bernhard Schölkopf, and Isabel Valera. 2021. “Algorithmic Recourse: From Counterfactual Explanations to Interventions.” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 353–62.\n\n\nLakshminarayanan, Balaji, Alexander Pritzel, and Charles Blundell. 2016. “Simple and Scalable Predictive Uncertainty Estimation Using Deep Ensembles.” https://arxiv.org/abs/1612.01474.\n\n\nMothilal, Ramaravind K, Amit Sharma, and Chenhao Tan. 2020. “Explaining Machine Learning Classifiers Through Diverse Counterfactual Explanations.” In Proceedings of the 2020 Conference on Fairness, Accountability, and Transparency, 607–17.\n\n\nPace, R Kelley, and Ronald Barry. 1997. “Sparse Spatial Autoregressions.” Statistics & Probability Letters 33 (3): 291–97.\n\n\nPedregosa, Fabian, Gaël Varoquaux, Alexandre Gramfort, Vincent Michel, Bertrand Thirion, Olivier Grisel, Mathieu Blondel, et al. 2011. “Scikit-Learn: Machine Learning in Python.” The Journal of Machine Learning Research 12: 2825–30.\n\n\nPindyck, Robert S, and Daniel L Rubinfeld. 2014. Microeconomics. Pearson Education.\n\n\nSchut, Lisa, Oscar Key, Rory Mc Grath, Luca Costabello, Bogdan Sacaleanu, Yarin Gal, et al. 2021. “Generating Interpretable Counterfactual Explanations By Implicit Minimisation of Epistemic and Aleatoric Uncertainties.” In International Conference on Artificial Intelligence and Statistics, 1756–64. PMLR.\n\n\nUpadhyay, Sohini, Shalmali Joshi, and Himabindu Lakkaraju. 2021. “Towards Robust and Reliable Algorithmic Recourse.” https://arxiv.org/abs/2102.13620.\n\n\nWachter, Sandra, Brent Mittelstadt, and Chris Russell. 2017. “Counterfactual Explanations Without Opening the Black Box: Automated Decisions and the GDPR.” Harv. JL & Tech. 31: 841.\n\n\nYeh, I-Cheng, and Che-hui Lien. 2009. “The Comparisons of Data Mining Techniques for the Predictive Accuracy of Probability of Default of Credit Card Clients.” Expert Systems with Applications 36 (2): 2473–80."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "",
    "text": "Currently 2nd year of PhD in Trustworthy Artificial Intelligence at Delft University of Technology.\nWorking on Counterfactual Explanations and Probabilistic Machine Learning with applications in Finance.\nPreviously, educational background in Economics and Finance and two years at the Bank of England.\nEnthusiastic about free open-source software, in particular Julia and Quarto."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#illustration",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#illustration",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Illustration",
    "text": "Illustration\n\nFigure 1: Dynamics in Algorithmic Recourse: (a) we have a simple linear classifier trained for binary classification where samples from the negative class (\\(y=0\\)) are marked in blue and samples of the positive class (\\(y=1\\)) are marked in orange; (b) the implementation of AR for a random subset of individuals leads to a noticable domain shift; (c) as the classifier is retrained we observe a corresponding model shift; (d) as this process is repeated, the decision boundary moves away from the target class.\nConsumer Credit ExampleStudent Admission Example\n\n\n\n\nSuppose Figure 1 relates to an automated decision-making system used by a retail bank to evaluate credit applicants with respect to their creditworthiness.\nAssume that the two features are meaningful in the sense that creditworthiness increases in the south-east direction.\nThen we can think of the outcome in panel (d) as representing a situation where the bank supplies credit to more borrowers (orange), but these borrowers are on average less creditworthy and more of them can be expected to default on their loan.\nThis represents a cost to the retail bank.\n\n\n\n\n\n\nSuppose Figure Figure 1 relates to an automated decision-making system used by a university in its student admission process.\nAssume that the two features are meaningful in the sense that the likelihood of students completing their degree increases in the south-east direction.\nThen we can think of the outcome in panel (b) as representing a situation where more students are admitted to university (orange), but they are more likely to fail their degree than students that were admitted in previous years.\nThe university admission committee catches on to this and suspends its efforts to offer Algorithmic Recourse.\nThis represents an opportunity cost to future student applicants, that may have derived utility from being offered recourse."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#proof-of-concept",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#proof-of-concept",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Proof-of-Concept",
    "text": "Proof-of-Concept\n\n\n\nExample 1 (Consumer Credit)  \n\nSuppose Figure 1 relates to an automated decision-making system used by a retail bank to evaluate credit applicants.\nCreditworthiness increases in the South-East direction.\nOutcome: bank supplies credit to more borrowers (orange), but these borrowers are riskier on average, which represents a cost to the retail bank.\n\n\n\nExample 2 (Student Admission)  \n\nSuppose Figure 1 relates to an automated decision-making system used by a university in its student admission process.\nLikelihood of students completing their degree increases in the South-East direction.\nOutcome: more students are admitted to university (orange), but they are more likely to fail their degree; the university suspends its efforts to offer AR, which represents a cost to future applicants.\n\n\n\n\n\n\nFigure 1: Dynamics in Algorithmic Recourse: (a) we have a simple linear classifier trained for binary classification where samples from the negative class (\\(y=0\\)) are marked in blue and samples of the positive class (\\(y=1\\)) are marked in orange; (b) the implementation of AR for a random subset of individuals leads to a noticable domain shift; (c) as the classifier is retrained we observe a corresponding model shift; (d) as this process is repeated, the decision boundary moves away from the target class."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#generators",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#generators",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Generators",
    "text": "Generators\n\nAll generators introduced earlier are included in the experiments: Wachter (Wachter, Mittelstadt, and Russell 2017), REVISE (Joshi et al. 2019), CLUE (Antorán et al. 2020), DiCE (Mothilal, Sharma, and Tan 2020) and Greedy (Schut et al. 2021).\nIn addition, we introduce two new generators that directly address the issue of endogenous domain and model shifts. We also test to what extent it may be beneficial to combine ideas underlying the various generators."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-2",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-2",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Initial Findings",
    "text": "Initial Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-1",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-1",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Mitigation Strategies",
    "text": "Mitigation Strategies\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-results",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-results",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Mitigation Strategies — Results",
    "text": "Mitigation Strategies — Results\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#initial-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#initial-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Initial Findings",
    "text": "Initial Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#mitigation-strategies-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Mitigation Strategies — Findings",
    "text": "Mitigation Strategies — Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#initial-results",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#initial-results",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Initial Results",
    "text": "Initial Results\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#research-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#research-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Research Findings",
    "text": "Research Findings\nPrincipal Concerns\n\nFirstly, endogenous dynamics do emerge in our experiments (Proposition 1) and we find them substantial enough to be considered costly (Proposition 2)\nSecondly, the choice of the counterfactual generator matters, with Latent Space search generally having a dampening effect (Proposition 3).\nThe observed dynamics, therefore, seem to be driven by a discrepancy between counterfactual outcomes that minimize costs to individuals and outcomes that comply with the data-generating process (Proposition 4).\n\nSecondary Concerns\n\nOur findings indicate that all three proposed mitigation strategies are at least at par with LS generators (Proposition 5)."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#limitations",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#limitations",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Limitations",
    "text": "Limitations\n\nPrivate vs. External Costs\n\nWe fall short of providing any definitive answers as to how to trade off private vs. external costs.\nProposed strategies are a good starting point, but they are ad-hoc.\n\nExperimental Setup\n\nExperimental design is a vast over-simplification of potential real-world scenarios.\n\nCausal Modelling\n\nHave focused on popular counterfactual generators that do not incorporate any causal knowledge.\nPerturbations therefore may involve changes to variables that affect the outcome predicted by the black-box model, but not the true, causal outcome.\nFuture work would likely benefit from including recent approaches to AR that incorporate causal knowledge such Karimi, Schölkopf, and Valera (2021).\n\nClassifiers\n\nWe have limited our analysis to differentiable linear and non-linear classifiers; empirical evidence suggests that other models such as boosted decision trees outperform DL on tabular data (Borisov et al. (2021), Grinsztajn, Oyallon, and Varoquaux (2022))."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#main-result",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#main-result",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Main Result",
    "text": "Main Result\n\n\nOur findings indicate that state-of-the-art approaches to Algorithmic Recourse induce substantial domain and model shifts.\n\n\n\n\nWe would argue that the expected external costs of individual recourse should be shared by all stakeholders.\n\n\n\n\nA straightforward way to achieve this is to penalize external costs in the counterfactual search objective function (Equation 4).\n\n\n\n\nVarious simple strategies based on this notion can be effectively used to mitigate shifts."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#key-takeaways",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#key-takeaways",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Key Takeaways 🔑",
    "text": "Key Takeaways 🔑\n\n\nOur findings indicate that state-of-the-art approaches to Algorithmic Recourse induce substantial domain and model shifts.\n\n\n\n\nWe would argue that the expected external costs of individual recourse should be shared by all stakeholders.\n\n\n\n\nA straightforward way to achieve this is to penalize external costs in the counterfactual search objective function (Equation 4).\n\n\n\n\nVarious simple strategies based on this notion can be effectively used to mitigate shifts."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#setup",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#setup",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Setup",
    "text": "Setup\n\nEvaluation MetricsModelsDataGenerators\n\n\n\n\nDomain Shifts\n\nMaximum Mean Discrepancy (MMD)\n\n\nModel Shifts\n\nPerturbations\nPredicted Probability MMD (PP MMD)\nDisagreement Coefficient\nDecisiveness\nPerformance\n\n\n\n\n\nClassifiers\n\nSimple linear classifier—Logistic Regression.\nMultilayer perceptron—MLP.\nDeep Ensemble composed of five MLPs following Lakshminarayanan, Pritzel, and Blundell (2016).\n\nGenerative Models\nDifferent specifications of a plain-vanilla Variational Autoencoder (VAE)\n\n\n\nModel parameters. {#tbl-models}\n\n\nModel\nData\nHidden Dim.\nLatent Dim.\nHidden Layers\nBatch\nDropout\nEpochs\n\n\n\n\nMLP\nSynthetic\n32\n-\n1\n-\n-\n100\n\n\nMLP\nReal-World\n64\n-\n2\n500\n0.1\n100\n\n\nVAE\nSynthetic\n32\n2\n1\n-\n-\n100\n\n\nVAE\nReal-World\n32\n8\n1\n-\n-\n250\n\n\n\n\n\n\n\n\n\nSynthetic Data\nFour synthetic binary classification datasets consisting of 1000 samples each: Overlapping, Linearly Separable, Circles and Moons (Figure 4).\n\n\n\nFigure 4: Synthetic classification datasets used in our experiments. Samples from the negative class (\\(y=0\\)) are marked in blue while samples of the positive class (\\(y=1\\)) are marked in orange.\n\n\n\nReal-World Data\nThree real-world datasets from the Finance and Economics domain: all tabular and can be used for binary classification.\n\nThe Give Me Some Credit dataset: predict whether a borrower is likely to experience financial difficulties in the next two years (Kaggle 2011).\nThe UCI defaultCredit dataset (Yeh and Lien 2009): a benchmark dataset that can be used to train binary classifiers to predict the whether credit card clients default on their payment.\nThe California Housing dataset Pace and Barry (1997): continuous outcome variable binarized as \\(\\tilde{y}=\\mathbb{I}_{y>\\text{median}(Y)}\\) indicating if the median house price of a given district is above the median of all districts.\n\n\n\n\n\n\nAll generators introduced earlier are included in the experiments: Wachter (Wachter, Mittelstadt, and Russell 2017), REVISE (Joshi et al. 2019), CLUE (Antorán et al. 2020), DiCE (Mothilal, Sharma, and Tan 2020) and Greedy (Schut et al. 2021).\nIn addition, we introduce two new generators that directly address the issue of endogenous domain and model shifts. We also test to what extent it may be beneficial to combine ideas underlying the various generators.\n\n\n\n\nMitigation strategies."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-setup",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#empirical-setup",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Empirical Setup",
    "text": "Empirical Setup\n\nEvaluation MetricsModelsDataGenerators\n\n\n\n\nDomain Shifts\n\nMaximum Mean Discrepancy (MMD): a measure of the distance between the kernel mean embeddings of two samples; in our context, large values indicate that a domain shift indeed seems to have occurred.\n\n\nModel Shifts\n\nPerturbations: following Upadhyay, Joshi, and Lakkaraju (2021) we define \\(\\Delta=||\\theta_{t+1}-\\theta_{t}||^2\\), that is the euclidean distance between the vectors of parameters before and after retraining the model \\(M\\).\nPredicted Probability MMD (PP MMD): instead of applying MMD to features directly, we apply it to the predicted probabilities assigned to a set of samples by the model \\(M\\).\nDisagreement Coefficient: this metric was introduced in Hanneke (2007) and estimates \\(p(M(x) \\neq M^\\prime(x))\\), that is the probability that two classifiers disagree on the predicted outcome for a randomly chosen sample.\nDecisiveness: we define the metric simply as \\({\\frac{1}{N}}\\sum_{i=0}^N(\\sigma(M(x)) - 0.5)^2\\) where \\(M(x)\\) are predicted logits from a binary classifier and \\(\\sigma\\) denotes the sigmoid function; it quantifies the likelihood that a model assigns a high probability to its classification of any given sample.\nPerformance: we compute the classifier’s F-score on a test sample that we leave untouched throughout the experiment.\n\n\n\n\n\nClassifiers\n\nSimple linear classifier—Logistic Regression.\nMultilayer perceptron—MLP.\nDeep Ensemble composed of five MLPs following Lakshminarayanan, Pritzel, and Blundell (2016).\n\nGenerative Models\nDifferent specifications of a plain-vanilla Variational Autoencoder (VAE)\n\n\n\nModel parameters. {#tbl-models}\n\n\nModel\nData\nHidden Dim.\nLatent Dim.\nHidden Layers\nBatch\nDropout\nEpochs\n\n\n\n\nMLP\nSynthetic\n32\n-\n1\n-\n-\n100\n\n\nMLP\nReal-World\n64\n-\n2\n500\n0.1\n100\n\n\nVAE\nSynthetic\n32\n2\n1\n-\n-\n100\n\n\nVAE\nReal-World\n32\n8\n1\n-\n-\n250\n\n\n\n\n\n\n\n\n\nSynthetic Data\nFour synthetic binary classification datasets consisting of 1000 samples each: Overlapping, Linearly Separable, Circles and Moons (Figure 4).\n\n\n\nFigure 4: Synthetic classification datasets used in our experiments. Samples from the negative class (\\(y=0\\)) are marked in blue while samples of the positive class (\\(y=1\\)) are marked in orange.\n\n\n\nReal-World Data\nThree real-world datasets from the Finance and Economics domain: all tabular and can be used for binary classification.\n\nThe Give Me Some Credit dataset: predict whether a borrower is likely to experience financial difficulties in the next two years (Kaggle 2011).\nThe UCI defaultCredit dataset (Yeh and Lien 2009): a benchmark dataset that can be used to train binary classifiers to predict the whether credit card clients default on their payment.\nThe California Housing dataset Pace and Barry (1997): continuous outcome variable binarized as \\(\\tilde{y}=\\mathbb{I}_{y>\\text{median}(Y)}\\) indicating if the median house price of a given district is above the median of all districts.\n\n\n\n\n\n\n\nPrimary Concerns\n\nWachter (Wachter, Mittelstadt, and Russell 2017)\nREVISE (Joshi et al. 2019)\nCLUE (Antorán et al. 2020)\nDiCE (Mothilal, Sharma, and Tan 2020)\nGreedy (Schut et al. 2021)\n\n\nSecondary Concerns\n\nMore Conservative Decision Thresholds\nClassifier Preserving ROAR (ClaPROAR):\n\n\\[\n\\begin{aligned}\n\\text{extcost}(f(\\mathbf{s}^\\prime)) = l(M(f(\\mathbf{s}^\\prime)),y^\\prime)\n\\end{aligned}\n\\qquad(5)\\]\n\nGravitational Counterfactual Explanations:\n\n\\[\n\\begin{aligned}\n\\text{extcost}(f(\\mathbf{s}^\\prime)) = \\text{dist}(f(\\mathbf{s}^\\prime),\\bar{x}^*)\n\\end{aligned}\n\\qquad(6)\\]\n\n\n\nMitigation strategies."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#primary-concerns-1",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#primary-concerns-1",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Primary Concerns",
    "text": "Primary Concerns\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#secondary-concerns-2",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#secondary-concerns-2",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Secondary Concerns",
    "text": "Secondary Concerns\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#primary-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#primary-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Primary Findings",
    "text": "Primary Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#secondary-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#secondary-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Secondary Findings",
    "text": "Secondary Findings\n\n\n\n\n\nResults for synthetic data.\n\n\n\n\n\n\nResults for real-world data."
  },
  {
    "objectID": "content/talks/posts/2023-ieee-satml/presentation.html#summary-of-findings",
    "href": "content/talks/posts/2023-ieee-satml/presentation.html#summary-of-findings",
    "title": "Endogenous Macrodynamics in Algorithmic Recourse",
    "section": "Summary of Findings",
    "text": "Summary of Findings\nPrincipal Concerns\n\nFirstly, endogenous dynamics do emerge in our experiments (Proposition 1) and we find them substantial enough to be considered costly (Proposition 2)\nSecondly, the choice of the counterfactual generator matters, with Latent Space search generally having a dampening effect (Proposition 3).\nThe observed dynamics, therefore, seem to be driven by a discrepancy between counterfactual outcomes that minimize costs to individuals and outcomes that comply with the data-generating process (Proposition 4).\n\nSecondary Concerns\n\nOur findings indicate that all three proposed mitigation strategies are at least at par with LS generators (Proposition 5)."
  }
]