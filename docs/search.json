[
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#motivation",
    "href": "content/talks/posts/2024-econdat/presentation.html#motivation",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Motivation",
    "text": "Motivation\n\n\n\n\\(A_1\\): „It is essential to bring inflation back to target to avoid drifting into deflation territory.“\n\\(A_2\\): „It is essential to bring the numbers of doves back to target to avoid drifting into dovelation territory.“\n\n\n“They’re exactly the same.”\n— Linear probe \\(\\widehat{cpi}=f(A)\\)"
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#position",
    "href": "content/talks/posts/2024-econdat/presentation.html#position",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Position",
    "text": "Position\n\nCurrent LLMs embed knowledge. They don‘t „understand“ anything. They are useful tools, but tools nonetheless.\n\n\n\nMeaningful patterns in embeddings are like doves in the sky.\nHumans are prone to seek patterns and anthropomorphize.\nObserved ‘sparks’ of Artificial General Intelligence are spurious.\nThe academic community should exercise extra caution.\nPublishing incentives need to be adjusted."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#outline",
    "href": "content/talks/posts/2024-econdat/presentation.html#outline",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Outline",
    "text": "Outline\n\n\nExperiments: We probe models of varying complexity including random projections, matrix decompositions, deep autoencoders and transformers.\n\nAll of them successfully distill knowledge and yet none of them develop true understanding.\n\nSocial sciences review: Humans are prone to seek patterns and anthropomorphize.\nConclusion and outlook: More caution at the individual level, and different incentives at the institutional level."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#the-holy-grail",
    "href": "content/talks/posts/2024-econdat/presentation.html#the-holy-grail",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "The Holy Grail",
    "text": "The Holy Grail\nAchievement of Artificial General Intelligence (AGI) has become a grand challenge, and in some cases, an explicit business goal.\n\n\nDefinition\nThe definition of AGI itself is not as clear-cut or consistent:\n\n(loosely) a phenomenon contrasting with ‘narrow AI’ systems, that were trained for specific tasks (Goertzel 2014).\n\n\nPractice\nResearchers have sought to show that AI models generalize to different (and possibly unseen) tasks or show performance considered ‘surprising’ to humans.\n\nFor example, Google DeepMind claimed their AlphaGeometry model (Trinh et al. 2024) reached a ‘milestone’ towards AGI."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#a-perfect-storm",
    "href": "content/talks/posts/2024-econdat/presentation.html#a-perfect-storm",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "A Perfect Storm",
    "text": "A Perfect Storm\nRecent developments in the field have created a ‘perfect storm’ for inflated claims:\n\n\nEarly sharing of preprints and code.\nVolume of publishable work has exploded.\nSocial media influencers start playing a role in article discovery and citeability (Weissburg et al. 2024).\nComplexity is increasing because it is incentivized (Birhane et al. 2022)."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#not-mere-stochastic-parrots",
    "href": "content/talks/posts/2024-econdat/presentation.html#not-mere-stochastic-parrots",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "“Not Mere Stochastic Parrots”",
    "text": "“Not Mere Stochastic Parrots”\n\nWe consider a recently viral work (Gurnee and Tegmark 2023a), in which claims about the learning of world models by LLMs were made.\n\nLinear probes (ridge regression) were successfully used to predict geographical locations from LLM embeddings.\n\nClaims on X that this indicates that LLMs are not mere ‘stochastic parrots’ (Bender et al. 2021).\nReactions on X seemed to largely exhibit excitement and surprise at the authors’ findings."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#are-neural-networks-born-with-world-models",
    "href": "content/talks/posts/2024-econdat/presentation.html#are-neural-networks-born-with-world-models",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Are Neural Networks Born with World Models?",
    "text": "Are Neural Networks Born with World Models?\n\n\n\nLlama-2 model tested in Gurnee and Tegmark (2023b) has ingested huge amounts of publicly available data (Touvron et al. 2023).\n\nGeographical locations are literally in the training data: e.g. Wikipedia article for “London”.\nWhere would this information be encoded if not in the embedding space \\(\\mathcal{A}\\)? Is it surprising that \\(A_{\\text{LDN}}=enc(\\text{\"London\"}) \\not\\!\\perp\\!\\!\\!\\perp (\\text{lat}_{\\text{LDN}},\\text{long}_{\\text{LDN}})\\)?\n\nFigure 1 shows the predicted coordinates of a linear probe on the final-layer activations of an untrained neural network.\n\n\n\n\n\n\n\n\nFigure 1: Predicted coordinate values (out-of-sample) from a linear probe on final-layer activations of an untrained neural network.\n\n\n\n\nModel has seen noisy coordinates plus \\(d\\) random features.\nSingle hidden layer with \\(h &lt; d\\) hidden units."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#pca-as-a-yield-curve-interpreter",
    "href": "content/talks/posts/2024-econdat/presentation.html#pca-as-a-yield-curve-interpreter",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "PCA as a Yield Curve Interpreter",
    "text": "PCA as a Yield Curve Interpreter\nWhat are principal components if not model embeddings?\n\n\n\n\n\n\nFigure 2: Top chart: The first two principal components of US Treasury yields over time at daily frequency. Bottom chart: Observed average level and 10yr-3mo spread of the yield curve. Vertical stalks roughly indicate the onset (|GFC) and the beginning of the aftermath (GFC|) of the Global Financial Crisis."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#embedding-fomc-comms",
    "href": "content/talks/posts/2024-econdat/presentation.html#embedding-fomc-comms",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Embedding FOMC comms",
    "text": "Embedding FOMC comms\n\nBERT-based model trained on FOMC minutes, speeches and press conferences to classify statements as hawkish or dovish (or neutral) (Shah, Paturi, and Chava 2023).\nWe linearly probe all layers to predict unseen economic indicators (CPI, PPI, UST yields).\nPredictive power increases with layer depth and probes outperform simple AR(\\(p\\)) models.\n\n\n\n\n\n\n\nFigure 3: Out-of-sample root mean squared error (RMSE) for the linear probe plotted against FOMC-RoBERTa’s n-th layer for different indicators."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#sparks-of-economic-understanding",
    "href": "content/talks/posts/2024-econdat/presentation.html#sparks-of-economic-understanding",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Sparks of Economic Understanding?",
    "text": "Sparks of Economic Understanding?\nPremise: If probe results were indicative of some intrinsic ‘understanding’ of the economy, then the probe should not be sensitive to random sentences unrelated to economics.\nParrot Test\n\nSelect the best-performing probe for each economic indicator.\nPredict inflation levels for real (related) and perturbed (unrelated) sentences.\n\n\n\n\n\n\n\nFigure 4: Probe predictions for sentences about inflation of prices (IP), deflation of prices (DP), inflation of birds (IB) and deflation of birds (DB). The vertical axis shows predicted inflation levels subtracted by the average predicted value of the probe for random noise.\n\n\n\nAs evidenced by Figure 4, the probe is easily fooled."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#spurious-relationships",
    "href": "content/talks/posts/2024-econdat/presentation.html#spurious-relationships",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Spurious Relationships",
    "text": "Spurious Relationships\nDefiniton: Varies somewhat (Haig 2003) but distinctly implies that the observation of correlations does not imply causation.\n\nHumans struggle to tell the difference between random and non-random sequences (Falk and Konold 1997).\nLack of expectation that randomness that hints towards a causal relationship will still appear at random.\nEven experts perceive correlations of inflated magnitude (Nickerson 1998) and causal relationships where none exist (Zgraggen et al. 2018)."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#antropomorphism",
    "href": "content/talks/posts/2024-econdat/presentation.html#antropomorphism",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Antropomorphism",
    "text": "Antropomorphism\nDefinition: Human tendency to attribute human-like characteristics to non-human agents and/or objects.\n\nExperience as humans is an always-readily-available template to interpret the world (Epley, Waytz, and Cacioppo 2007).\nMotivation to avoid loneliness may lead us to anthropomorphize inanimate objects Waytz, Epley, and Cacioppo (2010).\nMotivation to be competent may lead us anthropomorphize opaque technologies like LLMs Waytz, Epley, and Cacioppo (2010)"
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#confirmation-bias",
    "href": "content/talks/posts/2024-econdat/presentation.html#confirmation-bias",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Confirmation Bias",
    "text": "Confirmation Bias\nDefinition: Favoring interpretations of evidence that support existing beliefs or hypotheses (Nickerson 1998).\n\nHypotheses in present-day AI research are often implicit, often framed simply as a system being more accurate or efficient, compared to other systems.\n\nFailing to articulate a sufficiently strong null hypothesis leading to a ‘weak’ experiment (Claesen et al. 2022).\n\nIndividuals may place greater emphasis on evidence in support of their hypothesis, and lesser emphasis on evidence that opposes it (Nickerson 1998)."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#conclusion-and-outlook",
    "href": "content/talks/posts/2024-econdat/presentation.html#conclusion-and-outlook",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Conclusion and Outlook",
    "text": "Conclusion and Outlook\n\nWe call for the community to create explicit room for organized skepticism\n\nWelcome negative results\nEncouraging replication studies.\nMove from authorship to contribution-based credit (see e.g. Liem and Demetriou, 2023 and Smith, 1997).\n\nReturn to the Mertonian norms (communism, universalism, disinterestedness, organized skepticism) (Merton et al. 1942)."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#references",
    "href": "content/talks/posts/2024-econdat/presentation.html#references",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "References",
    "text": "References\n\n\nBender, Emily M, Timnit Gebru, Angelina McMillan-Major, and Shmargaret Shmitchell. 2021. “On the dangers of stochastic parrots: Can language models be too big? .” In Proceedings of the 2021 ACM Conference on Fairness, Accountability, and Transparency, 610–23.\n\n\nBirhane, Abeba, Pratyusha Kalluri, Dallas Card, William Agnew, Ravit Dotan, and Michelle Bao. 2022. “The Values Encoded in Machine Learning Research.” In Proceedings of the 2022 ACM Conference on Fairness, Accountability, and Transparency (FAccT ’22).\n\n\nClaesen, Aline, Daniel Lakens, Noah van Dongen, et al. 2022. “Severity and Crises in Science: Are We Getting It Right When We’re Right and Wrong When We’re Wrong?”\n\n\nEpley, Nicholas, Adam Waytz, and John T Cacioppo. 2007. “On seeing human: a three-factor theory of anthropomorphism.” Psychological Review 114 (4): 864.\n\n\nFalk, Ruma, and Clifford Konold. 1997. “Making sense of randomness: Implicit encoding as a basis for judgment.” Psychological Review 104 (2): 301.\n\n\nGoertzel, Ben. 2014. “Artificial general intelligence: concept, state of the art, and future prospects.” Journal of Artificial General Intelligence 5 (1): 1.\n\n\nGurnee, Wes, and Max Tegmark. 2023b. “Language Models Represent Space and Time.” arXiv Preprint arXiv:2310.02207v2.\n\n\n———. 2023a. “Language Models Represent Space and Time.” arXiv Preprint arXiv:2310.02207v1.\n\n\nHaig, Brian D. 2003. “What is a spurious correlation?” Understanding Statistics: Statistical Issues in Psychology, Education, and the Social Sciences 2 (2): 125–32.\n\n\nMerton, Robert K et al. 1942. “Science and technology in a democratic order.” Journal of Legal and Political Sociology 1 (1): 115–26.\n\n\nNickerson, Raymond S. 1998. “Confirmation bias: A ubiquitous phenomenon in many guises.” Review of General Psychology 2 (2): 175–220.\n\n\nShah, Agam, Suvan Paturi, and Sudheer Chava. 2023. “Trillion Dollar Words: A New Financial Dataset, Task & Market Analysis.” arXiv Preprint arXiv:2310.02207v1. https://arxiv.org/abs/2305.07972.\n\n\nTouvron, Hugo, Thibaut Lavril, Gautier Izacard, Xavier Martinet, Marie-Anne Lachaux, Timothée Lacroix, Baptiste Rozière, et al. 2023. “LLaMA: Open and Efficient Foundation Language Models.” https://arxiv.org/abs/2302.13971.\n\n\nTrinh, T. H., Wu, Y., Le, and Q. V. et al. 2024. “Solving olympiad geometry without human demonstrations.” Nature 625, 476–82. https://doi.org/https://doi.org/10.1038/s41586-023-06747-5.\n\n\nWaytz, Adam, Nicholas Epley, and John T Cacioppo. 2010. “Social cognition unbound: Insights into anthropomorphism and dehumanization.” Current Directions in Psychological Science 19 (1): 58–62.\n\n\nWeissburg, Iain Xie, Mehir Arora, Liangming Pan, and William Yang Wang. 2024. “Tweets to Citations: Unveiling the Impact of Social Media Influencers on AI Research Visibility.” arXiv Preprint arXiv:2401.13782.\n\n\nZgraggen, Emanuel, Zheguang Zhao, Robert Zeleznik, and Tim Kraska. 2018. “Investigating the effect of the multiple comparisons problem in visual analysis.” In Proceedings of the 2018 CHI Conference on Human Factors in Computing Systems, 1–12."
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#image-sources",
    "href": "content/talks/posts/2024-econdat/presentation.html#image-sources",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Image sources",
    "text": "Image sources\n\nLeonardo DiCaprio: Meme template by user on Reddit\nTarot cards: Photo by Viva Luna Studios on Unsplash\nWall-E: Photo by ray rui on Unsplash"
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#autoencoders-as-economic-growth-predictors",
    "href": "content/talks/posts/2024-econdat/presentation.html#autoencoders-as-economic-growth-predictors",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Autoencoders as Economic Growth Predictors",
    "text": "Autoencoders as Economic Growth Predictors\n\nWe train a neural network with a bottleneck layer to predict GDP growth from the yield curve.\nThis can be used for feature extraction and forecasting.\n\nBottle-neck layer embeddings predict spread and level of the yield curve.\n\n\n\n\n\n\n\n\nFigure 5: The left chart shows the actual GDP growth and fitted values from the autoencoder model. The right chart shows the observed average level and spread of the yield curve (solid) along with the predicted values (in-sample) from the linear probe based on the latent embeddings (dashed)"
  },
  {
    "objectID": "content/talks/posts/2024-econdat/presentation.html#quote-sources",
    "href": "content/talks/posts/2024-econdat/presentation.html#quote-sources",
    "title": "Against Spurious Sparks − Dovelating Inflated AI Claims 🕊️",
    "section": "Quote sources",
    "text": "Quote sources\n\n“There! It’s sentient”—that engineer at Google (probably!)\n“The human mind is a pattern-seeking device”—Daniel Kahneman\n“We’re fascinated with robots because they are reflections of ourselves.”—Ken Goldberg"
  }
]